{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36a57a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  train_loss=0.3712  val_loss=0.2919\n",
      "Epoch 02  train_loss=0.2066  val_loss=0.3048\n",
      "Epoch 03  train_loss=0.1673  val_loss=0.2532\n",
      "Epoch 04  train_loss=0.1640  val_loss=0.3110\n",
      "Epoch 05  train_loss=0.1563  val_loss=0.2003\n",
      "Epoch 06  train_loss=0.1477  val_loss=0.3941\n",
      "Epoch 07  train_loss=0.1426  val_loss=1.0057\n",
      "Epoch 08  train_loss=0.1434  val_loss=0.2375\n",
      "Epoch 09  train_loss=0.1211  val_loss=0.3437\n",
      "Epoch 10  train_loss=0.1160  val_loss=0.2721\n",
      "Epoch 11  train_loss=0.1147  val_loss=0.1732\n",
      "Epoch 12  train_loss=0.1046  val_loss=1.3323\n",
      "Epoch 13  train_loss=0.1142  val_loss=0.2677\n",
      "Epoch 14  train_loss=0.1034  val_loss=0.1915\n",
      "Epoch 15  train_loss=0.1139  val_loss=0.1490\n",
      "Epoch 16  train_loss=0.1015  val_loss=0.2421\n",
      "Epoch 17  train_loss=0.1008  val_loss=0.2169\n",
      "Epoch 18  train_loss=0.0946  val_loss=0.1399\n",
      "Epoch 19  train_loss=0.0827  val_loss=0.2168\n",
      "Epoch 20  train_loss=0.0862  val_loss=0.1368\n",
      "Epoch 21  train_loss=0.0849  val_loss=0.2953\n",
      "Epoch 22  train_loss=0.0754  val_loss=0.1221\n",
      "Epoch 23  train_loss=0.0765  val_loss=0.1742\n",
      "Epoch 24  train_loss=0.0691  val_loss=0.1417\n",
      "Epoch 25  train_loss=0.0736  val_loss=0.1823\n",
      "Epoch 26  train_loss=0.1014  val_loss=0.1400\n",
      "Epoch 27  train_loss=0.0815  val_loss=0.1575\n",
      "Epoch 28  train_loss=0.0596  val_loss=0.1681\n",
      "Epoch 29  train_loss=0.0635  val_loss=0.1577\n",
      "Epoch 30  train_loss=0.0709  val_loss=0.1860\n",
      "{'holdout_MAE': 10.316115379333496}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5276\n",
      "[LightGBM] [Info] Number of data points in the train set: 17328, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 3.408261\n",
      "Residual booster trained (LightGBM).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OperatingDTM</th>\n",
       "      <th>Interval</th>\n",
       "      <th>hb_houston_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>1</td>\n",
       "      <td>31.504337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>2</td>\n",
       "      <td>28.639776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>3</td>\n",
       "      <td>27.464830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>4</td>\n",
       "      <td>27.254873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>5</td>\n",
       "      <td>26.091288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>6</td>\n",
       "      <td>24.651674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>7</td>\n",
       "      <td>26.005545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>8</td>\n",
       "      <td>25.442276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>9</td>\n",
       "      <td>21.579998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>10</td>\n",
       "      <td>18.756807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>11</td>\n",
       "      <td>27.532866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>12</td>\n",
       "      <td>35.691906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>13</td>\n",
       "      <td>46.302837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>14</td>\n",
       "      <td>53.096855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>15</td>\n",
       "      <td>54.095577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>16</td>\n",
       "      <td>47.248405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>17</td>\n",
       "      <td>38.976597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>18</td>\n",
       "      <td>39.486920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>19</td>\n",
       "      <td>117.096811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>20</td>\n",
       "      <td>29.922519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>21</td>\n",
       "      <td>58.385774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>22</td>\n",
       "      <td>32.760963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>23</td>\n",
       "      <td>27.532694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>24</td>\n",
       "      <td>23.820854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OperatingDTM  Interval  hb_houston_pred\n",
       "0    2025-08-18         1        31.504337\n",
       "1    2025-08-18         2        28.639776\n",
       "2    2025-08-18         3        27.464830\n",
       "3    2025-08-18         4        27.254873\n",
       "4    2025-08-18         5        26.091288\n",
       "5    2025-08-18         6        24.651674\n",
       "6    2025-08-18         7        26.005545\n",
       "7    2025-08-18         8        25.442276\n",
       "8    2025-08-18         9        21.579998\n",
       "9    2025-08-18        10        18.756807\n",
       "10   2025-08-18        11        27.532866\n",
       "11   2025-08-18        12        35.691906\n",
       "12   2025-08-18        13        46.302837\n",
       "13   2025-08-18        14        53.096855\n",
       "14   2025-08-18        15        54.095577\n",
       "15   2025-08-18        16        47.248405\n",
       "16   2025-08-18        17        38.976597\n",
       "17   2025-08-18        18        39.486920\n",
       "18   2025-08-18        19       117.096811\n",
       "19   2025-08-18        20        29.922519\n",
       "20   2025-08-18        21        58.385774\n",
       "21   2025-08-18        22        32.760963\n",
       "22   2025-08-18        23        27.532694\n",
       "23   2025-08-18        24        23.820854"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== PyTorch seq2seq with ROBUST preprocessing (median impute + safe standardize + safe MAE) ====\n",
    "import duckdb, pandas as pd, numpy as np, datetime as dt, math\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()   # change target date if needed\n",
    "W_PAST = 672\n",
    "H_FUT = 24\n",
    "HOLDOUT_DAYS = 60\n",
    "HIDDEN = 128\n",
    "EPOCHS = 30\n",
    "LR = 1e-2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def fit_standardizer(X2d):\n",
    "    \"\"\"Return (mean,std) with std guarded (==0 -> 1). X2d: (N, F)\"\"\"\n",
    "    mu = np.nanmean(X2d, axis=0)\n",
    "    sd = np.nanstd(X2d, axis=0)\n",
    "    sd = np.where(sd <= 1e-8, 1.0, sd)\n",
    "    return mu, sd\n",
    "\n",
    "def apply_standardizer(X, mu, sd):\n",
    "    return (X - mu) / sd\n",
    "\n",
    "# robust 3D imputer\n",
    "def impute_inplace_3d(X, med_vec):\n",
    "    \"\"\"X: (N, T, F) or (N, W, F); med_vec: (F,). Fills NaNs in-place with per-feature medians.\"\"\"\n",
    "    mask = np.isnan(X)\n",
    "    if mask.any():\n",
    "        med_broadcast = np.broadcast_to(med_vec, X.shape)\n",
    "        X[mask] = med_broadcast[mask]\n",
    "\n",
    "# sanity checker\n",
    "def assert_finite(name, arr):\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = int(np.isnan(arr).sum() + np.isinf(arr).sum())\n",
    "        raise RuntimeError(f\"{name} has {bad} non-finite values\")\n",
    "\n",
    "# ---------- ensure weather-enhanced views (hist + fcst) exist & helper transforms ----------\n",
    "con_ensure = duckdb.connect(DB)\n",
    "# Historical weather enhanced (to backfill when forecast is missing during training)\n",
    "con_ensure.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW wx_hist_enh AS\n",
    "WITH agg AS (\n",
    "  SELECT\n",
    "    OperatingDTM, \"interval\" AS Interval,\n",
    "    (hist_temp_the_woodlands_tx + hist_temp_katy_tx + hist_temp_friendswood_tx + hist_temp_baytown_tx + hist_temp_houston_tx)/5.0 AS temp_avg,\n",
    "    (hist_hum_the_woodlands_tx  + hist_hum_katy_tx  + hist_hum_friendswood_tx  + hist_hum_baytown_tx  + hist_hum_houston_tx)/5.0  AS hum_avg,\n",
    "    GREATEST(hist_temp_the_woodlands_tx, hist_temp_katy_tx, hist_temp_friendswood_tx, hist_temp_baytown_tx, hist_temp_houston_tx)\n",
    "      - LEAST(hist_temp_the_woodlands_tx, hist_temp_katy_tx, hist_temp_friendswood_tx, hist_temp_baytown_tx, hist_temp_houston_tx) AS temp_spread,\n",
    "    GREATEST(hist_hum_the_woodlands_tx, hist_hum_katy_tx, hist_hum_friendswood_tx, hist_hum_baytown_tx, hist_hum_houston_tx)\n",
    "      - LEAST(hist_hum_the_woodlands_tx, hist_hum_katy_tx, hist_hum_friendswood_tx, hist_hum_baytown_tx, hist_hum_houston_tx) AS hum_spread\n",
    "  FROM vw_historical_weather_by_city\n",
    ")\n",
    "SELECT\n",
    "  a.*,\n",
    "  a.temp_avg - LAG(a.temp_avg) OVER (ORDER BY OperatingDTM, Interval) AS temp_avg_ramp1,\n",
    "  a.hum_avg  - LAG(a.hum_avg)  OVER (ORDER BY OperatingDTM, Interval) AS hum_avg_ramp1\n",
    "FROM agg a;\n",
    "\"\"\")\n",
    "con_ensure.close()\n",
    "\n",
    "# Signed log transforms for spikes\n",
    "sgn = np.sign\n",
    "abs_ = np.abs\n",
    "\n",
    "def sgn_log1p(y):\n",
    "    return sgn(y) * np.log1p(abs_(y))\n",
    "\n",
    "def sgn_expm1(z):\n",
    "    return sgn(z) * np.expm1(abs_(z))\n",
    "\n",
    "# Hour-of-day weights (emphasize 17-21 local hours)\n",
    "HOUR_WEIGHTS = np.ones(24, dtype=np.float32)\n",
    "# Reweight hours to reduce early overshoot and emphasize true peak\n",
    "# index 0->H1, ..., 23->H24\n",
    "HOUR_WEIGHTS[:] = 1.0\n",
    "HOUR_WEIGHTS[15] = 1.5     # H16\n",
    "HOUR_WEIGHTS[16:18] = 2.0  # H17-18\n",
    "HOUR_WEIGHTS[18:20] = 5.0  # H19-20 (peak focus)\n",
    "HOUR_WEIGHTS[20] = 3.0     # H21\n",
    "\n",
    "# ---------- pull data ----------\n",
    "con = duckdb.connect(DB)\n",
    "df_lags = con.execute(\"\"\"\n",
    "SELECT\n",
    "  ts, OperatingDTM, Interval, hb_houston,\n",
    "  p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "  dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "FROM vw_master_spine_lags\n",
    "ORDER BY ts\n",
    "\"\"\").df()\n",
    "df_lags[\"ts\"] = pd.to_datetime(df_lags[\"ts\"])\n",
    "\n",
    "df_future = con.execute(\"\"\"\n",
    "SELECT\n",
    "  f.OperatingDTM AS delivery_date,\n",
    "  f.Interval     AS delivery_interval,\n",
    "  f.hb_houston   AS target,\n",
    "  -- base future features\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "  f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f,\n",
    "  f.cal_sin_dow  AS cal_sin_dow_f,  f.cal_cos_dow  AS cal_cos_dow_f,\n",
    "  -- load-enhanced joins\n",
    "  l.net_wz, l.net_lz, l.net_wz_ramp1, l.net_lz_ramp1, l.lz_houston_ramp1, l.wz_spread, l.lz_spread,\n",
    "  -- weather: COALESCE forecast -> historical\n",
    "  COALESCE(wf.temp_avg,      wh.temp_avg)      AS temp_avg,\n",
    "  COALESCE(wf.temp_spread,   wh.temp_spread)   AS temp_spread,\n",
    "  COALESCE(wf.temp_avg_ramp1,wh.temp_avg_ramp1)AS temp_avg_ramp1,\n",
    "  COALESCE(wf.hum_avg,       wh.hum_avg)       AS hum_avg,\n",
    "  COALESCE(wf.hum_spread,    wh.hum_spread)    AS hum_spread,\n",
    "  COALESCE(wf.hum_avg_ramp1, wh.hum_avg_ramp1) AS hum_avg_ramp1,\n",
    "  CASE WHEN wf.temp_avg IS NULL THEN 1 ELSE 0 END AS is_weather_proxy,\n",
    "  -- additional ramps & day-over-day deltas to help with peaks (computed over time order)\n",
    "  (l.net_wz      - LAG(l.net_wz,      3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_ramp3,\n",
    "  (l.net_wz      - LAG(l.net_wz,      6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_ramp6,\n",
    "  (l.net_lz      - LAG(l.net_lz,      3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_ramp3,\n",
    "  (l.net_lz      - LAG(l.net_lz,      6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_ramp6,\n",
    "  (l.lz_houston  - LAG(l.lz_houston,  3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_ramp3,\n",
    "  (l.lz_houston  - LAG(l.lz_houston,  6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_ramp6,\n",
    "  (l.net_wz      - LAG(l.net_wz,     24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_dod,\n",
    "  (l.net_lz      - LAG(l.net_lz,     24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_dod,\n",
    "  (l.lz_houston  - LAG(l.lz_houston, 24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_dod\n",
    "FROM vw_master_spine_ts f\n",
    "LEFT JOIN load_fcst_enh l\n",
    "  ON l.OperatingDTM = f.OperatingDTM AND l.Interval = f.Interval\n",
    "LEFT JOIN wx_fcst_enh  wf\n",
    "  ON wf.OperatingDTM = f.OperatingDTM AND wf.Interval = f.Interval\n",
    "LEFT JOIN wx_hist_enh  wh\n",
    "  ON wh.OperatingDTM = f.OperatingDTM AND wh.Interval = f.Interval\n",
    "ORDER BY f.OperatingDTM, f.Interval\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "df_future[\"delivery_date\"] = pd.to_datetime(df_future[\"delivery_date\"]).dt.date\n",
    "df_future[\"delivery_interval\"] = df_future[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# ---------- assemble samples (day-ahead) ----------\n",
    "enc_cols = [\n",
    "    \"hb_houston\",\"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"\n",
    "]\n",
    "dec_future_cols = [\n",
    "    \"cal_hour_f\",\"cal_dow_f\",\"cal_is_weekend_f\",\"cal_sin_hour_f\",\"cal_cos_hour_f\",\"cal_sin_dow_f\",\"cal_cos_dow_f\",\n",
    "    \"net_wz\",\"net_lz\",\"net_wz_ramp1\",\"net_lz_ramp1\",\"lz_houston_ramp1\",\"wz_spread\",\"lz_spread\",\n",
    "    \"temp_avg\",\"temp_spread\",\"temp_avg_ramp1\",\"hum_avg\",\"hum_spread\",\"hum_avg_ramp1\",\n",
    "    # NEW: multi-horizon ramps + day-over-day deltas\n",
    "    \"net_wz_ramp3\",\"net_wz_ramp6\",\"net_lz_ramp3\",\"net_lz_ramp6\",\n",
    "    \"lz_houston_ramp3\",\"lz_houston_ramp6\",\"net_wz_dod\",\"net_lz_dod\",\"lz_houston_dod\",\n",
    "    # Identify when weather came from historical backfill instead of forecast\n",
    "    \"is_weather_proxy\"\n",
    "]\n",
    "\n",
    "# base rows = end-of-day with price\n",
    "base_rows = df_lags[(df_lags[\"hb_houston\"].notna()) & (df_lags[\"Interval\"]==24)][[\"ts\",\"OperatingDTM\",\"hb_houston\"]].copy()\n",
    "base_rows[\"base_date\"] = pd.to_datetime(base_rows[\"OperatingDTM\"]).dt.date\n",
    "base_rows[\"delivery_date\"] = base_rows[\"base_date\"] + dt.timedelta(days=1)\n",
    "\n",
    "# only use delivery days with 24 actuals for training/val\n",
    "has24 = df_future.groupby(\"delivery_date\")[\"target\"].apply(lambda s: s.notna().sum()==24)\n",
    "valid_delivery_dates = set(has24[has24].index)\n",
    "\n",
    "df_lags_idx = df_lags.set_index(\"ts\").sort_index()\n",
    "\n",
    "samples = []\n",
    "for _, r in base_rows.iterrows():\n",
    "    ddate = r[\"delivery_date\"]\n",
    "    ts0 = r[\"ts\"]\n",
    "    # encoder window\n",
    "    start = ts0 - pd.Timedelta(hours=W_PAST-1)\n",
    "    enc_df = df_lags_idx.loc[start:ts0][enc_cols]\n",
    "    if enc_df.shape[0] != W_PAST: \n",
    "        continue\n",
    "    if enc_df.isna().any().any():\n",
    "        continue  # strict: skip any encoder NaNs\n",
    "    # decoder future rows\n",
    "    fdf = df_future[(df_future[\"delivery_date\"]==ddate)].sort_values(\"delivery_interval\")\n",
    "    if fdf.shape[0] != H_FUT:\n",
    "        continue\n",
    "    X_enc = enc_df.values.astype(np.float32)\n",
    "    X_dec = fdf[dec_future_cols].values.astype(np.float32)\n",
    "    y     = fdf[\"target\"].values.astype(np.float32)\n",
    "    y0    = np.float32(r[\"hb_houston\"])\n",
    "    samples.append((X_enc, X_dec, y, y0, ddate))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples assembled; check data coverage or reduce W_PAST.\")\n",
    "\n",
    "# split by date\n",
    "dates = sorted({s[4] for s in samples})\n",
    "cut_date = dates[-1] - dt.timedelta(days=HOLDOUT_DAYS) if len(dates)>HOLDOUT_DAYS else dates[0]\n",
    "train_idx = [i for i,s in enumerate(samples) if s[4] <= cut_date and s[4] in valid_delivery_dates]\n",
    "val_idx   = [i for i,s in enumerate(samples) if s[4] >  cut_date and s[4] in valid_delivery_dates]\n",
    "\n",
    "def stack(idxs):\n",
    "    Xe = np.stack([samples[i][0] for i in idxs], axis=0)\n",
    "    Xd = np.stack([samples[i][1] for i in idxs], axis=0)\n",
    "    Y  = np.stack([samples[i][2] for i in idxs], axis=0)\n",
    "    Y0 = np.stack([samples[i][3] for i in idxs], axis=0)\n",
    "    return Xe, Xd, Y, Y0\n",
    "\n",
    "Xe_tr, Xd_tr, Y_tr, Y0_tr = stack(train_idx)\n",
    "Xe_va, Xd_va, Y_va, Y0_va = (None, None, None, None)\n",
    "if len(val_idx) > 0:\n",
    "    Xe_va, Xd_va, Y_va, Y0_va = stack(val_idx)\n",
    "\n",
    "# ---------- diagnose + drop all-NaN decoder features, then impute (robust) ----------\n",
    "# Flatten decoder train to 2D to inspect per-feature missingness\n",
    "F_orig = Xd_tr.shape[2]\n",
    "Xd_tr_2d_raw = Xd_tr.reshape(-1, F_orig)\n",
    "nan_counts = np.isnan(Xd_tr_2d_raw).sum(axis=0)\n",
    "all_nan_cols = nan_counts == Xd_tr_2d_raw.shape[0]\n",
    "\n",
    "if all_nan_cols.any():\n",
    "    dropped_cols = [dec_future_cols[i] for i,flag in enumerate(all_nan_cols) if flag]\n",
    "    print(f\"Dropping {int(all_nan_cols.sum())} decoder feature(s) with all-NaN in TRAIN: {dropped_cols}\")\n",
    "else:\n",
    "    dropped_cols = []\n",
    "\n",
    "# Keep mask to be reused at prediction time\n",
    "dec_keep_mask = ~all_nan_cols\n",
    "\n",
    "# Apply mask to decoder tensors\n",
    "Xd_tr = Xd_tr[:, :, dec_keep_mask]\n",
    "if Xe_va is not None:\n",
    "    Xd_va = Xd_va[:, :, dec_keep_mask]\n",
    "\n",
    "# Recompute medians on KEPT features\n",
    "dec_med = np.nanmedian(Xd_tr.reshape(-1, Xd_tr.shape[2]), axis=0)\n",
    "# Guard: if a median is still NaN for some reason, set to 0\n",
    "dec_med = np.where(np.isnan(dec_med), 0.0, dec_med)\n",
    "\n",
    "# Encoder median (rare but safe)\n",
    "enc_med = np.nanmedian(Xe_tr.reshape(-1, Xe_tr.shape[2]), axis=0)\n",
    "enc_med = np.where(np.isnan(enc_med), 0.0, enc_med)\n",
    "\n",
    "# Impute in-place (3D-safe)\n",
    "impute_inplace_3d(Xe_tr, enc_med)\n",
    "impute_inplace_3d(Xd_tr, dec_med)\n",
    "if Xe_va is not None:\n",
    "    impute_inplace_3d(Xe_va, enc_med)\n",
    "    impute_inplace_3d(Xd_va, dec_med)\n",
    "\n",
    "# Targets should be complete for training/val; assert\n",
    "assert np.isfinite(Y_tr).all(), \"Y_tr has non-finite values\"\n",
    "if Y_va is not None:\n",
    "    assert np.isfinite(Y_va).all(), \"Y_va has non-finite values\"\n",
    "\n",
    "# ---------- safe standardization ----------\n",
    "enc_mu, enc_sd = fit_standardizer(Xe_tr.reshape(-1, Xe_tr.shape[2]))\n",
    "dec_mu, dec_sd = fit_standardizer(Xd_tr.reshape(-1, Xd_tr.shape[2]))\n",
    "# ---- target transform (spike-aware) then standardize ----\n",
    "Y_tr_t  = sgn_log1p(Y_tr)\n",
    "y_mu, y_sd = fit_standardizer(Y_tr_t.reshape(-1,1))\n",
    "y_mu = float(np.atleast_1d(y_mu)[0]); y_sd = float(np.atleast_1d(y_sd)[0])\n",
    "\n",
    "Xe_tr_n = apply_standardizer(Xe_tr, enc_mu, enc_sd)\n",
    "Xd_tr_n = apply_standardizer(Xd_tr, dec_mu, dec_sd)\n",
    "Y_tr_n  = apply_standardizer(Y_tr_t, y_mu, y_sd).reshape(Y_tr.shape)\n",
    "Y0_tr_t = sgn_log1p(Y0_tr.reshape(-1,1)).reshape(-1,1)\n",
    "Y0_tr_n = apply_standardizer(Y0_tr_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = apply_standardizer(Xe_va, enc_mu, enc_sd)\n",
    "    Xd_va_n = apply_standardizer(Xd_va, dec_mu, dec_sd)\n",
    "    Y_va_t  = sgn_log1p(Y_va)\n",
    "    Y_va_n  = apply_standardizer(Y_va_t,  y_mu,  y_sd).reshape(Y_va.shape)\n",
    "    Y0_va_t = sgn_log1p(Y0_va.reshape(-1,1))\n",
    "    Y0_va_n = apply_standardizer(Y0_va_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "\n",
    "# Final safety: ensure everything is finite\n",
    "assert_finite(\"Xe_tr_n\", Xe_tr_n)\n",
    "assert_finite(\"Xd_tr_n\", Xd_tr_n)\n",
    "assert_finite(\"Y_tr_n\",  Y_tr_n)\n",
    "assert_finite(\"Y0_tr_n\", Y0_tr_n)\n",
    "if Xe_va is not None:\n",
    "    assert_finite(\"Xe_va_n\", Xe_va_n)\n",
    "    assert_finite(\"Xd_va_n\", Xd_va_n)\n",
    "    assert_finite(\"Y_va_n\",  Y_va_n)\n",
    "    assert_finite(\"Y0_va_n\", Y0_va_n)\n",
    "\n",
    "# ---------- torch datasets ----------\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xe, Xd, Y, Y0):\n",
    "        self.Xe = torch.from_numpy(Xe).float()\n",
    "        self.Xd = torch.from_numpy(Xd).float()\n",
    "        self.Y  = torch.from_numpy(Y).float()\n",
    "        self.Y0 = torch.from_numpy(Y0).float()\n",
    "    def __len__(self): return self.Xe.shape[0]\n",
    "    def __getitem__(self, i): return self.Xe[i], self.Xd[i], self.Y[i], self.Y0[i]\n",
    "\n",
    "bs = 32\n",
    "train_loader = torch.utils.data.DataLoader(Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=bs, shuffle=True)\n",
    "val_loader = None if Xe_va is None else torch.utils.data.DataLoader(Seq2SeqDataset(Xe_va_n, Xd_va_n, Y_va_n, Y0_va_n), batch_size=bs, shuffle=False)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim, hidden, batch_first=True)\n",
    "    def forward(self, x):  # x: (B,W,E)\n",
    "        _, h = self.rnn(x)\n",
    "        return h  # (1,B,H)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim + 1, hidden, batch_first=True)  # + prev y\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "    def forward(self, future_feats, h0, y0, teacher=None, tf_prob=0.5):\n",
    "        B, T, D = future_feats.shape\n",
    "        y_prev = y0.view(B,1)\n",
    "        h = h0\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            x_t = torch.cat([future_feats[:,t,:], y_prev], dim=1).unsqueeze(1)  # (B,1,D+1)\n",
    "            o, h = self.rnn(x_t, h)\n",
    "            y_t = self.out(o[:, -1, :]).squeeze(1)\n",
    "            outs.append(y_t)\n",
    "            if (teacher is not None) and (rng.random() < tf_prob):\n",
    "                y_prev = teacher[:,t].view(B,1)\n",
    "            else:\n",
    "                y_prev = y_t.view(B,1)\n",
    "        return torch.stack(outs, dim=1)  # (B,24)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(enc_in, hidden)\n",
    "        self.dec = Decoder(dec_in, hidden)\n",
    "    def forward(self, x_enc, x_dec, y0, y_teacher=None, tf_prob=0.5):\n",
    "        h = self.enc(x_enc)\n",
    "        return self.dec(x_dec, h, y0, y_teacher, tf_prob)\n",
    "\n",
    "enc_in = Xe_tr_n.shape[2]\n",
    "dec_in = Xd_tr_n.shape[2]\n",
    "model = Seq2Seq(enc_in, dec_in, HIDDEN).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "def huber_weighted(yhat, y, w, delta=1.0):\n",
    "    d = torch.abs(yhat - y)\n",
    "    q = torch.clamp(d, max=delta)\n",
    "    l = 0.5 * q**2 + delta * (d - q)\n",
    "    return (l * w).mean()\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in train_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=y, tf_prob=0.5)\n",
    "        # time-step weights (B,24)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    if val_loader is None: return None\n",
    "    model.eval()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(val_loader.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    print(f\"Epoch {ep:02d}  train_loss={tr:.4f}\" + (\"\" if va is None else f\"  val_loss={va:.4f}\"))\n",
    "\n",
    "# ---- evaluate holdout MAE in real units (SAFE) ----\n",
    "if val_loader is not None:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xenc, xdec, y, y0 in val_loader:\n",
    "            xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "            yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "            y_n    = y.detach().cpu().numpy()\n",
    "            # invert scaling + inverse spike transform\n",
    "            yhat_t = (yhat_n * y_sd + y_mu)\n",
    "            yt_t   = (y_n    * y_sd + y_mu)\n",
    "            yhat = sgn_expm1(yhat_t).reshape(-1)\n",
    "            yt   = sgn_expm1(yt_t).reshape(-1)\n",
    "            preds.append(yhat)\n",
    "            trues.append(yt)\n",
    "    preds = np.concatenate(preds)\n",
    "    trues = np.concatenate(trues)\n",
    "    mask = np.isfinite(preds) & np.isfinite(trues)\n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped:\n",
    "        print(f\"Dropped {dropped} non-finite pairs from holdout scoring.\")\n",
    "    print({\"holdout_MAE\": float(mean_absolute_error(trues[mask], preds[mask]))})\n",
    "\n",
    "# ---- optional LightGBM residual booster (train on TRAIN residuals) ----\n",
    "USE_LGBM_RESIDUAL = True\n",
    "booster = None\n",
    "if USE_LGBM_RESIDUAL:\n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        # Build a non-shuffled loader to preserve order\n",
    "        train_loader_eval = torch.utils.data.DataLoader(\n",
    "            Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=64, shuffle=False)\n",
    "        base_preds = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for xenc, xdec, y, y0 in train_loader_eval:\n",
    "                xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "                yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "                base_preds.append(yhat_n)\n",
    "        base_preds = np.concatenate(base_preds, axis=0)  # (N_train, 24)\n",
    "        # inverse transforms -> true $/MWh\n",
    "        base_preds_t = (base_preds * y_sd + y_mu)\n",
    "        base_preds_true = sgn_expm1(base_preds_t)\n",
    "        y_true_tr = Y_tr  # already true units\n",
    "        # flatten\n",
    "        y_base_flat = base_preds_true.reshape(-1)\n",
    "        y_true_flat = y_true_tr.reshape(-1)\n",
    "        X_flat = Xd_tr.reshape(-1, Xd_tr.shape[2])  # kept raw decoder features\n",
    "        mask = np.isfinite(y_base_flat) & np.isfinite(y_true_flat) & np.isfinite(X_flat).all(axis=1)\n",
    "        X_flat = X_flat[mask]\n",
    "        y_resid_flat = (y_true_flat - y_base_flat)[mask]\n",
    "        # Fit booster\n",
    "        booster = lgb.LGBMRegressor(\n",
    "            n_estimators=600, learning_rate=0.03, subsample=0.8, colsample_bytree=0.8,\n",
    "            max_depth=-1, reg_alpha=0.0, reg_lambda=0.0, min_child_samples=20, random_state=42)\n",
    "        booster.fit(X_flat, y_resid_flat)\n",
    "        print(\"Residual booster trained (LightGBM).\")\n",
    "    except Exception as e:\n",
    "        print(f\"LightGBM residual booster skipped: {e}\")\n",
    "\n",
    "# ---- predict requested DELIVERY_DATE ----\n",
    "\n",
    "# find base row (previous day EOD with price)\n",
    "prev_day = DELIVERY_DATE - dt.timedelta(days=1)\n",
    "base_row = df_lags[(pd.to_datetime(df_lags[\"OperatingDTM\"]).dt.date == prev_day) & (df_lags[\"Interval\"]==24) & df_lags[\"hb_houston\"].notna()]\n",
    "if base_row.empty:\n",
    "    raise RuntimeError(f\"No base EOD price for {prev_day}\")\n",
    "ts0 = pd.to_datetime(base_row.iloc[0][\"ts\"])\n",
    "y0  = np.float32(base_row.iloc[0][\"hb_houston\"])\n",
    "\n",
    "# encoder window\n",
    "win = df_lags.set_index(\"ts\").loc[ts0 - pd.Timedelta(hours=W_PAST-1): ts0][enc_cols]\n",
    "if win.shape[0] != W_PAST or win.isna().any().any():\n",
    "    raise RuntimeError(\"Insufficient/NaN history for encoder window.\")\n",
    "Xe_pred = win.values.astype(np.float32)[None, ...]\n",
    "\n",
    "# decoder future features (24 rows)\n",
    "frows = df_future[(df_future[\"delivery_date\"]==DELIVERY_DATE)].sort_values(\"delivery_interval\")\n",
    "if frows.shape[0] != H_FUT:\n",
    "    raise RuntimeError(\"Spine missing 24 rows for delivery date.\")\n",
    "Xd_pred_full = frows[dec_future_cols].values.astype(np.float32)[None, ...]\n",
    "\n",
    "# Apply the same keep mask used in training (drop columns that were all-NaN in train)\n",
    "if 'dec_keep_mask' in globals():\n",
    "    Xd_pred = Xd_pred_full[:, :, dec_keep_mask]\n",
    "else:\n",
    "    Xd_pred = Xd_pred_full\n",
    "\n",
    "# impute + standardize pred using TRAIN stats\n",
    "m = np.isnan(Xd_pred)\n",
    "if m.any():\n",
    "    Xd_pred[m] = np.broadcast_to(dec_med, Xd_pred.shape)[m]\n",
    "Xe_pred_n = apply_standardizer(Xe_pred, enc_mu, enc_sd)\n",
    "Xd_pred_n = apply_standardizer(Xd_pred, dec_mu, dec_sd)\n",
    "# compute normalized scalar y0_n using spike-aware transform\n",
    "y0_t = sgn_log1p(y0)\n",
    "y0_n = np.float32(((y0_t - y_mu) / y_sd)).ravel()[0]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_n = model(torch.from_numpy(Xe_pred_n).to(DEVICE),\n",
    "                   torch.from_numpy(Xd_pred_n).to(DEVICE),\n",
    "                   torch.tensor([y0_n], dtype=torch.float32, device=DEVICE),\n",
    "                   y_teacher=None, tf_prob=0.0).detach().cpu().numpy()[0]\n",
    "yhat_t = (yhat_n * y_sd + y_mu).reshape(-1)\n",
    "yhat = sgn_expm1(yhat_t)\n",
    "# --- optional: add residual booster only on peak hours (H18â€“H21) ---\n",
    "if booster is not None:\n",
    "    Xd_pred_kept = Xd_pred.copy()  # (1,24,F_kept)\n",
    "    resid_pred = booster.predict(Xd_pred_kept.reshape(24, Xd_pred_kept.shape[2]))\n",
    "    hour_mask = np.zeros(24, dtype=np.float32)\n",
    "    hour_mask[17:21] = 1.0  # apply only to intervals 18,19,20,21\n",
    "    yhat = yhat + resid_pred * hour_mask\n",
    "\n",
    "# replace any residual NaN with the day's median\n",
    "if np.isnan(yhat).any():\n",
    "    day_med = float(np.nanmedian(yhat))\n",
    "    yhat = np.nan_to_num(yhat, nan=day_med)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*H_FUT,\n",
    "    \"Interval\": list(range(1, H_FUT+1)),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "})\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4b8dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  train_loss=0.3602  val_loss=0.4567\n",
      "Epoch 02  train_loss=0.2156  val_loss=0.5112\n",
      "Epoch 03  train_loss=0.1802  val_loss=0.3709\n",
      "Epoch 04  train_loss=0.1574  val_loss=0.4478\n",
      "Epoch 05  train_loss=0.1605  val_loss=0.2136\n",
      "Epoch 06  train_loss=0.1471  val_loss=0.2455\n",
      "Epoch 07  train_loss=0.1507  val_loss=0.2474\n",
      "Epoch 08  train_loss=0.1323  val_loss=0.3780\n",
      "Epoch 09  train_loss=0.1327  val_loss=0.2690\n",
      "Epoch 10  train_loss=0.1202  val_loss=0.2851\n",
      "Epoch 11  train_loss=0.1288  val_loss=0.4964\n",
      "Epoch 12  train_loss=0.1281  val_loss=0.4486\n",
      "Epoch 13  train_loss=0.1328  val_loss=0.1486\n",
      "Epoch 14  train_loss=0.1413  val_loss=0.1517\n",
      "Epoch 15  train_loss=0.1189  val_loss=0.2193\n",
      "Epoch 16  train_loss=0.1006  val_loss=0.1624\n",
      "Epoch 17  train_loss=0.1199  val_loss=0.1646\n",
      "Epoch 18  train_loss=0.1102  val_loss=0.1511\n",
      "Epoch 19  train_loss=0.1063  val_loss=0.2288\n",
      "Epoch 20  train_loss=0.0901  val_loss=0.1795\n",
      "Epoch 21  train_loss=0.0831  val_loss=0.2677\n",
      "Epoch 22  train_loss=0.1042  val_loss=0.7247\n",
      "Epoch 23  train_loss=0.1206  val_loss=0.1667\n",
      "Epoch 24  train_loss=0.0895  val_loss=0.1569\n",
      "Epoch 25  train_loss=0.0824  val_loss=0.2200\n",
      "Epoch 26  train_loss=0.0751  val_loss=0.1697\n",
      "Epoch 27  train_loss=0.0690  val_loss=0.2418\n",
      "Epoch 28  train_loss=0.0673  val_loss=0.1452\n",
      "Epoch 29  train_loss=0.0637  val_loss=0.1901\n",
      "Epoch 30  train_loss=0.0898  val_loss=0.1780\n",
      "{'holdout_MAE': 9.256233215332031}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5328\n",
      "[LightGBM] [Info] Number of data points in the train set: 20352, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 9.040201\n",
      "Residual booster trained (LightGBM).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OperatingDTM</th>\n",
       "      <th>Interval</th>\n",
       "      <th>hb_houston_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>1</td>\n",
       "      <td>28.624001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>2</td>\n",
       "      <td>27.802418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>3</td>\n",
       "      <td>25.580601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>4</td>\n",
       "      <td>22.805994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>5</td>\n",
       "      <td>24.711779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>6</td>\n",
       "      <td>23.401142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>7</td>\n",
       "      <td>22.960243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>8</td>\n",
       "      <td>23.690546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>9</td>\n",
       "      <td>18.475698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>10</td>\n",
       "      <td>17.273062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>11</td>\n",
       "      <td>21.594927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>12</td>\n",
       "      <td>31.204277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>13</td>\n",
       "      <td>41.659058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>14</td>\n",
       "      <td>49.524105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>15</td>\n",
       "      <td>58.170345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>16</td>\n",
       "      <td>68.319824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>17</td>\n",
       "      <td>61.828094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>18</td>\n",
       "      <td>195.589414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>19</td>\n",
       "      <td>231.870295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>20</td>\n",
       "      <td>21.351045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>21</td>\n",
       "      <td>47.563955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>22</td>\n",
       "      <td>63.947838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>23</td>\n",
       "      <td>35.490776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>24</td>\n",
       "      <td>33.171886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OperatingDTM  Interval  hb_houston_pred\n",
       "0    2025-08-18         1        28.624001\n",
       "1    2025-08-18         2        27.802418\n",
       "2    2025-08-18         3        25.580601\n",
       "3    2025-08-18         4        22.805994\n",
       "4    2025-08-18         5        24.711779\n",
       "5    2025-08-18         6        23.401142\n",
       "6    2025-08-18         7        22.960243\n",
       "7    2025-08-18         8        23.690546\n",
       "8    2025-08-18         9        18.475698\n",
       "9    2025-08-18        10        17.273062\n",
       "10   2025-08-18        11        21.594927\n",
       "11   2025-08-18        12        31.204277\n",
       "12   2025-08-18        13        41.659058\n",
       "13   2025-08-18        14        49.524105\n",
       "14   2025-08-18        15        58.170345\n",
       "15   2025-08-18        16        68.319824\n",
       "16   2025-08-18        17        61.828094\n",
       "17   2025-08-18        18       195.589414\n",
       "18   2025-08-18        19       231.870295\n",
       "19   2025-08-18        20        21.351045\n",
       "20   2025-08-18        21        47.563955\n",
       "21   2025-08-18        22        63.947838\n",
       "22   2025-08-18        23        35.490776\n",
       "23   2025-08-18        24        33.171886"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== PyTorch seq2seq with ROBUST preprocessing (median impute + safe standardize + safe MAE) ====\n",
    "import duckdb, pandas as pd, numpy as np, datetime as dt, math\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()   # change target date if needed\n",
    "W_PAST = 168\n",
    "H_FUT = 24\n",
    "HOLDOUT_DAYS = 60\n",
    "HIDDEN = 128\n",
    "EPOCHS = 30\n",
    "LR = 1e-2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def fit_standardizer(X2d):\n",
    "    \"\"\"Return (mean,std) with std guarded (==0 -> 1). X2d: (N, F)\"\"\"\n",
    "    mu = np.nanmean(X2d, axis=0)\n",
    "    sd = np.nanstd(X2d, axis=0)\n",
    "    sd = np.where(sd <= 1e-8, 1.0, sd)\n",
    "    return mu, sd\n",
    "\n",
    "def apply_standardizer(X, mu, sd):\n",
    "    return (X - mu) / sd\n",
    "\n",
    "# robust 3D imputer\n",
    "def impute_inplace_3d(X, med_vec):\n",
    "    \"\"\"X: (N, T, F) or (N, W, F); med_vec: (F,). Fills NaNs in-place with per-feature medians.\"\"\"\n",
    "    mask = np.isnan(X)\n",
    "    if mask.any():\n",
    "        med_broadcast = np.broadcast_to(med_vec, X.shape)\n",
    "        X[mask] = med_broadcast[mask]\n",
    "\n",
    "# sanity checker\n",
    "def assert_finite(name, arr):\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = int(np.isnan(arr).sum() + np.isinf(arr).sum())\n",
    "        raise RuntimeError(f\"{name} has {bad} non-finite values\")\n",
    "\n",
    "# ---------- ensure weather-enhanced views (hist + fcst) exist & helper transforms ----------\n",
    "con_ensure = duckdb.connect(DB)\n",
    "# Historical weather enhanced (to backfill when forecast is missing during training)\n",
    "con_ensure.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW wx_hist_enh AS\n",
    "WITH agg AS (\n",
    "  SELECT\n",
    "    OperatingDTM, \"interval\" AS Interval,\n",
    "    (hist_temp_the_woodlands_tx + hist_temp_katy_tx + hist_temp_friendswood_tx + hist_temp_baytown_tx + hist_temp_houston_tx)/5.0 AS temp_avg,\n",
    "    (hist_hum_the_woodlands_tx  + hist_hum_katy_tx  + hist_hum_friendswood_tx  + hist_hum_baytown_tx  + hist_hum_houston_tx)/5.0  AS hum_avg,\n",
    "    GREATEST(hist_temp_the_woodlands_tx, hist_temp_katy_tx, hist_temp_friendswood_tx, hist_temp_baytown_tx, hist_temp_houston_tx)\n",
    "      - LEAST(hist_temp_the_woodlands_tx, hist_temp_katy_tx, hist_temp_friendswood_tx, hist_temp_baytown_tx, hist_temp_houston_tx) AS temp_spread,\n",
    "    GREATEST(hist_hum_the_woodlands_tx, hist_hum_katy_tx, hist_hum_friendswood_tx, hist_hum_baytown_tx, hist_hum_houston_tx)\n",
    "      - LEAST(hist_hum_the_woodlands_tx, hist_hum_katy_tx, hist_hum_friendswood_tx, hist_hum_baytown_tx, hist_hum_houston_tx) AS hum_spread\n",
    "  FROM vw_historical_weather_by_city\n",
    ")\n",
    "SELECT\n",
    "  a.*,\n",
    "  a.temp_avg - LAG(a.temp_avg) OVER (ORDER BY OperatingDTM, Interval) AS temp_avg_ramp1,\n",
    "  a.hum_avg  - LAG(a.hum_avg)  OVER (ORDER BY OperatingDTM, Interval) AS hum_avg_ramp1\n",
    "FROM agg a;\n",
    "\"\"\")\n",
    "con_ensure.close()\n",
    "\n",
    "# Signed log transforms for spikes\n",
    "sgn = np.sign\n",
    "abs_ = np.abs\n",
    "\n",
    "def sgn_log1p(y):\n",
    "    return sgn(y) * np.log1p(abs_(y))\n",
    "\n",
    "def sgn_expm1(z):\n",
    "    return sgn(z) * np.expm1(abs_(z))\n",
    "\n",
    "# Hour-of-day weights (emphasize 17-21 local hours)\n",
    "HOUR_WEIGHTS = np.ones(24, dtype=np.float32)\n",
    "# Reweight hours to reduce early overshoot and emphasize true peak\n",
    "# index 0->H1, ..., 23->H24\n",
    "HOUR_WEIGHTS[:] = 1.0\n",
    "HOUR_WEIGHTS[15] = 1.5     # H16\n",
    "HOUR_WEIGHTS[16:18] = 2.0  # H17-18\n",
    "HOUR_WEIGHTS[18:20] = 5.0  # H19-20 (peak focus)\n",
    "HOUR_WEIGHTS[20] = 3.0     # H21\n",
    "\n",
    "# ---------- pull data ----------\n",
    "con = duckdb.connect(DB)\n",
    "df_lags = con.execute(\"\"\"\n",
    "SELECT\n",
    "  ts, OperatingDTM, Interval, hb_houston,\n",
    "  p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "  dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "FROM vw_master_spine_lags\n",
    "ORDER BY ts\n",
    "\"\"\").df()\n",
    "df_lags[\"ts\"] = pd.to_datetime(df_lags[\"ts\"])\n",
    "\n",
    "df_future = con.execute(\"\"\"\n",
    "SELECT\n",
    "  f.OperatingDTM AS delivery_date,\n",
    "  f.Interval     AS delivery_interval,\n",
    "  f.hb_houston   AS target,\n",
    "  -- base future features\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "  f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f,\n",
    "  f.cal_sin_dow  AS cal_sin_dow_f,  f.cal_cos_dow  AS cal_cos_dow_f,\n",
    "  -- load-enhanced joins\n",
    "  l.net_wz, l.net_lz, l.net_wz_ramp1, l.net_lz_ramp1, l.lz_houston_ramp1, l.wz_spread, l.lz_spread,\n",
    "  -- weather: COALESCE forecast -> historical\n",
    "  COALESCE(wf.temp_avg,      wh.temp_avg)      AS temp_avg,\n",
    "  COALESCE(wf.temp_spread,   wh.temp_spread)   AS temp_spread,\n",
    "  COALESCE(wf.temp_avg_ramp1,wh.temp_avg_ramp1)AS temp_avg_ramp1,\n",
    "  COALESCE(wf.hum_avg,       wh.hum_avg)       AS hum_avg,\n",
    "  COALESCE(wf.hum_spread,    wh.hum_spread)    AS hum_spread,\n",
    "  COALESCE(wf.hum_avg_ramp1, wh.hum_avg_ramp1) AS hum_avg_ramp1,\n",
    "  CASE WHEN wf.temp_avg IS NULL THEN 1 ELSE 0 END AS is_weather_proxy,\n",
    "  -- additional ramps & day-over-day deltas to help with peaks (computed over time order)\n",
    "  (l.net_wz      - LAG(l.net_wz,      3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_ramp3,\n",
    "  (l.net_wz      - LAG(l.net_wz,      6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_ramp6,\n",
    "  (l.net_lz      - LAG(l.net_lz,      3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_ramp3,\n",
    "  (l.net_lz      - LAG(l.net_lz,      6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_ramp6,\n",
    "  (l.lz_houston  - LAG(l.lz_houston,  3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_ramp3,\n",
    "  (l.lz_houston  - LAG(l.lz_houston,  6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_ramp6,\n",
    "  (l.net_wz      - LAG(l.net_wz,     24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_dod,\n",
    "  (l.net_lz      - LAG(l.net_lz,     24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_dod,\n",
    "  (l.lz_houston  - LAG(l.lz_houston, 24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_dod\n",
    "FROM vw_master_spine_ts f\n",
    "LEFT JOIN load_fcst_enh l\n",
    "  ON l.OperatingDTM = f.OperatingDTM AND l.Interval = f.Interval\n",
    "LEFT JOIN wx_fcst_enh  wf\n",
    "  ON wf.OperatingDTM = f.OperatingDTM AND wf.Interval = f.Interval\n",
    "LEFT JOIN wx_hist_enh  wh\n",
    "  ON wh.OperatingDTM = f.OperatingDTM AND wh.Interval = f.Interval\n",
    "ORDER BY f.OperatingDTM, f.Interval\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "df_future[\"delivery_date\"] = pd.to_datetime(df_future[\"delivery_date\"]).dt.date\n",
    "df_future[\"delivery_interval\"] = df_future[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# ---------- assemble samples (day-ahead) ----------\n",
    "enc_cols = [\n",
    "    \"hb_houston\",\"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"\n",
    "]\n",
    "dec_future_cols = [\n",
    "    \"cal_hour_f\",\"cal_dow_f\",\"cal_is_weekend_f\",\"cal_sin_hour_f\",\"cal_cos_hour_f\",\"cal_sin_dow_f\",\"cal_cos_dow_f\",\n",
    "    \"net_wz\",\"net_lz\",\"net_wz_ramp1\",\"net_lz_ramp1\",\"lz_houston_ramp1\",\"wz_spread\",\"lz_spread\",\n",
    "    \"temp_avg\",\"temp_spread\",\"temp_avg_ramp1\",\"hum_avg\",\"hum_spread\",\"hum_avg_ramp1\",\n",
    "    # NEW: multi-horizon ramps + day-over-day deltas\n",
    "    \"net_wz_ramp3\",\"net_wz_ramp6\",\"net_lz_ramp3\",\"net_lz_ramp6\",\n",
    "    \"lz_houston_ramp3\",\"lz_houston_ramp6\",\"net_wz_dod\",\"net_lz_dod\",\"lz_houston_dod\",\n",
    "    # Identify when weather came from historical backfill instead of forecast\n",
    "    \"is_weather_proxy\"\n",
    "]\n",
    "\n",
    "# base rows = end-of-day with price\n",
    "base_rows = df_lags[(df_lags[\"hb_houston\"].notna()) & (df_lags[\"Interval\"]==24)][[\"ts\",\"OperatingDTM\",\"hb_houston\"]].copy()\n",
    "base_rows[\"base_date\"] = pd.to_datetime(base_rows[\"OperatingDTM\"]).dt.date\n",
    "base_rows[\"delivery_date\"] = base_rows[\"base_date\"] + dt.timedelta(days=1)\n",
    "\n",
    "# only use delivery days with 24 actuals for training/val\n",
    "has24 = df_future.groupby(\"delivery_date\")[\"target\"].apply(lambda s: s.notna().sum()==24)\n",
    "valid_delivery_dates = set(has24[has24].index)\n",
    "\n",
    "df_lags_idx = df_lags.set_index(\"ts\").sort_index()\n",
    "\n",
    "samples = []\n",
    "for _, r in base_rows.iterrows():\n",
    "    ddate = r[\"delivery_date\"]\n",
    "    ts0 = r[\"ts\"]\n",
    "    # encoder window\n",
    "    start = ts0 - pd.Timedelta(hours=W_PAST-1)\n",
    "    enc_df = df_lags_idx.loc[start:ts0][enc_cols]\n",
    "    if enc_df.shape[0] != W_PAST: \n",
    "        continue\n",
    "    if enc_df.isna().any().any():\n",
    "        continue  # strict: skip any encoder NaNs\n",
    "    # decoder future rows\n",
    "    fdf = df_future[(df_future[\"delivery_date\"]==ddate)].sort_values(\"delivery_interval\")\n",
    "    if fdf.shape[0] != H_FUT:\n",
    "        continue\n",
    "    X_enc = enc_df.values.astype(np.float32)\n",
    "    X_dec = fdf[dec_future_cols].values.astype(np.float32)\n",
    "    y     = fdf[\"target\"].values.astype(np.float32)\n",
    "    y0    = np.float32(r[\"hb_houston\"])\n",
    "    samples.append((X_enc, X_dec, y, y0, ddate))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples assembled; check data coverage or reduce W_PAST.\")\n",
    "\n",
    "# split by date\n",
    "dates = sorted({s[4] for s in samples})\n",
    "cut_date = dates[-1] - dt.timedelta(days=HOLDOUT_DAYS) if len(dates)>HOLDOUT_DAYS else dates[0]\n",
    "train_idx = [i for i,s in enumerate(samples) if s[4] <= cut_date and s[4] in valid_delivery_dates]\n",
    "val_idx   = [i for i,s in enumerate(samples) if s[4] >  cut_date and s[4] in valid_delivery_dates]\n",
    "\n",
    "def stack(idxs):\n",
    "    Xe = np.stack([samples[i][0] for i in idxs], axis=0)\n",
    "    Xd = np.stack([samples[i][1] for i in idxs], axis=0)\n",
    "    Y  = np.stack([samples[i][2] for i in idxs], axis=0)\n",
    "    Y0 = np.stack([samples[i][3] for i in idxs], axis=0)\n",
    "    return Xe, Xd, Y, Y0\n",
    "\n",
    "Xe_tr, Xd_tr, Y_tr, Y0_tr = stack(train_idx)\n",
    "Xe_va, Xd_va, Y_va, Y0_va = (None, None, None, None)\n",
    "if len(val_idx) > 0:\n",
    "    Xe_va, Xd_va, Y_va, Y0_va = stack(val_idx)\n",
    "\n",
    "# ---------- diagnose + drop all-NaN decoder features, then impute (robust) ----------\n",
    "# Flatten decoder train to 2D to inspect per-feature missingness\n",
    "F_orig = Xd_tr.shape[2]\n",
    "Xd_tr_2d_raw = Xd_tr.reshape(-1, F_orig)\n",
    "nan_counts = np.isnan(Xd_tr_2d_raw).sum(axis=0)\n",
    "all_nan_cols = nan_counts == Xd_tr_2d_raw.shape[0]\n",
    "\n",
    "if all_nan_cols.any():\n",
    "    dropped_cols = [dec_future_cols[i] for i,flag in enumerate(all_nan_cols) if flag]\n",
    "    print(f\"Dropping {int(all_nan_cols.sum())} decoder feature(s) with all-NaN in TRAIN: {dropped_cols}\")\n",
    "else:\n",
    "    dropped_cols = []\n",
    "\n",
    "# Keep mask to be reused at prediction time\n",
    "dec_keep_mask = ~all_nan_cols\n",
    "\n",
    "# Apply mask to decoder tensors\n",
    "Xd_tr = Xd_tr[:, :, dec_keep_mask]\n",
    "if Xe_va is not None:\n",
    "    Xd_va = Xd_va[:, :, dec_keep_mask]\n",
    "\n",
    "# Recompute medians on KEPT features\n",
    "dec_med = np.nanmedian(Xd_tr.reshape(-1, Xd_tr.shape[2]), axis=0)\n",
    "# Guard: if a median is still NaN for some reason, set to 0\n",
    "dec_med = np.where(np.isnan(dec_med), 0.0, dec_med)\n",
    "\n",
    "# Encoder median (rare but safe)\n",
    "enc_med = np.nanmedian(Xe_tr.reshape(-1, Xe_tr.shape[2]), axis=0)\n",
    "enc_med = np.where(np.isnan(enc_med), 0.0, enc_med)\n",
    "\n",
    "# Impute in-place (3D-safe)\n",
    "impute_inplace_3d(Xe_tr, enc_med)\n",
    "impute_inplace_3d(Xd_tr, dec_med)\n",
    "if Xe_va is not None:\n",
    "    impute_inplace_3d(Xe_va, enc_med)\n",
    "    impute_inplace_3d(Xd_va, dec_med)\n",
    "\n",
    "# Targets should be complete for training/val; assert\n",
    "assert np.isfinite(Y_tr).all(), \"Y_tr has non-finite values\"\n",
    "if Y_va is not None:\n",
    "    assert np.isfinite(Y_va).all(), \"Y_va has non-finite values\"\n",
    "\n",
    "# ---------- safe standardization ----------\n",
    "enc_mu, enc_sd = fit_standardizer(Xe_tr.reshape(-1, Xe_tr.shape[2]))\n",
    "dec_mu, dec_sd = fit_standardizer(Xd_tr.reshape(-1, Xd_tr.shape[2]))\n",
    "# ---- target transform (spike-aware) then standardize ----\n",
    "Y_tr_t  = sgn_log1p(Y_tr)\n",
    "y_mu, y_sd = fit_standardizer(Y_tr_t.reshape(-1,1))\n",
    "y_mu = float(np.atleast_1d(y_mu)[0]); y_sd = float(np.atleast_1d(y_sd)[0])\n",
    "\n",
    "Xe_tr_n = apply_standardizer(Xe_tr, enc_mu, enc_sd)\n",
    "Xd_tr_n = apply_standardizer(Xd_tr, dec_mu, dec_sd)\n",
    "Y_tr_n  = apply_standardizer(Y_tr_t, y_mu, y_sd).reshape(Y_tr.shape)\n",
    "Y0_tr_t = sgn_log1p(Y0_tr.reshape(-1,1)).reshape(-1,1)\n",
    "Y0_tr_n = apply_standardizer(Y0_tr_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = apply_standardizer(Xe_va, enc_mu, enc_sd)\n",
    "    Xd_va_n = apply_standardizer(Xd_va, dec_mu, dec_sd)\n",
    "    Y_va_t  = sgn_log1p(Y_va)\n",
    "    Y_va_n  = apply_standardizer(Y_va_t,  y_mu,  y_sd).reshape(Y_va.shape)\n",
    "    Y0_va_t = sgn_log1p(Y0_va.reshape(-1,1))\n",
    "    Y0_va_n = apply_standardizer(Y0_va_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "\n",
    "# Final safety: ensure everything is finite\n",
    "assert_finite(\"Xe_tr_n\", Xe_tr_n)\n",
    "assert_finite(\"Xd_tr_n\", Xd_tr_n)\n",
    "assert_finite(\"Y_tr_n\",  Y_tr_n)\n",
    "assert_finite(\"Y0_tr_n\", Y0_tr_n)\n",
    "if Xe_va is not None:\n",
    "    assert_finite(\"Xe_va_n\", Xe_va_n)\n",
    "    assert_finite(\"Xd_va_n\", Xd_va_n)\n",
    "    assert_finite(\"Y_va_n\",  Y_va_n)\n",
    "    assert_finite(\"Y0_va_n\", Y0_va_n)\n",
    "\n",
    "# ---------- torch datasets ----------\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xe, Xd, Y, Y0):\n",
    "        self.Xe = torch.from_numpy(Xe).float()\n",
    "        self.Xd = torch.from_numpy(Xd).float()\n",
    "        self.Y  = torch.from_numpy(Y).float()\n",
    "        self.Y0 = torch.from_numpy(Y0).float()\n",
    "    def __len__(self): return self.Xe.shape[0]\n",
    "    def __getitem__(self, i): return self.Xe[i], self.Xd[i], self.Y[i], self.Y0[i]\n",
    "\n",
    "bs = 32\n",
    "train_loader = torch.utils.data.DataLoader(Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=bs, shuffle=True)\n",
    "val_loader = None if Xe_va is None else torch.utils.data.DataLoader(Seq2SeqDataset(Xe_va_n, Xd_va_n, Y_va_n, Y0_va_n), batch_size=bs, shuffle=False)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim, hidden, batch_first=True)\n",
    "    def forward(self, x):  # x: (B,W,E)\n",
    "        _, h = self.rnn(x)\n",
    "        return h  # (1,B,H)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim + 1, hidden, batch_first=True)  # + prev y\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "    def forward(self, future_feats, h0, y0, teacher=None, tf_prob=0.5):\n",
    "        B, T, D = future_feats.shape\n",
    "        y_prev = y0.view(B,1)\n",
    "        h = h0\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            x_t = torch.cat([future_feats[:,t,:], y_prev], dim=1).unsqueeze(1)  # (B,1,D+1)\n",
    "            o, h = self.rnn(x_t, h)\n",
    "            y_t = self.out(o[:, -1, :]).squeeze(1)\n",
    "            outs.append(y_t)\n",
    "            if (teacher is not None) and (rng.random() < tf_prob):\n",
    "                y_prev = teacher[:,t].view(B,1)\n",
    "            else:\n",
    "                y_prev = y_t.view(B,1)\n",
    "        return torch.stack(outs, dim=1)  # (B,24)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(enc_in, hidden)\n",
    "        self.dec = Decoder(dec_in, hidden)\n",
    "    def forward(self, x_enc, x_dec, y0, y_teacher=None, tf_prob=0.5):\n",
    "        h = self.enc(x_enc)\n",
    "        return self.dec(x_dec, h, y0, y_teacher, tf_prob)\n",
    "\n",
    "enc_in = Xe_tr_n.shape[2]\n",
    "dec_in = Xd_tr_n.shape[2]\n",
    "model = Seq2Seq(enc_in, dec_in, HIDDEN).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "def huber_weighted(yhat, y, w, delta=1.0):\n",
    "    d = torch.abs(yhat - y)\n",
    "    q = torch.clamp(d, max=delta)\n",
    "    l = 0.5 * q**2 + delta * (d - q)\n",
    "    return (l * w).mean()\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in train_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=y, tf_prob=0.5)\n",
    "        # time-step weights (B,24)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    if val_loader is None: return None\n",
    "    model.eval()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(val_loader.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    print(f\"Epoch {ep:02d}  train_loss={tr:.4f}\" + (\"\" if va is None else f\"  val_loss={va:.4f}\"))\n",
    "\n",
    "# ---- evaluate holdout MAE in real units (SAFE) ----\n",
    "if val_loader is not None:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xenc, xdec, y, y0 in val_loader:\n",
    "            xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "            yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "            y_n    = y.detach().cpu().numpy()\n",
    "            # invert scaling + inverse spike transform\n",
    "            yhat_t = (yhat_n * y_sd + y_mu)\n",
    "            yt_t   = (y_n    * y_sd + y_mu)\n",
    "            yhat = sgn_expm1(yhat_t).reshape(-1)\n",
    "            yt   = sgn_expm1(yt_t).reshape(-1)\n",
    "            preds.append(yhat)\n",
    "            trues.append(yt)\n",
    "    preds = np.concatenate(preds)\n",
    "    trues = np.concatenate(trues)\n",
    "    mask = np.isfinite(preds) & np.isfinite(trues)\n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped:\n",
    "        print(f\"Dropped {dropped} non-finite pairs from holdout scoring.\")\n",
    "    print({\"holdout_MAE\": float(mean_absolute_error(trues[mask], preds[mask]))})\n",
    "\n",
    "# ---- optional LightGBM residual booster (train on TRAIN residuals) ----\n",
    "USE_LGBM_RESIDUAL = True\n",
    "booster = None\n",
    "if USE_LGBM_RESIDUAL:\n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        # Build a non-shuffled loader to preserve order\n",
    "        train_loader_eval = torch.utils.data.DataLoader(\n",
    "            Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=64, shuffle=False)\n",
    "        base_preds = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for xenc, xdec, y, y0 in train_loader_eval:\n",
    "                xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "                yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "                base_preds.append(yhat_n)\n",
    "        base_preds = np.concatenate(base_preds, axis=0)  # (N_train, 24)\n",
    "        # inverse transforms -> true $/MWh\n",
    "        base_preds_t = (base_preds * y_sd + y_mu)\n",
    "        base_preds_true = sgn_expm1(base_preds_t)\n",
    "        y_true_tr = Y_tr  # already true units\n",
    "        # flatten\n",
    "        y_base_flat = base_preds_true.reshape(-1)\n",
    "        y_true_flat = y_true_tr.reshape(-1)\n",
    "        X_flat = Xd_tr.reshape(-1, Xd_tr.shape[2])  # kept raw decoder features\n",
    "        mask = np.isfinite(y_base_flat) & np.isfinite(y_true_flat) & np.isfinite(X_flat).all(axis=1)\n",
    "        X_flat = X_flat[mask]\n",
    "        y_resid_flat = (y_true_flat - y_base_flat)[mask]\n",
    "        # Fit booster\n",
    "        booster = lgb.LGBMRegressor(\n",
    "            n_estimators=600, learning_rate=0.03, subsample=0.8, colsample_bytree=0.8,\n",
    "            max_depth=-1, reg_alpha=0.0, reg_lambda=0.0, min_child_samples=20, random_state=42)\n",
    "        booster.fit(X_flat, y_resid_flat)\n",
    "        print(\"Residual booster trained (LightGBM).\")\n",
    "    except Exception as e:\n",
    "        print(f\"LightGBM residual booster skipped: {e}\")\n",
    "\n",
    "# ---- predict requested DELIVERY_DATE ----\n",
    "\n",
    "# find base row (previous day EOD with price)\n",
    "prev_day = DELIVERY_DATE - dt.timedelta(days=1)\n",
    "base_row = df_lags[(pd.to_datetime(df_lags[\"OperatingDTM\"]).dt.date == prev_day) & (df_lags[\"Interval\"]==24) & df_lags[\"hb_houston\"].notna()]\n",
    "if base_row.empty:\n",
    "    raise RuntimeError(f\"No base EOD price for {prev_day}\")\n",
    "ts0 = pd.to_datetime(base_row.iloc[0][\"ts\"])\n",
    "y0  = np.float32(base_row.iloc[0][\"hb_houston\"])\n",
    "\n",
    "# encoder window\n",
    "win = df_lags.set_index(\"ts\").loc[ts0 - pd.Timedelta(hours=W_PAST-1): ts0][enc_cols]\n",
    "if win.shape[0] != W_PAST or win.isna().any().any():\n",
    "    raise RuntimeError(\"Insufficient/NaN history for encoder window.\")\n",
    "Xe_pred = win.values.astype(np.float32)[None, ...]\n",
    "\n",
    "# decoder future features (24 rows)\n",
    "frows = df_future[(df_future[\"delivery_date\"]==DELIVERY_DATE)].sort_values(\"delivery_interval\")\n",
    "if frows.shape[0] != H_FUT:\n",
    "    raise RuntimeError(\"Spine missing 24 rows for delivery date.\")\n",
    "Xd_pred_full = frows[dec_future_cols].values.astype(np.float32)[None, ...]\n",
    "\n",
    "# Apply the same keep mask used in training (drop columns that were all-NaN in train)\n",
    "if 'dec_keep_mask' in globals():\n",
    "    Xd_pred = Xd_pred_full[:, :, dec_keep_mask]\n",
    "else:\n",
    "    Xd_pred = Xd_pred_full\n",
    "\n",
    "# impute + standardize pred using TRAIN stats\n",
    "m = np.isnan(Xd_pred)\n",
    "if m.any():\n",
    "    Xd_pred[m] = np.broadcast_to(dec_med, Xd_pred.shape)[m]\n",
    "Xe_pred_n = apply_standardizer(Xe_pred, enc_mu, enc_sd)\n",
    "Xd_pred_n = apply_standardizer(Xd_pred, dec_mu, dec_sd)\n",
    "# compute normalized scalar y0_n using spike-aware transform\n",
    "y0_t = sgn_log1p(y0)\n",
    "y0_n = np.float32(((y0_t - y_mu) / y_sd)).ravel()[0]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_n = model(torch.from_numpy(Xe_pred_n).to(DEVICE),\n",
    "                   torch.from_numpy(Xd_pred_n).to(DEVICE),\n",
    "                   torch.tensor([y0_n], dtype=torch.float32, device=DEVICE),\n",
    "                   y_teacher=None, tf_prob=0.0).detach().cpu().numpy()[0]\n",
    "yhat_t = (yhat_n * y_sd + y_mu).reshape(-1)\n",
    "yhat = sgn_expm1(yhat_t)\n",
    "# --- optional: add residual booster only on peak hours (H18â€“H21) ---\n",
    "if booster is not None:\n",
    "    Xd_pred_kept = Xd_pred.copy()  # (1,24,F_kept)\n",
    "    resid_pred = booster.predict(Xd_pred_kept.reshape(24, Xd_pred_kept.shape[2]))\n",
    "    hour_mask = np.zeros(24, dtype=np.float32)\n",
    "    hour_mask[17:21] = 1.0  # apply only to intervals 18,19,20,21\n",
    "    yhat = yhat + resid_pred * hour_mask\n",
    "\n",
    "# replace any residual NaN with the day's median\n",
    "if np.isnan(yhat).any():\n",
    "    day_med = float(np.nanmedian(yhat))\n",
    "    yhat = np.nan_to_num(yhat, nan=day_med)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*H_FUT,\n",
    "    \"Interval\": list(range(1, H_FUT+1)),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "})\n",
    "forecast\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
