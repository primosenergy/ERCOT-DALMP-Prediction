{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf10bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables: [('LoadForecast',), ('LoadForecastPivot',), ('Prices',), ('SessionID',), ('WeatherFcst',), ('WeatherHist',)]\n",
      "Empty DataFrame\n",
      "Columns: [timestamp, load]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "BinderException",
     "evalue": "Binder Error: Referenced column \"OperatingDTM\" not found in FROM clause!\nCandidate bindings: \"load\"\n\nLINE 4:     WHERE OperatingDTM = '2025-08-16'\n                  ^",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBinderException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Example: filter by date\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m df2 = \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[33;43m    SELECT *\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[33;43m    FROM LoadForecast\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[33;43m    WHERE OperatingDTM = \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2025-08-16\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[33;43m    ORDER BY Interval\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m.df()\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(df2)\n\u001b[32m     25\u001b[39m con.close()\n",
      "\u001b[31mBinderException\u001b[39m: Binder Error: Referenced column \"OperatingDTM\" not found in FROM clause!\nCandidate bindings: \"load\"\n\nLINE 4:     WHERE OperatingDTM = '2025-08-16'\n                  ^"
     ]
    }
   ],
   "source": [
    "\n",
    "import duckdb\n",
    "\n",
    "# Path to your DB file\n",
    "DB_PATH = \"../ProjectMain/db/data.duckdb\"\n",
    "\n",
    "con = duckdb.connect(DB_PATH)\n",
    "\n",
    "# List tables\n",
    "print(\"Tables:\", con.execute(\"SHOW TABLES\").fetchall())\n",
    "\n",
    "# Look at the first few rows\n",
    "df = con.execute(\"SELECT * FROM LoadForecast LIMIT 10\").df()\n",
    "print(df)\n",
    "\n",
    "# Example: filter by date\n",
    "df2 = con.execute(\"\"\"\n",
    "    SELECT *\n",
    "    FROM LoadForecast\n",
    "    WHERE DATE(OperatingDTM) = '2025-08-16'\n",
    "    ORDER BY Interval\n",
    "\"\"\").df()\n",
    "\n",
    "print(df2)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c9c9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LoadForecast',), ('LoadForecastPivot',), ('Prices',), ('SessionID',), ('WeatherFcst',), ('WeatherHist',)]\n",
      "   cid       name       type  notnull dflt_value     pk\n",
      "0    0  timestamp  TIMESTAMP    False       None  False\n",
      "1    1       load     DOUBLE    False       None  False\n",
      "[(0,)]\n"
     ]
    },
    {
     "ename": "BinderException",
     "evalue": "Binder Error: Referenced column \"OperatingDTM\" not found in FROM clause!\nCandidate bindings: \"load\"\n\nLINE 2:   SELECT OperatingDTM, Interval, Location, ForecastMW\n                 ^",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBinderException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(con.execute(\u001b[33m\"\u001b[39m\u001b[33mPRAGMA table_info(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLoadForecast\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m).df())\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(con.execute(\u001b[33m\"\u001b[39m\u001b[33mSELECT COUNT(*) FROM LoadForecast\u001b[39m\u001b[33m\"\u001b[39m).fetchall())\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[33;43m  SELECT OperatingDTM, Interval, Location, ForecastMW\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[33;43m  FROM LoadForecast\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[33;43m  ORDER BY OperatingDTM, Interval, Location\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[33;43m  LIMIT 10\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m.df())\n\u001b[32m     12\u001b[39m con.close()\n",
      "\u001b[31mBinderException\u001b[39m: Binder Error: Referenced column \"OperatingDTM\" not found in FROM clause!\nCandidate bindings: \"load\"\n\nLINE 2:   SELECT OperatingDTM, Interval, Location, ForecastMW\n                 ^"
     ]
    }
   ],
   "source": [
    "\n",
    "import duckdb\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "print(con.execute(\"SHOW TABLES\").fetchall())\n",
    "print(con.execute(\"PRAGMA table_info('LoadForecast')\").df())\n",
    "print(con.execute(\"SELECT COUNT(*) FROM LoadForecast\").fetchall())\n",
    "print(con.execute(\"\"\"\n",
    "  SELECT OperatingDTM, Interval, Location, ForecastMW\n",
    "  FROM LoadForecast\n",
    "  ORDER BY OperatingDTM, Interval, Location\n",
    "  LIMIT 10\n",
    "\"\"\").df())\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2133442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<duckdb.duckdb.DuckDBPyConnection object at 0x0000023B211D7070>\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "\n",
    "print(con.execute(\"\"\"\n",
    "  DROP TABLE LoadForecast;\n",
    "\"\"\"))\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a3ef3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS \"LoadForecast\" (\n",
    "                OperatingDTM TIMESTAMP,\n",
    "                Interval     INTEGER,\n",
    "                Month        TEXT,\n",
    "                LocationType TEXT,\n",
    "                Location     TEXT,\n",
    "                DSTFlag      TEXT,\n",
    "                ForecastMW   DOUBLE\n",
    "            );\n",
    "        \"\"\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40f2f649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                timestamp  interval      lat      lon  temperature  humidity  \\\n",
      "0     2022-12-31 19:00:00        19  29.7604 -95.3698         22.0      47.0   \n",
      "1     2022-12-31 20:00:00        20  29.7604 -95.3698         21.0      53.0   \n",
      "2     2022-12-31 21:00:00        21  29.7604 -95.3698         19.0      60.0   \n",
      "3     2022-12-31 22:00:00        22  29.7604 -95.3698         18.0      78.0   \n",
      "4     2022-12-31 23:00:00        23  29.7604 -95.3698         17.0      88.0   \n",
      "...                   ...       ...      ...      ...          ...       ...   \n",
      "23013 2024-01-03 15:00:00        15  29.7604 -95.3698         13.0      67.0   \n",
      "23014 2024-01-03 16:00:00        16  29.7604 -95.3698         13.0      67.0   \n",
      "23015 2024-01-03 17:00:00        17  29.7604 -95.3698         13.0      67.0   \n",
      "23016 2024-01-03 18:00:00        18  29.7604 -95.3698         13.0      67.0   \n",
      "23017 2024-01-03 19:00:00        19  29.7604 -95.3698         13.0      67.0   \n",
      "\n",
      "       windspeed  precipitation     source  \n",
      "0       3.611111            0.0  meteostat  \n",
      "1       4.166667            0.0  meteostat  \n",
      "2       3.611111            0.0  meteostat  \n",
      "3       6.111111            0.0  meteostat  \n",
      "4       5.555556            0.0  meteostat  \n",
      "...          ...            ...        ...  \n",
      "23013   4.166667            0.0  meteostat  \n",
      "23014   4.722222            0.0  meteostat  \n",
      "23015   5.277778            0.0  meteostat  \n",
      "23016   2.500000            0.0  meteostat  \n",
      "23017   4.166667            0.0  meteostat  \n",
      "\n",
      "[23018 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"SELECT * FROM historical_weather\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "825babe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [lat, lon, missing_timestamp]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"WITH bounds AS (\n",
    "  SELECT\n",
    "    lat, lon,\n",
    "    date_trunc('hour', min(timestamp)) AS start_ts,\n",
    "    date_trunc('hour', max(timestamp)) AS end_ts\n",
    "  FROM historical_weather\n",
    "  GROUP BY lat, lon\n",
    "),\n",
    "expected AS (\n",
    "  SELECT b.lat, b.lon, gs.ts\n",
    "  FROM bounds b\n",
    "  CROSS JOIN LATERAL generate_series(b.start_ts, b.end_ts, INTERVAL 1 HOUR) AS gs(ts)\n",
    "),\n",
    "normalized_hw AS (\n",
    "  SELECT DISTINCT lat, lon, date_trunc('hour', timestamp) AS ts_hour\n",
    "  FROM historical_weather\n",
    ")\n",
    "SELECT e.lat, e.lon, e.ts AS missing_timestamp\n",
    "FROM expected e\n",
    "LEFT JOIN normalized_hw n\n",
    "  ON e.lat = n.lat AND e.lon = n.lon AND e.ts = n.ts_hour\n",
    "WHERE n.ts_hour IS NULL\n",
    "ORDER BY e.lat, e.lon, e.ts;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c3232",
   "metadata": {},
   "source": [
    "Backfill missing historic weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f55f3492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count\n",
      "0      0\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"WITH bounds AS (\n",
    "  SELECT\n",
    "    lat, lon,\n",
    "    date_trunc('hour', MIN(timestamp)) AS start_ts,\n",
    "    date_trunc('hour', MAX(timestamp)) AS end_ts\n",
    "  FROM historical_weather\n",
    "  GROUP BY lat, lon\n",
    "),\n",
    "expected AS (\n",
    "  SELECT b.lat, b.lon, gs.ts\n",
    "  FROM bounds b\n",
    "  CROSS JOIN LATERAL generate_series(b.start_ts, b.end_ts, INTERVAL 1 HOUR) AS gs(ts)\n",
    "),\n",
    "normalized_hw AS (\n",
    "  SELECT DISTINCT lat, lon, date_trunc('hour', timestamp) AS ts_hour\n",
    "  FROM historical_weather\n",
    "),\n",
    "missing_only AS (\n",
    "  SELECT e.lat, e.lon, e.ts AS missing_ts\n",
    "  FROM expected e\n",
    "  LEFT JOIN normalized_hw n\n",
    "    ON e.lat = n.lat AND e.lon = n.lon AND e.ts = n.ts_hour\n",
    "  WHERE n.ts_hour IS NULL\n",
    "),\n",
    "nearest_before AS (\n",
    "  SELECT\n",
    "    mo.lat,\n",
    "    mo.lon,\n",
    "    mo.missing_ts,\n",
    "    MAX(h.timestamp) AS prev_ts\n",
    "  FROM missing_only mo\n",
    "  JOIN historical_weather h\n",
    "    ON h.lat = mo.lat\n",
    "   AND h.lon = mo.lon\n",
    "   AND h.timestamp < mo.missing_ts\n",
    "  GROUP BY mo.lat, mo.lon, mo.missing_ts\n",
    "),\n",
    "nearest_after AS (\n",
    "  SELECT\n",
    "    mo.lat,\n",
    "    mo.lon,\n",
    "    mo.missing_ts,\n",
    "    MIN(h.timestamp) AS next_ts\n",
    "  FROM missing_only mo\n",
    "  JOIN historical_weather h\n",
    "    ON h.lat = mo.lat\n",
    "   AND h.lon = mo.lon\n",
    "   AND h.timestamp > mo.missing_ts\n",
    "  GROUP BY mo.lat, mo.lon, mo.missing_ts\n",
    "),\n",
    "interp AS (\n",
    "  SELECT\n",
    "    b.lat,\n",
    "    b.lon,\n",
    "    b.missing_ts,\n",
    "    'meteostat_interp' AS source,\n",
    "    /* ratio of how far missing_ts is between prev_ts and next_ts, in seconds */\n",
    "    CAST(DATEDIFF('second', b.prev_ts, b.missing_ts) AS DOUBLE)\n",
    "      / NULLIF(CAST(DATEDIFF('second', b.prev_ts, a.next_ts) AS DOUBLE), 0) AS ratio,\n",
    "    hb.temperature AS temp_prev,\n",
    "    ha.temperature AS temp_next,\n",
    "    hb.humidity    AS hum_prev,\n",
    "    ha.humidity    AS hum_next,\n",
    "    hb.windspeed   AS wind_prev,\n",
    "    ha.windspeed   AS wind_next,\n",
    "    hb.precipitation AS prcp_prev,\n",
    "    ha.precipitation AS prcp_next\n",
    "  FROM nearest_before b\n",
    "  JOIN nearest_after  a\n",
    "    ON b.lat = a.lat AND b.lon = a.lon AND b.missing_ts = a.missing_ts\n",
    "  JOIN historical_weather hb\n",
    "    ON hb.lat = b.lat AND hb.lon = b.lon AND hb.timestamp = b.prev_ts\n",
    "  JOIN historical_weather ha\n",
    "    ON ha.lat = a.lat AND ha.lon = a.lon AND ha.timestamp = a.next_ts\n",
    ")\n",
    "INSERT INTO historical_weather (\n",
    "  timestamp, interval, lat, lon, temperature, humidity, windspeed, precipitation, source\n",
    ")\n",
    "SELECT\n",
    "  i.missing_ts AS timestamp,\n",
    "  CASE WHEN EXTRACT(hour FROM i.missing_ts) = 0 THEN 24 ELSE EXTRACT(hour FROM i.missing_ts) END AS interval,\n",
    "  i.lat,\n",
    "  i.lon,\n",
    "  COALESCE(i.temp_prev + (i.temp_next - i.temp_prev) * i.ratio, i.temp_prev) AS temperature,\n",
    "  COALESCE(i.hum_prev  + (i.hum_next  - i.hum_prev)  * i.ratio, i.hum_prev)  AS humidity,\n",
    "  COALESCE(i.wind_prev + (i.wind_next - i.wind_prev) * i.ratio, i.wind_prev) AS windspeed,\n",
    "  COALESCE(i.prcp_prev + (i.prcp_next - i.prcp_prev) * i.ratio, i.prcp_prev) AS precipitation,\n",
    "  'meteostat_interp' AS source\n",
    "FROM interp i\n",
    "WHERE i.ratio IS NOT NULL;\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5991b7e",
   "metadata": {},
   "source": [
    "what was just inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68451299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [timestamp, interval, lat, lon, temperature, humidity, windspeed, precipitation, source]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"SELECT * FROM historical_weather WHERE source = 'meteostat_interp' ORDER BY timestamp, lat, lon LIMIT 20;\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ea75642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       lat      lon operating_date  interval_count\n",
      "0  29.7604 -95.3698     2022-12-31               5\n",
      "1  29.7604 -95.3698     2025-08-16              21\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"SELECT\n",
    "  lat,\n",
    "  lon,\n",
    "  DATE(timestamp) AS operating_date,\n",
    "  COUNT(DISTINCT interval) AS interval_count\n",
    "FROM historical_weather\n",
    "GROUP BY lat, lon, operating_date\n",
    "HAVING COUNT(DISTINCT interval) <> 24\n",
    "ORDER BY operating_date, lat, lon;\n",
    "\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce62c9",
   "metadata": {},
   "source": [
    "what prices we got?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bc4768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       OperatingDTM  TIME    Location   Price\n",
      "0        2023-01-01   900  hb_houston   19.50\n",
      "1        2023-01-01   960     hb_west   16.90\n",
      "2        2023-01-01  1080      hb_pan   33.44\n",
      "3        2023-01-01  1260  hb_houston   16.09\n",
      "4        2023-01-01  1440      hb_pan    5.20\n",
      "...             ...   ...         ...     ...\n",
      "138373   2025-08-18   360   hb_hubavg   29.44\n",
      "138374   2025-08-18   240     hb_west   30.30\n",
      "138375   2025-08-18  1320   hb_hubavg  136.14\n",
      "138376   2025-08-18  1080    hb_north   74.97\n",
      "138377   2025-08-18   660    hb_south   25.22\n",
      "\n",
      "[138378 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"SELECT * FROM prices_hourly\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff818ac4",
   "metadata": {},
   "source": [
    "refreshesr on all the tables and fields available so far for view creation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f613cbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   table_schema          table_name  ordinal_position    column_name  \\\n",
      "0          main        LoadForecast                 1   OperatingDTM   \n",
      "1          main        LoadForecast                 2       Interval   \n",
      "2          main        LoadForecast                 3          Month   \n",
      "3          main        LoadForecast                 4   LocationType   \n",
      "4          main        LoadForecast                 5       Location   \n",
      "5          main        LoadForecast                 6        DSTFlag   \n",
      "6          main        LoadForecast                 7     ForecastMW   \n",
      "7          main   LoadForecastPivot                 1   OperatingDTM   \n",
      "8          main   LoadForecastPivot                 2       Interval   \n",
      "9          main   LoadForecastPivot                 3   LocationType   \n",
      "10         main   LoadForecastPivot                 4       Location   \n",
      "11         main   LoadForecastPivot                 5        DSTFlag   \n",
      "12         main   LoadForecastPivot                 6     ForecastMW   \n",
      "13         main              Prices                 1      timestamp   \n",
      "14         main              Prices                 2            lmp   \n",
      "15         main           SessionID                 1             ts   \n",
      "16         main           SessionID                 2     session_id   \n",
      "17         main         WeatherFcst                 1      timestamp   \n",
      "18         main         WeatherFcst                 2           temp   \n",
      "19         main         WeatherFcst                 3            lat   \n",
      "20         main         WeatherFcst                 4            lon   \n",
      "21         main         WeatherHist                 1      timestamp   \n",
      "22         main         WeatherHist                 2           temp   \n",
      "23         main         WeatherHist                 3            lat   \n",
      "24         main         WeatherHist                 4            lon   \n",
      "25         main    forecast_weather                 1  forecast_time   \n",
      "26         main    forecast_weather                 2      timestamp   \n",
      "27         main    forecast_weather                 3       interval   \n",
      "28         main    forecast_weather                 4            lat   \n",
      "29         main    forecast_weather                 5            lon   \n",
      "30         main    forecast_weather                 6    temperature   \n",
      "31         main    forecast_weather                 7       humidity   \n",
      "32         main    forecast_weather                 8      windspeed   \n",
      "33         main    forecast_weather                 9  precipitation   \n",
      "34         main    forecast_weather                10         source   \n",
      "35         main  historical_weather                 1      timestamp   \n",
      "36         main  historical_weather                 2       interval   \n",
      "37         main  historical_weather                 3            lat   \n",
      "38         main  historical_weather                 4            lon   \n",
      "39         main  historical_weather                 5    temperature   \n",
      "40         main  historical_weather                 6       humidity   \n",
      "41         main  historical_weather                 7      windspeed   \n",
      "42         main  historical_weather                 8  precipitation   \n",
      "43         main  historical_weather                 9         source   \n",
      "44         main       prices_hourly                 1   OperatingDTM   \n",
      "45         main       prices_hourly                 2           TIME   \n",
      "46         main       prices_hourly                 3       Location   \n",
      "47         main       prices_hourly                 4          Price   \n",
      "\n",
      "    data_type is_nullable  \n",
      "0   TIMESTAMP         YES  \n",
      "1     INTEGER         YES  \n",
      "2     VARCHAR         YES  \n",
      "3     VARCHAR         YES  \n",
      "4     VARCHAR         YES  \n",
      "5     VARCHAR         YES  \n",
      "6      DOUBLE         YES  \n",
      "7   TIMESTAMP         YES  \n",
      "8     INTEGER         YES  \n",
      "9     VARCHAR         YES  \n",
      "10    VARCHAR         YES  \n",
      "11    VARCHAR         YES  \n",
      "12     DOUBLE         YES  \n",
      "13  TIMESTAMP         YES  \n",
      "14     DOUBLE         YES  \n",
      "15  TIMESTAMP         YES  \n",
      "16    VARCHAR         YES  \n",
      "17  TIMESTAMP         YES  \n",
      "18     DOUBLE         YES  \n",
      "19     DOUBLE         YES  \n",
      "20     DOUBLE         YES  \n",
      "21  TIMESTAMP         YES  \n",
      "22     DOUBLE         YES  \n",
      "23     DOUBLE         YES  \n",
      "24     DOUBLE         YES  \n",
      "25  TIMESTAMP          NO  \n",
      "26  TIMESTAMP          NO  \n",
      "27    INTEGER         YES  \n",
      "28     DOUBLE          NO  \n",
      "29     DOUBLE          NO  \n",
      "30     DOUBLE         YES  \n",
      "31     DOUBLE         YES  \n",
      "32     DOUBLE         YES  \n",
      "33     DOUBLE         YES  \n",
      "34    VARCHAR         YES  \n",
      "35  TIMESTAMP          NO  \n",
      "36    INTEGER         YES  \n",
      "37     DOUBLE          NO  \n",
      "38     DOUBLE          NO  \n",
      "39     DOUBLE         YES  \n",
      "40     DOUBLE         YES  \n",
      "41     DOUBLE         YES  \n",
      "42     DOUBLE         YES  \n",
      "43    VARCHAR         YES  \n",
      "44  TIMESTAMP         YES  \n",
      "45    VARCHAR         YES  \n",
      "46    VARCHAR         YES  \n",
      "47     DOUBLE         YES  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 SELECT\n",
    "                    c.table_schema,\n",
    "                    c.table_name,\n",
    "                    c.ordinal_position,\n",
    "                    c.column_name,\n",
    "                    c.data_type,\n",
    "                    c.is_nullable\n",
    "                FROM information_schema.columns AS c\n",
    "                  JOIN information_schema.tables  AS t\n",
    "                    ON c.table_schema = t.table_schema\n",
    "                    AND c.table_name   = t.table_name\n",
    "                WHERE t.table_type = 'BASE TABLE'            -- change to IN ('BASE TABLE','VIEW') to include views\n",
    "                    AND c.table_schema NOT IN ('information_schema','pg_catalog')\n",
    "                ORDER BY c.table_schema, c.table_name, c.ordinal_position;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bccf46",
   "metadata": {},
   "source": [
    "top records of all tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9a49d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   table_schema          table_name  ordinal_position    column_name  \\\n",
      "0          main        LoadForecast                 1   OperatingDTM   \n",
      "1          main        LoadForecast                 2       Interval   \n",
      "2          main        LoadForecast                 3          Month   \n",
      "3          main        LoadForecast                 4   LocationType   \n",
      "4          main        LoadForecast                 5       Location   \n",
      "5          main        LoadForecast                 6        DSTFlag   \n",
      "6          main        LoadForecast                 7     ForecastMW   \n",
      "7          main   LoadForecastPivot                 1   OperatingDTM   \n",
      "8          main   LoadForecastPivot                 2       Interval   \n",
      "9          main   LoadForecastPivot                 3   LocationType   \n",
      "10         main   LoadForecastPivot                 4       Location   \n",
      "11         main   LoadForecastPivot                 5        DSTFlag   \n",
      "12         main   LoadForecastPivot                 6     ForecastMW   \n",
      "13         main              Prices                 1      timestamp   \n",
      "14         main              Prices                 2            lmp   \n",
      "15         main           SessionID                 1             ts   \n",
      "16         main           SessionID                 2     session_id   \n",
      "17         main         WeatherFcst                 1      timestamp   \n",
      "18         main         WeatherFcst                 2           temp   \n",
      "19         main         WeatherFcst                 3            lat   \n",
      "20         main         WeatherFcst                 4            lon   \n",
      "21         main         WeatherHist                 1      timestamp   \n",
      "22         main         WeatherHist                 2           temp   \n",
      "23         main         WeatherHist                 3            lat   \n",
      "24         main         WeatherHist                 4            lon   \n",
      "25         main    forecast_weather                 1  forecast_time   \n",
      "26         main    forecast_weather                 2      timestamp   \n",
      "27         main    forecast_weather                 3       interval   \n",
      "28         main    forecast_weather                 4            lat   \n",
      "29         main    forecast_weather                 5            lon   \n",
      "30         main    forecast_weather                 6    temperature   \n",
      "31         main    forecast_weather                 7       humidity   \n",
      "32         main    forecast_weather                 8      windspeed   \n",
      "33         main    forecast_weather                 9  precipitation   \n",
      "34         main    forecast_weather                10         source   \n",
      "35         main  historical_weather                 1      timestamp   \n",
      "36         main  historical_weather                 2       interval   \n",
      "37         main  historical_weather                 3            lat   \n",
      "38         main  historical_weather                 4            lon   \n",
      "39         main  historical_weather                 5    temperature   \n",
      "40         main  historical_weather                 6       humidity   \n",
      "41         main  historical_weather                 7      windspeed   \n",
      "42         main  historical_weather                 8  precipitation   \n",
      "43         main  historical_weather                 9         source   \n",
      "44         main       prices_hourly                 1   OperatingDTM   \n",
      "45         main       prices_hourly                 2           TIME   \n",
      "46         main       prices_hourly                 3       Location   \n",
      "47         main       prices_hourly                 4          Price   \n",
      "\n",
      "    data_type is_nullable  \n",
      "0   TIMESTAMP         YES  \n",
      "1     INTEGER         YES  \n",
      "2     VARCHAR         YES  \n",
      "3     VARCHAR         YES  \n",
      "4     VARCHAR         YES  \n",
      "5     VARCHAR         YES  \n",
      "6      DOUBLE         YES  \n",
      "7   TIMESTAMP         YES  \n",
      "8     INTEGER         YES  \n",
      "9     VARCHAR         YES  \n",
      "10    VARCHAR         YES  \n",
      "11    VARCHAR         YES  \n",
      "12     DOUBLE         YES  \n",
      "13  TIMESTAMP         YES  \n",
      "14     DOUBLE         YES  \n",
      "15  TIMESTAMP         YES  \n",
      "16    VARCHAR         YES  \n",
      "17  TIMESTAMP         YES  \n",
      "18     DOUBLE         YES  \n",
      "19     DOUBLE         YES  \n",
      "20     DOUBLE         YES  \n",
      "21  TIMESTAMP         YES  \n",
      "22     DOUBLE         YES  \n",
      "23     DOUBLE         YES  \n",
      "24     DOUBLE         YES  \n",
      "25  TIMESTAMP          NO  \n",
      "26  TIMESTAMP          NO  \n",
      "27    INTEGER         YES  \n",
      "28     DOUBLE          NO  \n",
      "29     DOUBLE          NO  \n",
      "30     DOUBLE         YES  \n",
      "31     DOUBLE         YES  \n",
      "32     DOUBLE         YES  \n",
      "33     DOUBLE         YES  \n",
      "34    VARCHAR         YES  \n",
      "35  TIMESTAMP          NO  \n",
      "36    INTEGER         YES  \n",
      "37     DOUBLE          NO  \n",
      "38     DOUBLE          NO  \n",
      "39     DOUBLE         YES  \n",
      "40     DOUBLE         YES  \n",
      "41     DOUBLE         YES  \n",
      "42     DOUBLE         YES  \n",
      "43    VARCHAR         YES  \n",
      "44  TIMESTAMP         YES  \n",
      "45    VARCHAR         YES  \n",
      "46    VARCHAR         YES  \n",
      "47     DOUBLE         YES  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 SELECT\n",
    "                    c.table_schema,\n",
    "                    c.table_name,\n",
    "                    c.ordinal_position,\n",
    "                    c.column_name,\n",
    "                    c.data_type,\n",
    "                    c.is_nullable\n",
    "                FROM information_schema.columns AS c\n",
    "                  JOIN information_schema.tables  AS t\n",
    "                    ON c.table_schema = t.table_schema\n",
    "                    AND c.table_name   = t.table_name\n",
    "                WHERE t.table_type = 'BASE TABLE'            -- change to IN ('BASE TABLE','VIEW') to include views\n",
    "                    AND c.table_schema NOT IN ('information_schema','pg_catalog')\n",
    "                ORDER BY c.table_schema, c.table_name, c.ordinal_position;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e62aea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           table_name  row_count\n",
      "0       prices_hourly     138378\n",
      "1  historical_weather      23018\n",
      "2    forecast_weather          0\n",
      "3        LoadForecast     278196\n",
      "4         WeatherFcst          0\n",
      "5         WeatherHist          0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                SELECT 'prices_hourly'      AS table_name, COUNT(*) AS row_count FROM prices_hourly\n",
    "UNION ALL SELECT 'historical_weather', COUNT(*) FROM historical_weather\n",
    "UNION ALL SELECT 'forecast_weather',  COUNT(*) FROM forecast_weather\n",
    "UNION ALL SELECT 'LoadForecast',      COUNT(*) FROM \"LoadForecast\"\n",
    "UNION ALL SELECT 'WeatherFcst',       COUNT(*) FROM \"WeatherFcst\"\n",
    "UNION ALL SELECT 'WeatherHist',       COUNT(*) FROM \"WeatherHist\";\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06104299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           table_name  row_count\n",
      "0       prices_hourly     138378\n",
      "1  historical_weather      23018\n",
      "2    forecast_weather          0\n",
      "3        LoadForecast     278196\n",
      "4         WeatherFcst          0\n",
      "5         WeatherHist          0\n",
      "   table_schema          table_name  ordinal_position    column_name  \\\n",
      "0          main              Prices                 1      timestamp   \n",
      "1          main              Prices                 2            lmp   \n",
      "2          main           SessionID                 1             ts   \n",
      "3          main           SessionID                 2     session_id   \n",
      "4          main    forecast_weather                 1  forecast_time   \n",
      "5          main    forecast_weather                 2      timestamp   \n",
      "6          main    forecast_weather                 3       interval   \n",
      "7          main    forecast_weather                 4            lat   \n",
      "8          main    forecast_weather                 5            lon   \n",
      "9          main    forecast_weather                 6    temperature   \n",
      "10         main    forecast_weather                 7       humidity   \n",
      "11         main    forecast_weather                 8      windspeed   \n",
      "12         main    forecast_weather                 9  precipitation   \n",
      "13         main    forecast_weather                10         source   \n",
      "14         main  historical_weather                 1      timestamp   \n",
      "15         main  historical_weather                 2       interval   \n",
      "16         main  historical_weather                 3            lat   \n",
      "17         main  historical_weather                 4            lon   \n",
      "18         main  historical_weather                 5    temperature   \n",
      "19         main  historical_weather                 6       humidity   \n",
      "20         main  historical_weather                 7      windspeed   \n",
      "21         main  historical_weather                 8  precipitation   \n",
      "22         main  historical_weather                 9         source   \n",
      "\n",
      "    data_type is_nullable  \n",
      "0   TIMESTAMP         YES  \n",
      "1      DOUBLE         YES  \n",
      "2   TIMESTAMP         YES  \n",
      "3     VARCHAR         YES  \n",
      "4   TIMESTAMP          NO  \n",
      "5   TIMESTAMP          NO  \n",
      "6     INTEGER         YES  \n",
      "7      DOUBLE          NO  \n",
      "8      DOUBLE          NO  \n",
      "9      DOUBLE         YES  \n",
      "10     DOUBLE         YES  \n",
      "11     DOUBLE         YES  \n",
      "12     DOUBLE         YES  \n",
      "13    VARCHAR         YES  \n",
      "14  TIMESTAMP          NO  \n",
      "15    INTEGER         YES  \n",
      "16     DOUBLE          NO  \n",
      "17     DOUBLE          NO  \n",
      "18     DOUBLE         YES  \n",
      "19     DOUBLE         YES  \n",
      "20     DOUBLE         YES  \n",
      "21     DOUBLE         YES  \n",
      "22    VARCHAR         YES  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                SELECT 'prices_hourly'      AS table_name, COUNT(*) AS row_count FROM prices_hourly\n",
    "UNION ALL SELECT 'historical_weather', COUNT(*) FROM historical_weather\n",
    "UNION ALL SELECT 'forecast_weather',  COUNT(*) FROM forecast_weather\n",
    "UNION ALL SELECT 'LoadForecast',      COUNT(*) FROM \"LoadForecast\"\n",
    "UNION ALL SELECT 'WeatherFcst',       COUNT(*) FROM \"WeatherFcst\"\n",
    "UNION ALL SELECT 'WeatherHist',       COUNT(*) FROM \"WeatherHist\";\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n",
    "\n",
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ercot_app/ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                SELECT\n",
    "                    c.table_schema,\n",
    "                    c.table_name,\n",
    "                    c.ordinal_position,\n",
    "                    c.column_name,\n",
    "                    c.data_type,\n",
    "                    c.is_nullable\n",
    "                FROM information_schema.columns AS c\n",
    "                  JOIN information_schema.tables  AS t\n",
    "                    ON c.table_schema = t.table_schema\n",
    "                    AND c.table_name   = t.table_name\n",
    "                WHERE t.table_type = 'BASE TABLE'            -- change to IN ('BASE TABLE','VIEW') to include views\n",
    "                    AND c.table_schema NOT IN ('information_schema','pg_catalog')\n",
    "                ORDER BY c.table_schema, c.table_name, c.ordinal_position;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "611547ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utilities/\n",
      "├── hw_maintenance.py\n",
      "├── lookup_locations.py\n",
      "├── preview_load_request.py\n",
      "├── queries.ipynb\n",
      "└── query_duck.py\n"
     ]
    }
   ],
   "source": [
    "# save as print_tree.py and run: python print_tree.py > scaffold.txt\n",
    "from pathlib import Path\n",
    "\n",
    "IGNORE = {'.git', '.venv', '__pycache__'}\n",
    "root = Path('.').resolve()\n",
    "\n",
    "def walk(dir_path: Path, prefix=\"\"):\n",
    "  entries = sorted([p for p in dir_path.iterdir() if p.name not in IGNORE], key=lambda p: (p.is_file(), p.name.lower()))\n",
    "  for i, p in enumerate(entries):\n",
    "    connector = \"└── \" if i == len(entries)-1 else \"├── \"\n",
    "    print(prefix + connector + p.name + (\"/\" if p.is_dir() else \"\"))\n",
    "    if p.is_dir():\n",
    "      extension = \"    \" if i == len(entries)-1 else \"│   \"\n",
    "      walk(p, prefix + extension)\n",
    "\n",
    "print(root.name + \"/\")\n",
    "walk(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77732ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   seq  name                                               file\n",
      "0  570  data  c:\\users\\db\\drewbcapstone\\utilities\\..\\project...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                PRAGMA database_list;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b784696f",
   "metadata": {},
   "source": [
    "comparing ercot_app db to main db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ccaada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           table_name  row_count\n",
      "0              prices          0\n",
      "1  historical_weather     114380\n",
      "2    forecast_weather       3768\n",
      "           table_name  row_count\n",
      "0       prices_hourly     138378\n",
      "1  historical_weather      23310\n",
      "2    forecast_weather          0\n",
      "3        LoadForecast     278196\n",
      "4         WeatherFcst          0\n",
      "5         WeatherHist          0\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ercot_app/ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                SELECT 'prices'      AS table_name, COUNT(*) AS row_count FROM prices\n",
    "UNION ALL SELECT 'historical_weather', COUNT(*) FROM historical_weather\n",
    "UNION ALL SELECT 'forecast_weather',  COUNT(*) FROM forecast_weather;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n",
    "\n",
    "\n",
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                SELECT 'prices_hourly'      AS table_name, COUNT(*) AS row_count FROM prices_hourly\n",
    "UNION ALL SELECT 'historical_weather', COUNT(*) FROM historical_weather\n",
    "UNION ALL SELECT 'forecast_weather',  COUNT(*) FROM forecast_weather\n",
    "UNION ALL SELECT 'LoadForecast',      COUNT(*) FROM \"LoadForecast\"\n",
    "UNION ALL SELECT 'WeatherFcst',       COUNT(*) FROM \"WeatherFcst\"\n",
    "UNION ALL SELECT 'WeatherHist',       COUNT(*) FROM \"WeatherHist\";\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a9dd762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           table_name  row_count\n",
      "0              prices          0\n",
      "1  historical_weather     114380\n",
      "2    forecast_weather       3768\n",
      "           table_name  row_count\n",
      "0       prices_hourly     138378\n",
      "1  historical_weather     114719\n",
      "2    forecast_weather          0\n",
      "3        LoadForecast     278196\n",
      "4         WeatherFcst          0\n",
      "5         WeatherHist          0\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ercot_app/ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                SELECT 'prices'      AS table_name, COUNT(*) AS row_count FROM prices\n",
    "UNION ALL SELECT 'historical_weather', COUNT(*) FROM historical_weather\n",
    "UNION ALL SELECT 'forecast_weather',  COUNT(*) FROM forecast_weather;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n",
    "\n",
    "\n",
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                SELECT 'prices_hourly'      AS table_name, COUNT(*) AS row_count FROM prices_hourly\n",
    "UNION ALL SELECT 'historical_weather', COUNT(*) FROM historical_weather\n",
    "UNION ALL SELECT 'forecast_weather',  COUNT(*) FROM forecast_weather\n",
    "UNION ALL SELECT 'LoadForecast',      COUNT(*) FROM \"LoadForecast\"\n",
    "UNION ALL SELECT 'WeatherFcst',       COUNT(*) FROM \"WeatherFcst\"\n",
    "UNION ALL SELECT 'WeatherHist',       COUNT(*) FROM \"WeatherHist\";\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "376457d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           table_name  row_count\n",
      "0              prices          0\n",
      "1  historical_weather     114380\n",
      "2    forecast_weather       3768\n",
      "           table_name  row_count\n",
      "0       prices_hourly     138378\n",
      "1  historical_weather     114719\n",
      "2    forecast_weather        960\n",
      "3        LoadForecast     278196\n",
      "4         WeatherFcst          0\n",
      "5         WeatherHist          0\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ercot_app/ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                SELECT 'prices'      AS table_name, COUNT(*) AS row_count FROM prices\n",
    "UNION ALL SELECT 'historical_weather', COUNT(*) FROM historical_weather\n",
    "UNION ALL SELECT 'forecast_weather',  COUNT(*) FROM forecast_weather;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n",
    "\n",
    "\n",
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                SELECT 'prices_hourly'      AS table_name, COUNT(*) AS row_count FROM prices_hourly\n",
    "UNION ALL SELECT 'historical_weather', COUNT(*) FROM historical_weather\n",
    "UNION ALL SELECT 'forecast_weather',  COUNT(*) FROM forecast_weather\n",
    "UNION ALL SELECT 'LoadForecast',      COUNT(*) FROM \"LoadForecast\"\n",
    "UNION ALL SELECT 'WeatherFcst',       COUNT(*) FROM \"WeatherFcst\"\n",
    "UNION ALL SELECT 'WeatherHist',       COUNT(*) FROM \"WeatherHist\";\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09596cd",
   "metadata": {},
   "source": [
    "FInally, the main database is populated with all the data needed to build a view for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c38ac299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   table_schema          table_name  ordinal_position    column_name  \\\n",
      "0          main        LoadForecast                 1   OperatingDTM   \n",
      "1          main        LoadForecast                 2       Interval   \n",
      "2          main        LoadForecast                 3          Month   \n",
      "3          main        LoadForecast                 4   LocationType   \n",
      "4          main        LoadForecast                 5       Location   \n",
      "5          main        LoadForecast                 6        DSTFlag   \n",
      "6          main        LoadForecast                 7     ForecastMW   \n",
      "7          main   LoadForecastPivot                 1   OperatingDTM   \n",
      "8          main   LoadForecastPivot                 2       Interval   \n",
      "9          main   LoadForecastPivot                 3   LocationType   \n",
      "10         main   LoadForecastPivot                 4       Location   \n",
      "11         main   LoadForecastPivot                 5        DSTFlag   \n",
      "12         main   LoadForecastPivot                 6     ForecastMW   \n",
      "13         main              Prices                 1      timestamp   \n",
      "14         main              Prices                 2            lmp   \n",
      "15         main           SessionID                 1             ts   \n",
      "16         main           SessionID                 2     session_id   \n",
      "17         main         WeatherFcst                 1      timestamp   \n",
      "18         main         WeatherFcst                 2           temp   \n",
      "19         main         WeatherFcst                 3            lat   \n",
      "20         main         WeatherFcst                 4            lon   \n",
      "21         main         WeatherHist                 1      timestamp   \n",
      "22         main         WeatherHist                 2           temp   \n",
      "23         main         WeatherHist                 3            lat   \n",
      "24         main         WeatherHist                 4            lon   \n",
      "25         main    forecast_weather                 1  forecast_time   \n",
      "26         main    forecast_weather                 2      timestamp   \n",
      "27         main    forecast_weather                 3       interval   \n",
      "28         main    forecast_weather                 4            lat   \n",
      "29         main    forecast_weather                 5            lon   \n",
      "30         main    forecast_weather                 6    temperature   \n",
      "31         main    forecast_weather                 7       humidity   \n",
      "32         main    forecast_weather                 8      windspeed   \n",
      "33         main    forecast_weather                 9  precipitation   \n",
      "34         main    forecast_weather                10         source   \n",
      "35         main  historical_weather                 1      timestamp   \n",
      "36         main  historical_weather                 2       interval   \n",
      "37         main  historical_weather                 3            lat   \n",
      "38         main  historical_weather                 4            lon   \n",
      "39         main  historical_weather                 5    temperature   \n",
      "40         main  historical_weather                 6       humidity   \n",
      "41         main  historical_weather                 7      windspeed   \n",
      "42         main  historical_weather                 8  precipitation   \n",
      "43         main  historical_weather                 9         source   \n",
      "44         main       prices_hourly                 1   OperatingDTM   \n",
      "45         main       prices_hourly                 2           TIME   \n",
      "46         main       prices_hourly                 3       Location   \n",
      "47         main       prices_hourly                 4          Price   \n",
      "\n",
      "    data_type is_nullable  \n",
      "0   TIMESTAMP         YES  \n",
      "1     INTEGER         YES  \n",
      "2     VARCHAR         YES  \n",
      "3     VARCHAR         YES  \n",
      "4     VARCHAR         YES  \n",
      "5     VARCHAR         YES  \n",
      "6      DOUBLE         YES  \n",
      "7   TIMESTAMP         YES  \n",
      "8     INTEGER         YES  \n",
      "9     VARCHAR         YES  \n",
      "10    VARCHAR         YES  \n",
      "11    VARCHAR         YES  \n",
      "12     DOUBLE         YES  \n",
      "13  TIMESTAMP         YES  \n",
      "14     DOUBLE         YES  \n",
      "15  TIMESTAMP         YES  \n",
      "16    VARCHAR         YES  \n",
      "17  TIMESTAMP         YES  \n",
      "18     DOUBLE         YES  \n",
      "19     DOUBLE         YES  \n",
      "20     DOUBLE         YES  \n",
      "21  TIMESTAMP         YES  \n",
      "22     DOUBLE         YES  \n",
      "23     DOUBLE         YES  \n",
      "24     DOUBLE         YES  \n",
      "25  TIMESTAMP          NO  \n",
      "26  TIMESTAMP          NO  \n",
      "27    INTEGER         YES  \n",
      "28     DOUBLE          NO  \n",
      "29     DOUBLE          NO  \n",
      "30     DOUBLE         YES  \n",
      "31     DOUBLE         YES  \n",
      "32     DOUBLE         YES  \n",
      "33     DOUBLE         YES  \n",
      "34    VARCHAR         YES  \n",
      "35  TIMESTAMP          NO  \n",
      "36    INTEGER         YES  \n",
      "37     DOUBLE          NO  \n",
      "38     DOUBLE          NO  \n",
      "39     DOUBLE         YES  \n",
      "40     DOUBLE         YES  \n",
      "41     DOUBLE         YES  \n",
      "42     DOUBLE         YES  \n",
      "43    VARCHAR         YES  \n",
      "44  TIMESTAMP         YES  \n",
      "45    VARCHAR         YES  \n",
      "46    VARCHAR         YES  \n",
      "47     DOUBLE         YES  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 SELECT\n",
    "                    c.table_schema,\n",
    "                    c.table_name,\n",
    "                    c.ordinal_position,\n",
    "                    c.column_name,\n",
    "                    c.data_type,\n",
    "                    c.is_nullable\n",
    "                FROM information_schema.columns AS c\n",
    "                  JOIN information_schema.tables  AS t\n",
    "                    ON c.table_schema = t.table_schema\n",
    "                    AND c.table_name   = t.table_name\n",
    "                WHERE t.table_type = 'BASE TABLE'            -- change to IN ('BASE TABLE','VIEW') to include views\n",
    "                    AND c.table_schema NOT IN ('information_schema','pg_catalog')\n",
    "                ORDER BY c.table_schema, c.table_name, c.ordinal_position;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8870e0d",
   "metadata": {},
   "source": [
    "Pivoted LoadForecast query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d15e44c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      OperatingDTM  Interval   Month DSTFlag  lz_houston  lz_north  lz_south  \\\n",
      "0       2023-01-01         1  1-2023       N         NaN       NaN       NaN   \n",
      "1       2023-01-01         2  1-2023       N         NaN       NaN       NaN   \n",
      "2       2023-01-01         3  1-2023       N         NaN       NaN       NaN   \n",
      "3       2023-01-01         4  1-2023       N         NaN       NaN       NaN   \n",
      "4       2023-01-01         5  1-2023       N         NaN       NaN       NaN   \n",
      "...            ...       ...     ...     ...         ...       ...       ...   \n",
      "23178   2025-08-23        20  8-2025       N         NaN       NaN       NaN   \n",
      "23179   2025-08-23        21  8-2025       N         NaN       NaN       NaN   \n",
      "23180   2025-08-23        22  8-2025       N         NaN       NaN       NaN   \n",
      "23181   2025-08-23        23  8-2025       N         NaN       NaN       NaN   \n",
      "23182   2025-08-23        24  8-2025       N         NaN       NaN       NaN   \n",
      "\n",
      "       lz_west  \n",
      "0          NaN  \n",
      "1          NaN  \n",
      "2          NaN  \n",
      "3          NaN  \n",
      "4          NaN  \n",
      "...        ...  \n",
      "23178      NaN  \n",
      "23179      NaN  \n",
      "23180      NaN  \n",
      "23181      NaN  \n",
      "23182      NaN  \n",
      "\n",
      "[23183 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "-- Wide, one row per OperatingDTM+Interval (and Month/DSTFlag), one column per Location\n",
    "SELECT\n",
    "  OperatingDTM,\n",
    "  CAST(Interval AS INTEGER) AS Interval,\n",
    "  Month,\n",
    "  DSTFlag,\n",
    "  SUM(CASE WHEN lower(Location) = 'lz_houston'  THEN ForecastMW END) AS lz_houston,\n",
    "  SUM(CASE WHEN lower(Location) = 'lz_north'    THEN ForecastMW END) AS lz_north,\n",
    "  SUM(CASE WHEN lower(Location) = 'lz_south'    THEN ForecastMW END) AS lz_south,\n",
    "  SUM(CASE WHEN lower(Location) = 'lz_west'     THEN ForecastMW END) AS lz_west\n",
    "  -- add more locations as needed...\n",
    "FROM \"LoadForecast\"\n",
    "GROUP BY 1,2,3,4\n",
    "ORDER BY OperatingDTM, Interval;\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af246e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Location  LocationType\n",
      "0   SouthCentral  Weather Zone\n",
      "1           East  Weather Zone\n",
      "2           West  Weather Zone\n",
      "3   NorthCentral  Weather Zone\n",
      "4        FarWest  Weather Zone\n",
      "5          North     Load Zone\n",
      "6           West     Load Zone\n",
      "7          North  Weather Zone\n",
      "8       Southern  Weather Zone\n",
      "9          Coast  Weather Zone\n",
      "10         South     Load Zone\n",
      "11       Houston     Load Zone\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "-- Wide, one row per OperatingDTM+Interval (and Month/DSTFlag), one column per Location\n",
    "SELECT Distinct Location,LocationType          \n",
    "  FROM \"LoadForecast\"\n",
    ";\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "312f83f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OperatingDTM  Interval   Month  LocationType      Location DSTFlag  \\\n",
      "0   2023-01-01         1  1-2023  Weather Zone         North       N   \n",
      "1   2023-01-01         1  1-2023  Weather Zone      Southern       N   \n",
      "2   2023-01-01         1  1-2023  Weather Zone  SouthCentral       N   \n",
      "3   2023-01-01         1  1-2023  Weather Zone  NorthCentral       N   \n",
      "4   2023-01-01         1  1-2023  Weather Zone         Coast       N   \n",
      "5   2023-01-01         1  1-2023  Weather Zone          East       N   \n",
      "6   2023-01-01         1  1-2023  Weather Zone       FarWest       N   \n",
      "7   2023-01-01         1  1-2023  Weather Zone          West       N   \n",
      "8   2023-01-01         1  1-2023     Load Zone         North       N   \n",
      "9   2023-01-01         1  1-2023     Load Zone         South       N   \n",
      "\n",
      "     ForecastMW  \n",
      "0    758.773000  \n",
      "1   2815.000000  \n",
      "2   5831.879900  \n",
      "3   9965.370100  \n",
      "4   8766.250000  \n",
      "5   1219.980000  \n",
      "6   5324.109900  \n",
      "7   1129.740000  \n",
      "8  11568.836588  \n",
      "9   9328.544071  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "select * from LoadForecast\n",
    "                 LIMIT 10;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "select * from LoadForecast\n",
    "                 LIMIT 10;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c1210",
   "metadata": {},
   "source": [
    "Pivoted Load Forecast data by Location Type and Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d425b41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      OperatingDTM  Interval   Month DSTFlag  wz_southcentral    wz_east  \\\n",
      "0       2023-01-01         1  1-2023       N        5831.8799  1219.9800   \n",
      "1       2023-01-01         2  1-2023       N        5677.6401  1219.2800   \n",
      "2       2023-01-01         3  1-2023       N        5544.1802  1244.1200   \n",
      "3       2023-01-01         4  1-2023       N        5460.6899  1201.3199   \n",
      "4       2023-01-01         5  1-2023       N        5512.1699  1204.2400   \n",
      "...            ...       ...     ...     ...              ...        ...   \n",
      "23178   2025-08-23        20  8-2025       N       12319.8994  2547.4299   \n",
      "23179   2025-08-23        21  8-2025       N       11860.6748  2438.8701   \n",
      "23180   2025-08-23        22  8-2025       N       11418.9746  2320.7075   \n",
      "23181   2025-08-23        23  8-2025       N       10797.6992  2187.1775   \n",
      "23182   2025-08-23        24  8-2025       N       10105.1426  2033.3350   \n",
      "\n",
      "         wz_west  wz_northcentral  wz_farwest   wz_north  wz_southern  \\\n",
      "0      1129.7400        9965.3701   5324.1099   758.7730    2815.0000   \n",
      "1      1110.2400        9831.7500   5194.3501   761.5800    2935.3999   \n",
      "2      1063.9600        9737.5303   5249.2900   728.3730    2707.7100   \n",
      "3      1058.7100        9606.1904   5245.5098   724.6090    2626.5801   \n",
      "4      1007.4600        9510.8096   5282.1499   753.9290    2597.5400   \n",
      "...          ...              ...         ...        ...          ...   \n",
      "23178  1751.6400       21937.4258   7929.2900  2461.0977    5614.1445   \n",
      "23179  1687.8900       20975.9746   7657.3301  2337.6174    5419.7524   \n",
      "23180  1637.4351       20113.9238   7670.5352  2298.1899    5248.0527   \n",
      "23181  1535.3601       18937.5488   7716.1201  2187.9351    5001.8779   \n",
      "23182  1456.1176       17665.5742   7745.1724  2098.2500    4774.8325   \n",
      "\n",
      "         wz_coast      lz_north       lz_west      lz_south    lz_houston  \n",
      "0       8766.2500  11568.836588   6453.429054   9328.544071   8460.293131  \n",
      "1       8328.8203  11436.339408   6311.435466   9272.991016   8038.294596  \n",
      "2       8175.9399  11346.541315   6322.941056   8890.909060   7890.711902  \n",
      "3       8108.9702  11171.331687   6312.799135   8722.415717   7826.032806  \n",
      "4       7950.1699  11094.672204   6329.715194   8721.117526   7672.963460  \n",
      "...           ...           ...           ...           ...           ...  \n",
      "23178  18228.7500  25746.330833  10306.903018  19143.676012  17592.767504  \n",
      "23179  17830.9004  24612.970052   9931.967546  18455.502232  17208.570056  \n",
      "23180  17334.3008  23616.953894   9887.404742  17808.591979  16729.169013  \n",
      "23181  16618.4746  22251.600534   9809.834502  16882.606018  16038.152305  \n",
      "23182  15830.6494  20784.072440   9737.260024  15910.119744  15277.621401  \n",
      "\n",
      "[23183 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 WITH base AS (\n",
    "  SELECT\n",
    "      OperatingDTM,\n",
    "      CAST(Interval AS INTEGER) AS Interval,\n",
    "      Month,\n",
    "      DSTFlag,\n",
    "      lower(LocationType) AS lt,\n",
    "      lower(Location)     AS loc,\n",
    "      ForecastMW\n",
    "  FROM \"LoadForecast\"\n",
    ")\n",
    "SELECT\n",
    "  OperatingDTM,\n",
    "  Interval,\n",
    "  Month,\n",
    "  DSTFlag,\n",
    "\n",
    "  -- Weather Zone columns\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='southcentral' THEN ForecastMW END) AS wz_southcentral,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='east'         THEN ForecastMW END) AS wz_east,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='west'         THEN ForecastMW END) AS wz_west,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='northcentral' THEN ForecastMW END) AS wz_northcentral,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='farwest'      THEN ForecastMW END) AS wz_farwest,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='north'        THEN ForecastMW END) AS wz_north,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='southern'     THEN ForecastMW END) AS wz_southern,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='coast'        THEN ForecastMW END) AS wz_coast,\n",
    "\n",
    "  -- Load Zone columns\n",
    "  SUM(CASE WHEN lt='load zone' AND loc='north'           THEN ForecastMW END) AS lz_north,\n",
    "  SUM(CASE WHEN lt='load zone' AND loc='west'            THEN ForecastMW END) AS lz_west,\n",
    "  SUM(CASE WHEN lt='load zone' AND loc='south'           THEN ForecastMW END) AS lz_south,\n",
    "  SUM(CASE WHEN lt='load zone' AND loc='houston'         THEN ForecastMW END) AS lz_houston\n",
    "\n",
    "FROM base\n",
    "GROUP BY 1,2,3,4\n",
    "ORDER BY OperatingDTM, Interval;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80c6e1",
   "metadata": {},
   "source": [
    "Create a view with that query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "09a35986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"CREATE OR REPLACE VIEW vw_loadforecast_by_zone AS\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "      OperatingDTM,\n",
    "      CAST(Interval AS INTEGER) AS Interval,\n",
    "      Month,\n",
    "      DSTFlag,\n",
    "      lower(LocationType) AS lt,\n",
    "      lower(Location)     AS loc,\n",
    "      ForecastMW\n",
    "  FROM \"LoadForecast\"\n",
    ")\n",
    "SELECT\n",
    "  OperatingDTM,\n",
    "  Interval,\n",
    "  Month,\n",
    "  DSTFlag,\n",
    "\n",
    "  -- Weather Zone columns\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='southcentral' THEN ForecastMW END) AS wz_southcentral,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='east'         THEN ForecastMW END) AS wz_east,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='west'         THEN ForecastMW END) AS wz_west,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='northcentral' THEN ForecastMW END) AS wz_northcentral,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='farwest'      THEN ForecastMW END) AS wz_farwest,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='north'        THEN ForecastMW END) AS wz_north,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='southern'     THEN ForecastMW END) AS wz_southern,\n",
    "  SUM(CASE WHEN lt='weather zone' AND loc='coast'        THEN ForecastMW END) AS wz_coast,\n",
    "\n",
    "  -- Load Zone columns\n",
    "  SUM(CASE WHEN lt='load zone' AND loc='north'           THEN ForecastMW END) AS lz_north,\n",
    "  SUM(CASE WHEN lt='load zone' AND loc='west'            THEN ForecastMW END) AS lz_west,\n",
    "  SUM(CASE WHEN lt='load zone' AND loc='south'           THEN ForecastMW END) AS lz_south,\n",
    "  SUM(CASE WHEN lt='load zone' AND loc='houston'         THEN ForecastMW END) AS lz_houston\n",
    "\n",
    "FROM base\n",
    "GROUP BY 1,2,3,4;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49339691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      OperatingDTM  Interval   Month DSTFlag  wz_southcentral    wz_east  \\\n",
      "0       2025-08-23         1  8-2025       N        9345.7148  1915.8475   \n",
      "1       2025-08-23         4  8-2025       N        7997.4878  1647.8125   \n",
      "2       2025-08-23        12  8-2025       N       10691.9756  2247.4026   \n",
      "3       2025-08-23        22  8-2025       N       11418.9746  2320.7075   \n",
      "4       2025-08-23         9  8-2025       N        8461.2676  1736.9750   \n",
      "...            ...       ...     ...     ...              ...        ...   \n",
      "23178   2023-01-01         9  1-2023       N        6065.9102  1427.3000   \n",
      "23179   2023-01-01        20  1-2023       N        7026.8945  1459.9000   \n",
      "23180   2023-01-01        22  1-2023       N        6619.1650  1423.3440   \n",
      "23181   2023-01-01        10  1-2023       N        6097.6401  1406.4800   \n",
      "23182   2023-01-01        16  1-2023       N        6567.7715  1376.9348   \n",
      "\n",
      "         wz_west  wz_northcentral  wz_farwest   wz_north  wz_southern  \\\n",
      "0      1368.0699       16427.4746   7675.9053  1962.3776    4584.2178   \n",
      "1      1221.5125       13922.4248   7594.1177  1793.1250    4057.8052   \n",
      "2      1566.4100       19094.5742   8605.0225  2266.9275    5220.9248   \n",
      "3      1637.4351       20113.9238   7670.5352  2298.1899    5248.0527   \n",
      "4      1249.4851       14889.7500   8254.7852  1939.4225    4216.0449   \n",
      "...          ...              ...         ...        ...          ...   \n",
      "23178  1229.4600       11118.7002   5192.6602   818.9920    2711.8301   \n",
      "23179  1038.2687       11534.4014   5155.7168   967.1737    3716.4785   \n",
      "23180  1084.7209       11364.4434   5291.0078   714.3179    3502.2146   \n",
      "23181  1127.5400       11377.9004   5256.2300   777.4200    2767.2400   \n",
      "23182  1050.1935       10519.2559   4905.3125   793.2255    3668.6626   \n",
      "\n",
      "         wz_coast      lz_north       lz_west      lz_south    lz_houston  \n",
      "0      15226.0000  19359.290070   9541.666791  14911.004026  14693.646658  \n",
      "1      13598.9004  16508.766941   9268.660756  12932.741462  13123.016632  \n",
      "2      17718.6992  22513.169718  10753.250236  17046.544420  17098.972027  \n",
      "3      17334.3008  23616.953894   9887.404742  17808.591979  16729.169013  \n",
      "4      14535.6758  17639.978548  10020.401432  13596.120349  14026.905677  \n",
      "...           ...           ...           ...           ...           ...  \n",
      "23178   8511.6904  12955.120470   6422.029549   9484.290838   8215.102174  \n",
      "23179  10624.2324  13477.777122   6337.280761  11454.628558  10253.379598  \n",
      "23180  10145.5488  13129.330699   6390.617945  10833.518074   9791.295733  \n",
      "23181   8724.0596  13161.994465   6408.997342   9543.590453   8419.927812  \n",
      "23182  10347.5791  12291.537763   6007.641696  10943.703990   9986.051853  \n",
      "\n",
      "[23183 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 select * from vw_loadforecast_by_zone order by 1,2 desc Limit 200\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d131f",
   "metadata": {},
   "source": [
    "testing out prices_hourly table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60e35df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Location\n",
      "0   hb_hubavg\n",
      "1     hb_west\n",
      "2    hb_north\n",
      "3    hb_south\n",
      "4  hb_houston\n",
      "5      hb_pan\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "-- Wide, one row per OperatingDTM+Interval (and Month/DSTFlag), one column per Location\n",
    "SELECT Distinct Location          \n",
    "  FROM \"prices_hourly\"\n",
    ";\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c74d766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"CREATE OR REPLACE VIEW vw_prices_by_hub AS\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "      p.OperatingDTM,\n",
    "      CAST(\n",
    "        COALESCE(\n",
    "          TRY_CAST(regexp_extract(p.\"TIME\", 'HE(\\\\d+)', 1) AS INTEGER),\n",
    "          CASE\n",
    "            WHEN TRY_CAST(p.\"TIME\" AS INTEGER) IS NOT NULL\n",
    "            THEN FLOOR(TRY_CAST(p.\"TIME\" AS INTEGER) / 60)\n",
    "            ELSE CAST(EXTRACT(hour FROM p.OperatingDTM) AS INTEGER)\n",
    "          END\n",
    "        ) AS INTEGER\n",
    "      ) AS Interval,\n",
    "      lower(p.Location) AS loc,\n",
    "      p.Price           AS price\n",
    "  FROM prices_hourly p\n",
    ")\n",
    "SELECT\n",
    "  OperatingDTM,\n",
    "  Interval,\n",
    "  AVG(CASE WHEN loc='hb_hubavg'  THEN price END) AS hb_hubavg,\n",
    "  AVG(CASE WHEN loc='hb_west'    THEN price END) AS hb_west,\n",
    "  AVG(CASE WHEN loc='hb_north'   THEN price END) AS hb_north,\n",
    "  AVG(CASE WHEN loc='hb_south'   THEN price END) AS hb_south,\n",
    "  AVG(CASE WHEN loc='hb_houston' THEN price END) AS hb_houston,\n",
    "  AVG(CASE WHEN loc='hb_pan'     THEN price END) AS hb_pan\n",
    "FROM base\n",
    "GROUP BY 1,2;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0ae72a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      OperatingDTM  Interval  hb_hubavg  hb_west  hb_north  hb_south  \\\n",
      "0       2023-01-01        24       8.14     5.60     10.37      6.93   \n",
      "1       2023-01-01        23      12.69    11.20     14.79     11.00   \n",
      "2       2023-01-01        22      16.36    17.10     17.70     13.90   \n",
      "3       2023-01-01        21      15.71    16.61     16.95     13.18   \n",
      "4       2023-01-01        20      17.92    19.06     19.24     15.02   \n",
      "...            ...       ...        ...      ...       ...       ...   \n",
      "23058   2025-08-18         5      27.78    30.65     26.70     26.97   \n",
      "23059   2025-08-18         4      26.86    30.30     25.53     25.91   \n",
      "23060   2025-08-18         3      28.46    32.31     26.89     27.48   \n",
      "23061   2025-08-18         2      29.90    34.29     27.98     28.91   \n",
      "23062   2025-08-18         1      32.21    37.29     29.89     31.13   \n",
      "\n",
      "       hb_houston  hb_pan  \n",
      "0            9.65    5.20  \n",
      "1           13.78   10.86  \n",
      "2           16.74   16.68  \n",
      "3           16.09   16.02  \n",
      "4           18.36   18.56  \n",
      "...           ...     ...  \n",
      "23058       26.80   26.44  \n",
      "23059       25.68   25.08  \n",
      "23060       27.16   25.91  \n",
      "23061       28.42   26.24  \n",
      "23062       30.52   26.73  \n",
      "\n",
      "[23063 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 select * from vw_prices_by_hub order by 1,2 desc\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06dd496",
   "metadata": {},
   "source": [
    "make a mapping table of lat/lon to city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3feb79ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count\n",
      "0      5\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 CREATE OR REPLACE TABLE city_coords (\n",
    "  city VARCHAR,\n",
    "  lat  DOUBLE,\n",
    "  lon  DOUBLE\n",
    ");\n",
    "\n",
    "INSERT INTO city_coords (city, lat, lon) VALUES\n",
    "  ('The_Woodlands_TX', 30.1658, -95.4613),\n",
    "  ('Katy_TX',           29.7858, -95.8245),\n",
    "  ('Friendswood_TX',    29.5294, -95.2010),\n",
    "  ('Baytown_TX',        29.7355, -94.9774),\n",
    "  ('Houston_TX',        29.7604, -95.3698);\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106bed7",
   "metadata": {},
   "source": [
    "Join the label table and pivot the locations with weather forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27c6b757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW vw_forecast_weather_by_city AS\n",
    "WITH labeled AS (\n",
    "  SELECT\n",
    "      f.forecast_time,\n",
    "      f.\"timestamp\",\n",
    "      f.\"interval\",\n",
    "      /* OperatingDTM logic:\n",
    "         - If time = 00:00:00, assign to prior operating day\n",
    "         - Else assign to same day\n",
    "      */\n",
    "      CASE\n",
    "        WHEN STRFTIME(f.\"timestamp\", '%H:%M:%S') = '00:00:00'\n",
    "          THEN CAST(DATE_TRUNC('day', f.\"timestamp\") - INTERVAL 1 DAY AS DATE)\n",
    "        ELSE CAST(DATE_TRUNC('day', f.\"timestamp\") AS DATE)\n",
    "      END AS OperatingDTM,\n",
    "      f.lat,\n",
    "      f.lon,\n",
    "      COALESCE(c.city, 'unmapped') AS city_name,\n",
    "      -- slug for flexible naming if needed later\n",
    "      REGEXP_REPLACE(LOWER(COALESCE(c.city, 'unmapped')), '[^a-z0-9]+', '_') AS city_key,\n",
    "      f.temperature,\n",
    "      f.humidity,\n",
    "      f.windspeed,\n",
    "      f.precipitation\n",
    "  FROM forecast_weather f\n",
    "  LEFT JOIN city_coords c\n",
    "    ON ROUND(f.lat, 4) = ROUND(c.lat, 4)\n",
    "   AND ROUND(f.lon, 4) = ROUND(c.lon, 4)\n",
    "),\n",
    "latest AS (\n",
    "  -- keep the most recent forecast issuance per delivery hour & city\n",
    "  SELECT *\n",
    "  FROM (\n",
    "    SELECT\n",
    "      labeled.*,\n",
    "      ROW_NUMBER() OVER (\n",
    "        PARTITION BY \"timestamp\", \"interval\", city_name\n",
    "        ORDER BY forecast_time DESC\n",
    "      ) AS rn\n",
    "    FROM labeled\n",
    "  )\n",
    "  WHERE rn = 1\n",
    ")\n",
    "SELECT\n",
    "  /* Keys for joining to loadforecast */\n",
    "  OperatingDTM,\n",
    "  \"timestamp\",\n",
    "  \"interval\",\n",
    "\n",
    "  /* The_Woodlands_TX */\n",
    "  AVG(CASE WHEN city_name='The_Woodlands_TX' THEN temperature   END) AS temp_the_woodlands_tx,\n",
    "  AVG(CASE WHEN city_name='The_Woodlands_TX' THEN humidity      END) AS hum_the_woodlands_tx,\n",
    "  AVG(CASE WHEN city_name='The_Woodlands_TX' THEN windspeed     END) AS wind_the_woodlands_tx,\n",
    "  AVG(CASE WHEN city_name='The_Woodlands_TX' THEN precipitation END) AS precip_the_woodlands_tx,\n",
    "\n",
    "  /* Katy_TX */\n",
    "  AVG(CASE WHEN city_name='Katy_TX' THEN temperature   END) AS temp_katy_tx,\n",
    "  AVG(CASE WHEN city_name='Katy_TX' THEN humidity      END) AS hum_katy_tx,\n",
    "  AVG(CASE WHEN city_name='Katy_TX' THEN windspeed     END) AS wind_katy_tx,\n",
    "  AVG(CASE WHEN city_name='Katy_TX' THEN precipitation END) AS precip_katy_tx,\n",
    "\n",
    "  /* Friendswood_TX */\n",
    "  AVG(CASE WHEN city_name='Friendswood_TX' THEN temperature   END) AS temp_friendswood_tx,\n",
    "  AVG(CASE WHEN city_name='Friendswood_TX' THEN humidity      END) AS hum_friendswood_tx,\n",
    "  AVG(CASE WHEN city_name='Friendswood_TX' THEN windspeed     END) AS wind_friendswood_tx,\n",
    "  AVG(CASE WHEN city_name='Friendswood_TX' THEN precipitation END) AS precip_friendswood_tx,\n",
    "\n",
    "  /* Baytown_TX */\n",
    "  AVG(CASE WHEN city_name='Baytown_TX' THEN temperature   END) AS temp_baytown_tx,\n",
    "  AVG(CASE WHEN city_name='Baytown_TX' THEN humidity      END) AS hum_baytown_tx,\n",
    "  AVG(CASE WHEN city_name='Baytown_TX' THEN windspeed     END) AS wind_baytown_tx,\n",
    "  AVG(CASE WHEN city_name='Baytown_TX' THEN precipitation END) AS precip_baytown_tx,\n",
    "\n",
    "  /* Houston_TX */\n",
    "  AVG(CASE WHEN city_name='Houston_TX' THEN temperature   END) AS temp_houston_tx,\n",
    "  AVG(CASE WHEN city_name='Houston_TX' THEN humidity      END) AS hum_houston_tx,\n",
    "  AVG(CASE WHEN city_name='Houston_TX' THEN windspeed     END) AS wind_houston_tx,\n",
    "  AVG(CASE WHEN city_name='Houston_TX' THEN precipitation END) AS precip_houston_tx\n",
    "\n",
    "FROM latest\n",
    "GROUP BY OperatingDTM, \"timestamp\", \"interval\"\n",
    "ORDER BY OperatingDTM, \"timestamp\", \"interval\";\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0a170668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              timestamp  interval  temp_the_woodlands_tx  \\\n",
      "0   2025-08-17 12:00:00        12                   31.3   \n",
      "1   2025-08-18 04:00:00         4                   26.6   \n",
      "2   2025-08-18 12:00:00        12                   33.6   \n",
      "3   2025-08-18 16:00:00        16                   37.7   \n",
      "4   2025-08-19 14:00:00        14                   28.2   \n",
      "..                  ...       ...                    ...   \n",
      "187 2025-08-24 13:00:00        13                   33.6   \n",
      "188 2025-08-23 08:00:00         8                   24.1   \n",
      "189 2025-08-23 23:00:00        23                   30.2   \n",
      "190 2025-08-23 21:00:00        21                   31.8   \n",
      "191 2025-08-23 18:00:00        18                   34.4   \n",
      "\n",
      "     hum_the_woodlands_tx  wind_the_woodlands_tx  precip_the_woodlands_tx  \\\n",
      "0                    74.0                   0.45                      0.0   \n",
      "1                    79.0                   1.84                      0.0   \n",
      "2                    54.0                   1.52                      0.0   \n",
      "3                    35.0                   2.11                      0.0   \n",
      "4                    73.0                   4.17                      0.0   \n",
      "..                    ...                    ...                      ...   \n",
      "187                  45.0                   1.71                      0.0   \n",
      "188                  86.0                   1.10                      0.1   \n",
      "189                  59.0                   3.10                      0.0   \n",
      "190                  52.0                   4.11                      0.0   \n",
      "191                  42.0                   2.11                      0.0   \n",
      "\n",
      "     temp_katy_tx  hum_katy_tx  wind_katy_tx  precip_katy_tx  ...  \\\n",
      "0            31.0         77.0          0.61             0.0  ...   \n",
      "1            26.4         88.0          1.34             0.0  ...   \n",
      "2            32.5         61.0          1.90             0.0  ...   \n",
      "3            37.3         38.0          1.30             0.0  ...   \n",
      "4            29.8         60.0          9.10             0.0  ...   \n",
      "..            ...          ...           ...             ...  ...   \n",
      "187          32.5         53.0          1.61             0.0  ...   \n",
      "188          23.4         82.0          0.22             0.0  ...   \n",
      "189          27.6         70.0          4.60             0.0  ...   \n",
      "190          29.1         63.0          3.94             0.0  ...   \n",
      "191          33.7         44.0          1.50             0.0  ...   \n",
      "\n",
      "     wind_friendswood_tx  precip_friendswood_tx  temp_baytown_tx  \\\n",
      "0                   1.20                    0.0             33.2   \n",
      "1                   1.12                    0.0             26.4   \n",
      "2                   1.22                    0.0             33.1   \n",
      "3                   3.45                    0.0             36.2   \n",
      "4                   0.91                    0.0             29.0   \n",
      "..                   ...                    ...              ...   \n",
      "187                 0.61                    0.0             31.3   \n",
      "188                 1.10                    0.0             25.1   \n",
      "189                 3.92                    0.0             28.7   \n",
      "190                 3.55                    0.0             29.3   \n",
      "191                 4.25                    0.0             30.0   \n",
      "\n",
      "     hum_baytown_tx  wind_baytown_tx  precip_baytown_tx  temp_houston_tx  \\\n",
      "0              64.0             1.21                0.0             30.2   \n",
      "1              86.0             1.26                0.0             26.0   \n",
      "2              61.0             1.27                0.0             30.1   \n",
      "3              47.0             3.37                0.0             35.3   \n",
      "4              73.0             2.67                0.0             25.7   \n",
      "..              ...              ...                ...              ...   \n",
      "187            57.0             2.66                0.0             35.1   \n",
      "188            84.0             1.75                0.0             26.4   \n",
      "189            70.0             4.60                0.0             29.0   \n",
      "190            67.0             4.14                0.0             31.4   \n",
      "191            63.0             4.64                0.0             36.1   \n",
      "\n",
      "     hum_houston_tx  wind_houston_tx  precip_houston_tx  \n",
      "0              82.0             1.14                0.0  \n",
      "1              88.0             1.58                0.0  \n",
      "2              77.0             1.75                0.0  \n",
      "3              48.0             0.22                0.0  \n",
      "4              71.0             4.16                0.7  \n",
      "..              ...              ...                ...  \n",
      "187            42.0             0.95                0.0  \n",
      "188            72.0             1.02                0.0  \n",
      "189            67.0             4.63                0.0  \n",
      "190            55.0             3.05                0.0  \n",
      "191            37.0             2.19                0.0  \n",
      "\n",
      "[192 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 select * from vw_forecast_weather_by_city \n",
    ";\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c4863",
   "metadata": {},
   "source": [
    "So close, just the historic weather to build then we have four views to work across for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f8ee3484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW vw_historical_weather_by_city AS\n",
    "WITH labeled AS (\n",
    "  SELECT\n",
    "    h.\"timestamp\",\n",
    "    h.\"interval\",\n",
    "    COALESCE(c.city, 'unmapped') AS city_name,\n",
    "    h.temperature,\n",
    "    h.humidity,\n",
    "    h.windspeed,\n",
    "    h.precipitation\n",
    "  FROM historical_weather h\n",
    "  LEFT JOIN city_coords c\n",
    "    ON ROUND(h.lat, 4) = ROUND(c.lat, 4)\n",
    "   AND ROUND(h.lon, 4) = ROUND(c.lon, 4)\n",
    ")\n",
    "SELECT\n",
    "  \"timestamp\",\n",
    "  \"interval\",\n",
    "\n",
    "  /* The_Woodlands_TX */\n",
    "  AVG(CASE WHEN city_name='The_Woodlands_TX' THEN temperature   END) AS temp_the_woodlands_tx,\n",
    "  AVG(CASE WHEN city_name='The_Woodlands_TX' THEN humidity      END) AS hum_the_woodlands_tx,\n",
    "  AVG(CASE WHEN city_name='The_Woodlands_TX' THEN windspeed     END) AS wind_the_woodlands_tx,\n",
    "  AVG(CASE WHEN city_name='The_Woodlands_TX' THEN precipitation END) AS precip_the_woodlands_tx,\n",
    "\n",
    "  /* Katy_TX */\n",
    "  AVG(CASE WHEN city_name='Katy_TX' THEN temperature   END) AS temp_katy_tx,\n",
    "  AVG(CASE WHEN city_name='Katy_TX' THEN humidity      END) AS hum_katy_tx,\n",
    "  AVG(CASE WHEN city_name='Katy_TX' THEN windspeed     END) AS wind_katy_tx,\n",
    "  AVG(CASE WHEN city_name='Katy_TX' THEN precipitation END) AS precip_katy_tx,\n",
    "\n",
    "  /* Friendswood_TX */\n",
    "  AVG(CASE WHEN city_name='Friendswood_TX' THEN temperature   END) AS temp_friendswood_tx,\n",
    "  AVG(CASE WHEN city_name='Friendswood_TX' THEN humidity      END) AS hum_friendswood_tx,\n",
    "  AVG(CASE WHEN city_name='Friendswood_TX' THEN windspeed     END) AS wind_friendswood_tx,\n",
    "  AVG(CASE WHEN city_name='Friendswood_TX' THEN precipitation END) AS precip_friendswood_tx,\n",
    "\n",
    "  /* Baytown_TX */\n",
    "  AVG(CASE WHEN city_name='Baytown_TX' THEN temperature   END) AS temp_baytown_tx,\n",
    "  AVG(CASE WHEN city_name='Baytown_TX' THEN humidity      END) AS hum_baytown_tx,\n",
    "  AVG(CASE WHEN city_name='Baytown_TX' THEN windspeed     END) AS wind_baytown_tx,\n",
    "  AVG(CASE WHEN city_name='Baytown_TX' THEN precipitation END) AS precip_baytown_tx,\n",
    "\n",
    "  /* Houston_TX */\n",
    "  AVG(CASE WHEN city_name='Houston_TX' THEN temperature   END) AS temp_houston_tx,\n",
    "  AVG(CASE WHEN city_name='Houston_TX' THEN humidity      END) AS hum_houston_tx,\n",
    "  AVG(CASE WHEN city_name='Houston_TX' THEN windspeed     END) AS wind_houston_tx,\n",
    "  AVG(CASE WHEN city_name='Houston_TX' THEN precipitation END) AS precip_houston_tx\n",
    "\n",
    "FROM labeled\n",
    "GROUP BY 1,2;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e4f306b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                timestamp  interval  temp_the_woodlands_tx  \\\n",
      "0     2023-10-25 12:00:00        12                   28.3   \n",
      "1     2023-10-25 18:00:00        18                   27.8   \n",
      "2     2023-10-26 01:00:00         1                   25.6   \n",
      "3     2023-10-26 16:00:00        16                   26.1   \n",
      "4     2023-10-26 20:00:00        20                   26.1   \n",
      "...                   ...       ...                    ...   \n",
      "23013 2023-10-24 10:00:00        10                   25.0   \n",
      "23014 2023-10-24 12:00:00        12                   29.4   \n",
      "23015 2023-10-24 13:00:00        13                   30.6   \n",
      "23016 2023-10-24 16:00:00        16                   30.6   \n",
      "23017 2023-10-24 20:00:00        20                   26.7   \n",
      "\n",
      "       hum_the_woodlands_tx  wind_the_woodlands_tx  precip_the_woodlands_tx  \\\n",
      "0                      70.0               6.694444                      0.0   \n",
      "1                      74.0               7.694444                      0.3   \n",
      "2                      84.0               4.111111                      0.0   \n",
      "3                      90.0               4.111111                      3.3   \n",
      "4                      88.0               3.111111                      NaN   \n",
      "...                     ...                    ...                      ...   \n",
      "23013                  79.0               3.611111                      0.0   \n",
      "23014                  57.0               5.694444                      0.0   \n",
      "23015                  50.0               6.694444                      0.0   \n",
      "23016                  51.0               5.111111                      0.0   \n",
      "23017                  67.0               5.694444                      0.0   \n",
      "\n",
      "       temp_katy_tx  hum_katy_tx  wind_katy_tx  precip_katy_tx  ...  \\\n",
      "0              29.0         70.0      8.805556             0.0  ...   \n",
      "1              29.0         70.0      5.694444             0.5  ...   \n",
      "2              26.0         84.0      6.694444             0.0  ...   \n",
      "3              29.0         79.0      5.694444             0.0  ...   \n",
      "4              28.0         79.0      4.611111             0.0  ...   \n",
      "...             ...          ...           ...             ...  ...   \n",
      "23013          26.0         74.0      6.694444             0.0  ...   \n",
      "23014          29.0         62.0      9.305556             0.1  ...   \n",
      "23015          30.0         59.0      8.805556             0.0  ...   \n",
      "23016          29.0         62.0      6.694444             0.0  ...   \n",
      "23017          27.0         66.0      5.694444             0.0  ...   \n",
      "\n",
      "       wind_friendswood_tx  precip_friendswood_tx  temp_baytown_tx  \\\n",
      "0                 6.694444                    0.0             29.0   \n",
      "1                 8.805556                    NaN             28.0   \n",
      "2                 4.111111                    0.0             26.0   \n",
      "3                 6.611111                    NaN             27.0   \n",
      "4                 4.305556                    0.0             26.0   \n",
      "...                    ...                    ...              ...   \n",
      "23013             5.694444                    0.0             27.0   \n",
      "23014             7.194444                    0.0             30.0   \n",
      "23015             7.194444                    0.0             30.0   \n",
      "23016             7.194444                    0.0             28.0   \n",
      "23017             5.694444                    0.0             25.0   \n",
      "\n",
      "       hum_baytown_tx  wind_baytown_tx  precip_baytown_tx  temp_houston_tx  \\\n",
      "0                75.0         7.777778                0.0             28.0   \n",
      "1                74.0         8.888889                0.0             28.0   \n",
      "2                89.0         5.555556                0.0             26.0   \n",
      "3                87.0         8.333333                0.2             25.0   \n",
      "4                84.0         4.166667                0.2             26.0   \n",
      "...               ...              ...                ...              ...   \n",
      "23013            79.0         6.111111                0.0             25.0   \n",
      "23014            62.0         6.111111                0.0             28.0   \n",
      "23015            59.0         9.166667                0.0             28.0   \n",
      "23016            68.0         8.888889                0.0             28.0   \n",
      "23017            82.0         5.555556                0.0             26.0   \n",
      "\n",
      "       hum_houston_tx  wind_houston_tx  precip_houston_tx  \n",
      "0                70.0         9.166667                0.1  \n",
      "1                70.0         8.333333                0.0  \n",
      "2                84.0         6.111111                0.0  \n",
      "3               100.0         4.722222                0.1  \n",
      "4                89.0         5.277778                0.5  \n",
      "...               ...              ...                ...  \n",
      "23013            74.0         6.111111                0.0  \n",
      "23014            58.0         9.722222                0.0  \n",
      "23015            58.0        12.222222                0.0  \n",
      "23016            62.0        10.277778                0.0  \n",
      "23017            70.0         9.166667                0.0  \n",
      "\n",
      "[23018 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 select * from vw_historical_weather_by_city \n",
    ";\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b518fc1",
   "metadata": {},
   "source": [
    "Now we have four views of base data both forecast and historic.\n",
    "\n",
    "# vw_loadforecast_by_zone,\n",
    "# vw_forecast_weather_by_city,\n",
    "# vw_historical_weather_by_city,\n",
    "# vw_prices_by_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "486dd8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     OperatingDTM  Interval   Month DSTFlag  wz_southcentral    wz_east  \\\n",
      "0      2025-08-23        24  8-2025       N       10105.1426  2033.3350   \n",
      "1      2025-08-23        23  8-2025       N       10797.6992  2187.1775   \n",
      "2      2025-08-23        22  8-2025       N       11418.9746  2320.7075   \n",
      "3      2025-08-23        21  8-2025       N       11860.6748  2438.8701   \n",
      "4      2025-08-23        20  8-2025       N       12319.8994  2547.4299   \n",
      "...           ...       ...     ...     ...              ...        ...   \n",
      "1995   2025-06-01        21  6-2025       N       12427.0996  1958.4700   \n",
      "1996   2025-06-01        20  6-2025       N       12868.0000  2070.0701   \n",
      "1997   2025-06-01        19  6-2025       N       13119.7002  2131.6799   \n",
      "1998   2025-06-01        18  6-2025       N       13110.9004  2152.7600   \n",
      "1999   2025-06-01        17  6-2025       N       12815.2998  2104.6299   \n",
      "\n",
      "        wz_west  wz_northcentral  wz_farwest   wz_north  wz_southern  \\\n",
      "0     1456.1176       17665.5742   7745.1724  2098.2500    4774.8325   \n",
      "1     1535.3601       18937.5488   7716.1201  2187.9351    5001.8779   \n",
      "2     1637.4351       20113.9238   7670.5352  2298.1899    5248.0527   \n",
      "3     1687.8900       20975.9746   7657.3301  2337.6174    5419.7524   \n",
      "4     1751.6400       21937.4258   7929.2900  2461.0977    5614.1445   \n",
      "...         ...              ...         ...        ...          ...   \n",
      "1995  1854.1700       18101.5000   7395.3101  1985.8199    5394.5098   \n",
      "1996  1941.4900       19276.9004   7561.7500  1866.9000    5474.0098   \n",
      "1997  2010.6300       20198.0996   7864.9502  1953.0400    5614.8398   \n",
      "1998  2036.8800       20273.9004   7941.4902  2260.5300    5761.8799   \n",
      "1999  1950.5200       19762.8008   8034.2598  2197.1399    5847.1802   \n",
      "\n",
      "        wz_coast      lz_north       lz_west      lz_south    lz_houston  \n",
      "0     15830.6494  20784.072440   9737.260024  15910.119744  15277.621401  \n",
      "1     16618.4746  22251.600534   9809.834502  16882.606018  16038.152305  \n",
      "2     17334.3008  23616.953894   9887.404742  17808.591979  16729.169013  \n",
      "3     17830.9004  24612.970052   9931.967546  18455.502232  17208.570056  \n",
      "4     18228.7500  25746.330833  10306.903018  19143.676012  17592.767504  \n",
      "...          ...           ...           ...           ...           ...  \n",
      "1995  17566.0996  21101.665862   9586.275984  19041.134785  16953.902373  \n",
      "1996  18075.1992  22311.667263   9768.879275  19608.416714  17445.356207  \n",
      "1997  18823.3008  23336.470346  10161.879470  20050.838201  18167.052584  \n",
      "1998  19332.0000  23609.558953  10386.779258  20216.422051  18657.580680  \n",
      "1999  19360.9004  23015.373585  10393.899687  19978.362834  18685.094606  \n",
      "\n",
      "[2000 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 select * from vw_loadforecast_by_zone order by 1 desc,2 desc Limit 2000\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0cb94e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              timestamp  interval  temp_the_woodlands_tx  \\\n",
      "0   2025-08-25 00:00:00        24                   30.0   \n",
      "1   2025-08-24 23:00:00        23                   30.7   \n",
      "2   2025-08-24 22:00:00        22                   31.4   \n",
      "3   2025-08-24 21:00:00        21                   32.3   \n",
      "4   2025-08-24 20:00:00        20                   33.3   \n",
      "..                  ...       ...                    ...   \n",
      "187 2025-08-17 05:00:00         5                   25.1   \n",
      "188 2025-08-17 04:00:00         4                   25.5   \n",
      "189 2025-08-17 03:00:00         3                   25.9   \n",
      "190 2025-08-17 02:00:00         2                   26.3   \n",
      "191 2025-08-17 01:00:00         1                   26.5   \n",
      "\n",
      "     hum_the_woodlands_tx  wind_the_woodlands_tx  precip_the_woodlands_tx  \\\n",
      "0                    61.0                   3.70                      0.0   \n",
      "1                    58.0                   4.22                      0.0   \n",
      "2                    54.0                   4.18                      0.0   \n",
      "3                    50.0                   3.76                      0.0   \n",
      "4                    46.0                   3.32                      0.0   \n",
      "..                    ...                    ...                      ...   \n",
      "187                  92.0                   1.20                      0.0   \n",
      "188                  94.0                   0.36                      0.0   \n",
      "189                  92.0                   1.20                      0.0   \n",
      "190                  93.0                   0.72                      0.0   \n",
      "191                  93.0                   1.43                      0.0   \n",
      "\n",
      "     temp_katy_tx  hum_katy_tx  wind_katy_tx  precip_katy_tx  ...  \\\n",
      "0            26.8         77.0          4.31             0.0  ...   \n",
      "1            27.6         73.0          4.59             0.0  ...   \n",
      "2            28.3         69.0          4.78             0.0  ...   \n",
      "3            29.1         64.0          4.88             0.0  ...   \n",
      "4            30.1         60.0          4.88             0.0  ...   \n",
      "..            ...          ...           ...             ...  ...   \n",
      "187          26.1         93.0          0.71             0.0  ...   \n",
      "188          26.2         94.0          1.52             0.0  ...   \n",
      "189          26.7         93.0          0.94             0.0  ...   \n",
      "190          26.8         91.0          2.15             0.0  ...   \n",
      "191          27.3         87.0          1.94             0.0  ...   \n",
      "\n",
      "     wind_friendswood_tx  precip_friendswood_tx  temp_baytown_tx  \\\n",
      "0                   3.01                    0.0             28.9   \n",
      "1                   3.10                    0.0             29.1   \n",
      "2                   3.37                    0.0             29.4   \n",
      "3                   3.58                    0.0             29.6   \n",
      "4                   3.86                    0.0             30.0   \n",
      "..                   ...                    ...              ...   \n",
      "187                 1.80                    0.0             27.1   \n",
      "188                 1.66                    0.0             27.0   \n",
      "189                 2.08                    0.0             27.0   \n",
      "190                 1.00                    0.0             27.5   \n",
      "191                 0.98                    0.0             28.4   \n",
      "\n",
      "     hum_baytown_tx  wind_baytown_tx  precip_baytown_tx  temp_houston_tx  \\\n",
      "0              73.0             4.10                0.0             29.4   \n",
      "1              72.0             4.03                0.0             30.0   \n",
      "2              70.0             4.11                0.0             30.7   \n",
      "3              68.0             4.27                0.0             31.6   \n",
      "4              66.0             4.61                0.0             32.4   \n",
      "..              ...              ...                ...              ...   \n",
      "187            81.0             0.58                0.0             26.6   \n",
      "188            87.0             0.40                0.0             26.8   \n",
      "189            87.0             0.28                0.0             27.1   \n",
      "190            88.0             1.20                0.0             27.2   \n",
      "191            81.0             0.63                0.0             27.1   \n",
      "\n",
      "     hum_houston_tx  wind_houston_tx  precip_houston_tx  \n",
      "0              66.0             4.10                0.0  \n",
      "1              63.0             4.23                0.0  \n",
      "2              59.0             4.03                0.0  \n",
      "3              56.0             3.81                0.0  \n",
      "4              52.0             3.76                0.0  \n",
      "..              ...              ...                ...  \n",
      "187            92.0             1.08                0.0  \n",
      "188            91.0             1.71                0.0  \n",
      "189            91.0             1.53                0.0  \n",
      "190            93.0             0.54                0.0  \n",
      "191            89.0             1.20                0.0  \n",
      "\n",
      "[192 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 select * from vw_forecast_weather_by_city order by 1 desc,2 desc Limit 2000\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af3044",
   "metadata": {},
   "source": [
    "attempt 1 at join:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"-- One-stop wide table for modeling\n",
    "CREATE OR REPLACE VIEW vw_spine_load_wx_price AS\n",
    "WITH spine AS (\n",
    "  SELECT\n",
    "    /* normalize to hour if needed; comment out DATE_TRUNC if already hourly */\n",
    "    DATE_TRUNC('hour', OperatingDTM) AS ts,\n",
    "    OperatingDTM,\n",
    "    Interval AS load_interval,\n",
    "    Month,\n",
    "    DSTFlag,\n",
    "    -- keep your load forecast columns (zones & load zones); include * if you prefer\n",
    "    wz_southcentral, wz_east, wz_west, wz_northcentral, wz_farwest, wz_north, wz_southern, wz_coast,\n",
    "    lz_north, lz_west, lz_south, lz_houston\n",
    "  FROM vw_loadforecast_by_zone\n",
    "),\n",
    "calendar_feats AS (\n",
    "  SELECT\n",
    "    ts,\n",
    "    EXTRACT(hour  FROM ts)::INT  AS cal_hour,\n",
    "    EXTRACT(dow   FROM ts)::INT  AS cal_dow,\n",
    "    EXTRACT(month FROM ts)::INT  AS cal_month_num,\n",
    "    STRFTIME(ts, '%Y-%m')        AS cal_ym,\n",
    "    CASE WHEN EXTRACT(dow FROM ts) IN (0,6) THEN 1 ELSE 0 END AS cal_is_weekend\n",
    "  FROM spine\n",
    "),\n",
    "-- Forecast weather already pivoted wide (your sample shows columns like temp_the_woodlands_tx)\n",
    "wx_forecast AS (\n",
    "  SELECT\n",
    "    DATE_TRUNC('hour', \"timestamp\") AS ts,\n",
    "    interval AS wx_interval,\n",
    "    -- include the columns you have; sample starts with temp_* — add wind/hum/precip columns too if present\n",
    "    *\n",
    "  FROM vw_forecast_weather_by_city\n",
    "),\n",
    "-- Historical weather: if you also have pivoted wide columns, this is fine.\n",
    "-- If your historical view is LONG (city rows), replace this CTE with an aggregate-pivot (see note below).\n",
    "wx_historical AS (\n",
    "  SELECT\n",
    "    DATE_TRUNC('hour', \"timestamp\") AS ts,\n",
    "    interval AS hist_interval,\n",
    "    *\n",
    "  FROM vw_historical_weather_by_city\n",
    "),\n",
    "prices AS (\n",
    "  SELECT\n",
    "    DATE_TRUNC('hour', OperatingDTM) AS ts,\n",
    "    hb_houston\n",
    "  FROM vw_prices_by_hub\n",
    ")\n",
    "SELECT\n",
    "  -- spine (load forecast) = source of truth timeline\n",
    "  s.*,\n",
    "  -- calendar features\n",
    "  cf.* EXCLUDE (ts),\n",
    "  -- forecast weather (left join, future-known)\n",
    "  wf.* EXCLUDE (ts),\n",
    "  -- historical weather (left join, past-only)\n",
    "  wh.* EXCLUDE (ts),\n",
    "  -- prices (left join; will be NULL for future)\n",
    "  p.hb_houston AS price_hb_houston\n",
    "FROM spine s\n",
    "LEFT JOIN calendar_feats cf USING (ts)\n",
    "LEFT JOIN wx_forecast   wf USING (ts)\n",
    "LEFT JOIN wx_historical wh USING (ts)\n",
    "LEFT JOIN prices        p  USING (ts)\n",
    "ORDER BY s.ts;\n",
    "\n",
    "                 \"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388b036c",
   "metadata": {},
   "source": [
    "Export last X days to csv for adhoc analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8150d526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 210240 rows covering 20 days to last20_days_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- config ---\n",
    "DB_PATH = \"../ProjectMain/db/data.duckdb\"   # adjust if different\n",
    "OUT_PATH = Path(\"last20_days_dataset.csv\")\n",
    "\n",
    "# --- connect ---\n",
    "con = duckdb.connect(DB_PATH)\n",
    "\n",
    "# 1. Find last 20 operating dates in vw_spine_load_wx_price\n",
    "dates_query = \"\"\"\n",
    "SELECT DISTINCT DATE(ts) AS operating_date\n",
    "FROM vw_spine_load_wx_price\n",
    "ORDER BY operating_date DESC\n",
    "LIMIT 20;\n",
    "\"\"\"\n",
    "last20_dates = con.execute(dates_query).fetchdf()\n",
    "date_list = last20_dates[\"operating_date\"].tolist()\n",
    "\n",
    "# 2. Query the full dataset for those dates\n",
    "# DuckDB accepts a list via VALUES\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM vw_spine_load_wx_price\n",
    "WHERE DATE(ts) IN (\n",
    "    SELECT UNNEST(LIST_VALUE({','.join([f\"DATE '{d}'\" for d in date_list])}))\n",
    ")\n",
    "ORDER BY ts;\n",
    "\"\"\"\n",
    "\n",
    "df = con.execute(query).fetchdf()\n",
    "\n",
    "# 3. Export to CSV\n",
    "df.to_csv(OUT_PATH, index=False)\n",
    "print(f\"Exported {len(df)} rows covering {len(date_list)} days to {OUT_PATH}\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd22e5f",
   "metadata": {},
   "source": [
    "Not a great file lets start over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2371f7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OperatingDTM  Interval   Month DSTFlag  wz_southcentral    wz_east  \\\n",
      "0    2025-08-18         1  8-2025       N       10345.2998  1994.8000   \n",
      "1    2025-08-18         2  8-2025       N        9684.5996  1886.0500   \n",
      "2    2025-08-18         3  8-2025       N        9151.6699  1806.5300   \n",
      "3    2025-08-18         4  8-2025       N        8843.3496  1754.6300   \n",
      "4    2025-08-18         5  8-2025       N        8732.6504  1729.5000   \n",
      "5    2025-08-18         6  8-2025       N        8838.7402  1767.5000   \n",
      "6    2025-08-18         7  8-2025       N        9106.2305  1845.3700   \n",
      "7    2025-08-18         8  8-2025       N        9247.9404  1864.5200   \n",
      "8    2025-08-18         9  8-2025       N        9665.3301  1984.9800   \n",
      "9    2025-08-18        10  8-2025       N       10330.2002  2191.4299   \n",
      "10   2025-08-18        11  8-2025       N       11081.2998  2370.3401   \n",
      "11   2025-08-18        12  8-2025       N       11862.7002  2560.0701   \n",
      "12   2025-08-18        13  8-2025       N       12620.4004  2744.0000   \n",
      "13   2025-08-18        14  8-2025       N       13276.2998  2867.0701   \n",
      "14   2025-08-18        15  8-2025       N       13719.5000  2962.8601   \n",
      "15   2025-08-18        16  8-2025       N       13962.7998  2990.7800   \n",
      "16   2025-08-18        17  8-2025       N       14122.5000  2980.3501   \n",
      "17   2025-08-18        18  8-2025       N       14207.5000  2932.7800   \n",
      "18   2025-08-18        19  8-2025       N       14244.5000  2894.0300   \n",
      "19   2025-08-18        20  8-2025       N       13924.0000  2795.3000   \n",
      "20   2025-08-18        21  8-2025       N       13440.5000  2701.5500   \n",
      "21   2025-08-18        22  8-2025       N       12908.0000  2556.4399   \n",
      "22   2025-08-18        23  8-2025       N       12043.7998  2373.4399   \n",
      "23   2025-08-18        24  8-2025       N       11139.7002  2206.1499   \n",
      "\n",
      "      wz_west  wz_northcentral  wz_farwest   wz_north  ...  cal_year  \\\n",
      "0   1572.1100       18137.3008   7693.4800  2240.9299  ...      2025   \n",
      "1   1494.6100       16991.5000   7641.0298  2148.0801  ...      2025   \n",
      "2   1438.0200       16109.2002   7679.6201  2074.9099  ...      2025   \n",
      "3   1383.0500       15601.2002   7611.6602  2027.2700  ...      2025   \n",
      "4   1360.3199       15417.2998   7560.7900  2006.0699  ...      2025   \n",
      "5   1380.6300       15672.7002   7464.0898  2019.8900  ...      2025   \n",
      "6   1404.5400       16223.4004   7439.5400  2071.8999  ...      2025   \n",
      "7   1431.9200       16488.8008   7474.3599  2103.5500  ...      2025   \n",
      "8   1496.5800       17472.1992   8196.5400  2254.7400  ...      2025   \n",
      "9   1619.4600       18916.0000   8699.4502  2369.7800  ...      2025   \n",
      "10  1742.0200       20438.0996   8688.0898  2470.0300  ...      2025   \n",
      "11  1832.6801       22021.8008   8549.9297  2555.6799  ...      2025   \n",
      "12  1886.0000       23559.5000   8476.6201  2669.0100  ...      2025   \n",
      "13  1972.4200       24884.3008   8357.4004  2764.5100  ...      2025   \n",
      "14  2029.7700       25800.3008   8097.2402  2752.8401  ...      2025   \n",
      "15  2074.1101       26332.5996   7878.9702  2733.0901  ...      2025   \n",
      "16  2089.2900       26638.9004   7739.0098  2708.8999  ...      2025   \n",
      "17  2067.3899       26652.0996   7617.3999  2676.0901  ...      2025   \n",
      "18  2044.1100       26219.4004   7576.7900  2665.1599  ...      2025   \n",
      "19  1961.4500       25135.3008   7412.8398  2615.4800  ...      2025   \n",
      "20  1887.4700       23977.5000   7310.1699  2520.2500  ...      2025   \n",
      "21  1819.6300       22850.4004   7452.9600  2474.8101  ...      2025   \n",
      "22  1698.2200       21221.3008   7594.8301  2385.9099  ...      2025   \n",
      "23  1584.1400       19550.1992   7689.3301  2288.4399  ...      2025   \n",
      "\n",
      "    cal_month  cal_dow  cal_is_weekend  cal_hour  cal_hour_of_week  \\\n",
      "0           8        1               0         1                24   \n",
      "1           8        1               0         2                25   \n",
      "2           8        1               0         3                26   \n",
      "3           8        1               0         4                27   \n",
      "4           8        1               0         5                28   \n",
      "5           8        1               0         6                29   \n",
      "6           8        1               0         7                30   \n",
      "7           8        1               0         8                31   \n",
      "8           8        1               0         9                32   \n",
      "9           8        1               0        10                33   \n",
      "10          8        1               0        11                34   \n",
      "11          8        1               0        12                35   \n",
      "12          8        1               0        13                36   \n",
      "13          8        1               0        14                37   \n",
      "14          8        1               0        15                38   \n",
      "15          8        1               0        16                39   \n",
      "16          8        1               0        17                40   \n",
      "17          8        1               0        18                41   \n",
      "18          8        1               0        19                42   \n",
      "19          8        1               0        20                43   \n",
      "20          8        1               0        21                44   \n",
      "21          8        1               0        22                45   \n",
      "22          8        1               0        23                46   \n",
      "23          8        1               0        24                47   \n",
      "\n",
      "    cal_sin_hour  cal_cos_hour  cal_sin_dow  cal_cos_dow  \n",
      "0   2.588190e-01  9.659258e-01     0.781831      0.62349  \n",
      "1   5.000000e-01  8.660254e-01     0.781831      0.62349  \n",
      "2   7.071068e-01  7.071068e-01     0.781831      0.62349  \n",
      "3   8.660254e-01  5.000000e-01     0.781831      0.62349  \n",
      "4   9.659258e-01  2.588190e-01     0.781831      0.62349  \n",
      "5   1.000000e+00  6.123234e-17     0.781831      0.62349  \n",
      "6   9.659258e-01 -2.588190e-01     0.781831      0.62349  \n",
      "7   8.660254e-01 -5.000000e-01     0.781831      0.62349  \n",
      "8   7.071068e-01 -7.071068e-01     0.781831      0.62349  \n",
      "9   5.000000e-01 -8.660254e-01     0.781831      0.62349  \n",
      "10  2.588190e-01 -9.659258e-01     0.781831      0.62349  \n",
      "11  1.224647e-16 -1.000000e+00     0.781831      0.62349  \n",
      "12 -2.588190e-01 -9.659258e-01     0.781831      0.62349  \n",
      "13 -5.000000e-01 -8.660254e-01     0.781831      0.62349  \n",
      "14 -7.071068e-01 -7.071068e-01     0.781831      0.62349  \n",
      "15 -8.660254e-01 -5.000000e-01     0.781831      0.62349  \n",
      "16 -9.659258e-01 -2.588190e-01     0.781831      0.62349  \n",
      "17 -1.000000e+00 -1.836970e-16     0.781831      0.62349  \n",
      "18 -9.659258e-01  2.588190e-01     0.781831      0.62349  \n",
      "19 -8.660254e-01  5.000000e-01     0.781831      0.62349  \n",
      "20 -7.071068e-01  7.071068e-01     0.781831      0.62349  \n",
      "21 -5.000000e-01  8.660254e-01     0.781831      0.62349  \n",
      "22 -2.588190e-01  9.659258e-01     0.781831      0.62349  \n",
      "23 -2.449294e-16  1.000000e+00     0.781831      0.62349  \n",
      "\n",
      "[24 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"SELECT\n",
    "    lf.*,\n",
    "    -- calendar features from the date\n",
    "    EXTRACT(year  FROM OperatingDTM)::INT  AS cal_year,\n",
    "    EXTRACT(month FROM OperatingDTM)::INT  AS cal_month,\n",
    "    EXTRACT(dow   FROM OperatingDTM)::INT  AS cal_dow,    -- 0=Sunday .. 6=Saturday\n",
    "    CASE WHEN EXTRACT(dow FROM OperatingDTM) IN (0,6) THEN 1 ELSE 0 END AS cal_is_weekend,\n",
    "\n",
    "    -- hour features from Interval\n",
    "    Interval AS cal_hour,\n",
    "    ((EXTRACT(dow FROM OperatingDTM)::INT) * 24 + (Interval - 1)) AS cal_hour_of_week,\n",
    "\n",
    "    -- optional cyclic encodings\n",
    "    SIN(2*PI() * Interval/24.0)                  AS cal_sin_hour,\n",
    "    COS(2*PI() * Interval/24.0)                  AS cal_cos_hour,\n",
    "    SIN(2*PI() * EXTRACT(dow FROM OperatingDTM)/7.0) AS cal_sin_dow,\n",
    "    COS(2*PI() * EXTRACT(dow FROM OperatingDTM)/7.0) AS cal_cos_dow\n",
    "\n",
    "FROM vw_loadforecast_by_zone lf\n",
    "WHERE OperatingDTM = DATE '2025-08-18'\n",
    "ORDER BY Interval;\n",
    "\n",
    "                 \"\"\").fetchdf()\n",
    "print(df)\n",
    "#print(df.describe())\n",
    "#print(df.info)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c941b",
   "metadata": {},
   "source": [
    "Okay that works for me. Lets add the left join of the vw_forecast_weather_by_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d40cf630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OperatingDTM           timestamp  interval  temp_the_woodlands_tx  \\\n",
      "0    2025-08-24 2025-08-25 00:00:00        24                   30.0   \n",
      "1    2025-08-24 2025-08-24 23:00:00        23                   30.7   \n",
      "2    2025-08-24 2025-08-24 22:00:00        22                   31.4   \n",
      "3    2025-08-24 2025-08-24 21:00:00        21                   32.3   \n",
      "4    2025-08-24 2025-08-24 20:00:00        20                   33.3   \n",
      "5    2025-08-24 2025-08-24 19:00:00        19                   34.7   \n",
      "6    2025-08-24 2025-08-24 18:00:00        18                   36.3   \n",
      "7    2025-08-24 2025-08-24 17:00:00        17                   37.2   \n",
      "8    2025-08-24 2025-08-24 16:00:00        16                   37.0   \n",
      "9    2025-08-24 2025-08-24 15:00:00        15                   36.2   \n",
      "10   2025-08-24 2025-08-24 14:00:00        14                   35.1   \n",
      "11   2025-08-24 2025-08-24 13:00:00        13                   33.6   \n",
      "12   2025-08-24 2025-08-24 12:00:00        12                   31.8   \n",
      "13   2025-08-24 2025-08-24 11:00:00        11                   30.1   \n",
      "14   2025-08-24 2025-08-24 10:00:00        10                   28.5   \n",
      "15   2025-08-24 2025-08-24 09:00:00         9                   27.0   \n",
      "16   2025-08-24 2025-08-24 08:00:00         8                   26.1   \n",
      "17   2025-08-24 2025-08-24 07:00:00         7                   26.0   \n",
      "18   2025-08-24 2025-08-24 06:00:00         6                   26.6   \n",
      "19   2025-08-24 2025-08-24 05:00:00         5                   27.1   \n",
      "20   2025-08-24 2025-08-24 04:00:00         4                   27.4   \n",
      "21   2025-08-24 2025-08-24 03:00:00         3                   27.7   \n",
      "22   2025-08-24 2025-08-24 02:00:00         2                   28.1   \n",
      "23   2025-08-24 2025-08-24 01:00:00         1                   28.7   \n",
      "24   2025-08-23 2025-08-24 00:00:00        24                   29.4   \n",
      "\n",
      "    hum_the_woodlands_tx  wind_the_woodlands_tx  precip_the_woodlands_tx  \\\n",
      "0                   61.0                   3.70                      0.0   \n",
      "1                   58.0                   4.22                      0.0   \n",
      "2                   54.0                   4.18                      0.0   \n",
      "3                   50.0                   3.76                      0.0   \n",
      "4                   46.0                   3.32                      0.0   \n",
      "5                   41.0                   2.69                      0.0   \n",
      "6                   35.0                   1.90                      0.0   \n",
      "7                   32.0                   1.43                      0.0   \n",
      "8                   32.0                   1.41                      0.0   \n",
      "9                   35.0                   1.63                      0.0   \n",
      "10                  39.0                   1.81                      0.0   \n",
      "11                  45.0                   1.71                      0.0   \n",
      "12                  53.0                   1.71                      0.0   \n",
      "13                  60.0                   1.72                      0.0   \n",
      "14                  65.0                   1.63                      0.0   \n",
      "15                  70.0                   1.42                      0.0   \n",
      "16                  73.0                   1.36                      0.0   \n",
      "17                  73.0                   1.50                      0.0   \n",
      "18                  72.0                   1.64                      0.0   \n",
      "19                  71.0                   1.84                      0.0   \n",
      "20                  70.0                   1.98                      0.0   \n",
      "21                  69.0                   2.05                      0.0   \n",
      "22                  68.0                   2.16                      0.0   \n",
      "23                  65.0                   2.38                      0.0   \n",
      "24                  62.0                   2.75                      0.0   \n",
      "\n",
      "    temp_katy_tx  hum_katy_tx  wind_katy_tx  ...  wind_friendswood_tx  \\\n",
      "0           26.8         77.0          4.31  ...                 3.01   \n",
      "1           27.6         73.0          4.59  ...                 3.10   \n",
      "2           28.3         69.0          4.78  ...                 3.37   \n",
      "3           29.1         64.0          4.88  ...                 3.58   \n",
      "4           30.1         60.0          4.88  ...                 3.86   \n",
      "5           31.4         55.0          4.70  ...                 3.92   \n",
      "6           32.8         50.0          4.55  ...                 3.75   \n",
      "7           33.8         46.0          4.05  ...                 3.33   \n",
      "8           34.2         44.0          3.34  ...                 2.50   \n",
      "9           34.3         44.0          2.52  ...                 1.39   \n",
      "10          33.8         46.0          1.88  ...                 0.58   \n",
      "11          32.5         53.0          1.61  ...                 0.61   \n",
      "12          30.8         62.0          1.84  ...                 1.22   \n",
      "13          29.0         71.0          1.97  ...                 1.65   \n",
      "14          27.1         78.0          1.75  ...                 1.39   \n",
      "15          25.3         83.0          1.46  ...                 1.00   \n",
      "16          24.0         87.0          1.33  ...                 0.86   \n",
      "17          23.6         88.0          1.49  ...                 1.08   \n",
      "18          23.7         86.0          1.92  ...                 1.36   \n",
      "19          24.0         85.0          2.20  ...                 1.63   \n",
      "20          24.2         85.0          2.19  ...                 1.84   \n",
      "21          24.5         84.0          2.04  ...                 2.06   \n",
      "22          25.0         83.0          2.30  ...                 2.44   \n",
      "23          25.7         79.0          3.10  ...                 2.94   \n",
      "24          26.7         75.0          4.10  ...                 3.50   \n",
      "\n",
      "    precip_friendswood_tx  temp_baytown_tx  hum_baytown_tx  wind_baytown_tx  \\\n",
      "0                     0.0             28.9            73.0             4.10   \n",
      "1                     0.0             29.1            72.0             4.03   \n",
      "2                     0.0             29.4            70.0             4.11   \n",
      "3                     0.0             29.6            68.0             4.27   \n",
      "4                     0.0             30.0            66.0             4.61   \n",
      "5                     0.0             30.7            63.0             4.79   \n",
      "6                     0.0             31.5            60.0             4.84   \n",
      "7                     0.2             32.0            57.0             4.71   \n",
      "8                     0.2             32.2            55.0             4.21   \n",
      "9                     0.2             32.1            55.0             3.89   \n",
      "10                    0.0             31.8            55.0             3.67   \n",
      "11                    0.0             31.3            57.0             2.66   \n",
      "12                    0.0             30.6            61.0             1.66   \n",
      "13                    0.0             29.7            65.0             1.52   \n",
      "14                    0.0             28.5            70.0             1.63   \n",
      "15                    0.0             27.1            76.0             1.61   \n",
      "16                    0.0             26.2            80.0             1.71   \n",
      "17                    0.0             26.2            80.0             1.93   \n",
      "18                    0.0             26.6            79.0             2.10   \n",
      "19                    0.0             27.0            77.0             2.30   \n",
      "20                    0.0             27.3            75.0             2.55   \n",
      "21                    0.0             27.6            74.0             2.97   \n",
      "22                    0.0             27.9            72.0             3.40   \n",
      "23                    0.0             28.2            71.0             3.86   \n",
      "24                    0.0             28.5            71.0             4.36   \n",
      "\n",
      "    precip_baytown_tx  temp_houston_tx  hum_houston_tx  wind_houston_tx  \\\n",
      "0                 0.0             29.4            66.0             4.10   \n",
      "1                 0.0             30.0            63.0             4.23   \n",
      "2                 0.0             30.7            59.0             4.03   \n",
      "3                 0.0             31.6            56.0             3.81   \n",
      "4                 0.0             32.4            52.0             3.76   \n",
      "5                 0.0             32.9            49.0             4.34   \n",
      "6                 0.0             33.4            47.0             5.13   \n",
      "7                 0.0             34.0            45.0             5.35   \n",
      "8                 0.0             35.0            41.0             4.15   \n",
      "9                 0.0             36.1            38.0             2.16   \n",
      "10                0.0             36.4            37.0             0.90   \n",
      "11                0.0             35.1            42.0             0.95   \n",
      "12                0.0             32.9            50.0             1.49   \n",
      "13                0.0             30.9            57.0             1.90   \n",
      "14                0.0             29.3            62.0             1.71   \n",
      "15                0.0             27.9            67.0             1.34   \n",
      "16                0.0             27.0            70.0             1.17   \n",
      "17                0.0             26.8            70.0             1.44   \n",
      "18                0.0             27.2            68.0             1.94   \n",
      "19                0.0             27.6            67.0             2.33   \n",
      "20                0.0             27.8            67.0             2.59   \n",
      "21                0.0             28.0            68.0             2.69   \n",
      "22                0.0             28.2            69.0             3.01   \n",
      "23                0.0             28.3            69.0             3.62   \n",
      "24                0.0             28.5            69.0             4.30   \n",
      "\n",
      "    precip_houston_tx  \n",
      "0                 0.0  \n",
      "1                 0.0  \n",
      "2                 0.0  \n",
      "3                 0.0  \n",
      "4                 0.0  \n",
      "5                 0.0  \n",
      "6                 0.0  \n",
      "7                 0.0  \n",
      "8                 0.0  \n",
      "9                 0.0  \n",
      "10                0.0  \n",
      "11                0.0  \n",
      "12                0.0  \n",
      "13                0.0  \n",
      "14                0.0  \n",
      "15                0.0  \n",
      "16                0.0  \n",
      "17                0.0  \n",
      "18                0.0  \n",
      "19                0.0  \n",
      "20                0.0  \n",
      "21                0.0  \n",
      "22                0.0  \n",
      "23                0.0  \n",
      "24                0.0  \n",
      "\n",
      "[25 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 select * from vw_forecast_weather_by_city  order by 1 desc,2 desc Limit 25\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64332cd7",
   "metadata": {},
   "source": [
    "Joining forecast weather onto forecast MW + calendar features query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba79625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OperatingDTM  Interval   Month DSTFlag  wz_southcentral    wz_east  \\\n",
      "0    2025-08-18         1  8-2025       N       10345.2998  1994.8000   \n",
      "1    2025-08-18         2  8-2025       N        9684.5996  1886.0500   \n",
      "2    2025-08-18         3  8-2025       N        9151.6699  1806.5300   \n",
      "3    2025-08-18         4  8-2025       N        8843.3496  1754.6300   \n",
      "4    2025-08-18         5  8-2025       N        8732.6504  1729.5000   \n",
      "5    2025-08-18         6  8-2025       N        8838.7402  1767.5000   \n",
      "6    2025-08-18         7  8-2025       N        9106.2305  1845.3700   \n",
      "7    2025-08-18         8  8-2025       N        9247.9404  1864.5200   \n",
      "8    2025-08-18         9  8-2025       N        9665.3301  1984.9800   \n",
      "9    2025-08-18        10  8-2025       N       10330.2002  2191.4299   \n",
      "10   2025-08-18        11  8-2025       N       11081.2998  2370.3401   \n",
      "11   2025-08-18        12  8-2025       N       11862.7002  2560.0701   \n",
      "12   2025-08-18        13  8-2025       N       12620.4004  2744.0000   \n",
      "13   2025-08-18        14  8-2025       N       13276.2998  2867.0701   \n",
      "14   2025-08-18        15  8-2025       N       13719.5000  2962.8601   \n",
      "15   2025-08-18        16  8-2025       N       13962.7998  2990.7800   \n",
      "16   2025-08-18        17  8-2025       N       14122.5000  2980.3501   \n",
      "17   2025-08-18        18  8-2025       N       14207.5000  2932.7800   \n",
      "18   2025-08-18        19  8-2025       N       14244.5000  2894.0300   \n",
      "19   2025-08-18        20  8-2025       N       13924.0000  2795.3000   \n",
      "20   2025-08-18        21  8-2025       N       13440.5000  2701.5500   \n",
      "21   2025-08-18        22  8-2025       N       12908.0000  2556.4399   \n",
      "22   2025-08-18        23  8-2025       N       12043.7998  2373.4399   \n",
      "23   2025-08-18        24  8-2025       N       11139.7002  2206.1499   \n",
      "\n",
      "      wz_west  wz_northcentral  wz_farwest   wz_north  ...  \\\n",
      "0   1572.1100       18137.3008   7693.4800  2240.9299  ...   \n",
      "1   1494.6100       16991.5000   7641.0298  2148.0801  ...   \n",
      "2   1438.0200       16109.2002   7679.6201  2074.9099  ...   \n",
      "3   1383.0500       15601.2002   7611.6602  2027.2700  ...   \n",
      "4   1360.3199       15417.2998   7560.7900  2006.0699  ...   \n",
      "5   1380.6300       15672.7002   7464.0898  2019.8900  ...   \n",
      "6   1404.5400       16223.4004   7439.5400  2071.8999  ...   \n",
      "7   1431.9200       16488.8008   7474.3599  2103.5500  ...   \n",
      "8   1496.5800       17472.1992   8196.5400  2254.7400  ...   \n",
      "9   1619.4600       18916.0000   8699.4502  2369.7800  ...   \n",
      "10  1742.0200       20438.0996   8688.0898  2470.0300  ...   \n",
      "11  1832.6801       22021.8008   8549.9297  2555.6799  ...   \n",
      "12  1886.0000       23559.5000   8476.6201  2669.0100  ...   \n",
      "13  1972.4200       24884.3008   8357.4004  2764.5100  ...   \n",
      "14  2029.7700       25800.3008   8097.2402  2752.8401  ...   \n",
      "15  2074.1101       26332.5996   7878.9702  2733.0901  ...   \n",
      "16  2089.2900       26638.9004   7739.0098  2708.8999  ...   \n",
      "17  2067.3899       26652.0996   7617.3999  2676.0901  ...   \n",
      "18  2044.1100       26219.4004   7576.7900  2665.1599  ...   \n",
      "19  1961.4500       25135.3008   7412.8398  2615.4800  ...   \n",
      "20  1887.4700       23977.5000   7310.1699  2520.2500  ...   \n",
      "21  1819.6300       22850.4004   7452.9600  2474.8101  ...   \n",
      "22  1698.2200       21221.3008   7594.8301  2385.9099  ...   \n",
      "23  1584.1400       19550.1992   7689.3301  2288.4399  ...   \n",
      "\n",
      "    wind_friendswood_tx  precip_friendswood_tx  temp_baytown_tx  \\\n",
      "0                  1.50                    0.0             27.8   \n",
      "1                  1.39                    0.0             27.5   \n",
      "2                  1.75                    0.0             27.0   \n",
      "3                  1.12                    0.0             26.4   \n",
      "4                  1.14                    0.0             26.1   \n",
      "5                  1.50                    0.0             25.8   \n",
      "6                  1.66                    0.0             25.5   \n",
      "7                  1.35                    0.0             25.4   \n",
      "8                  1.03                    0.0             27.3   \n",
      "9                  1.84                    0.0             29.5   \n",
      "10                 2.01                    0.0             31.3   \n",
      "11                 1.22                    0.0             33.1   \n",
      "12                 0.51                    0.0             34.5   \n",
      "13                 1.30                    0.0             35.3   \n",
      "14                 3.42                    0.0             35.6   \n",
      "15                 3.45                    0.0             36.2   \n",
      "16                 3.41                    0.0             36.0   \n",
      "17                 3.31                    0.0             35.9   \n",
      "18                 3.52                    0.0             35.1   \n",
      "19                 3.77                    0.0             32.8   \n",
      "20                 3.69                    0.0             31.3   \n",
      "21                 3.18                    0.0             30.3   \n",
      "22                 2.28                    0.0             29.4   \n",
      "23                 1.72                    0.0             28.9   \n",
      "\n",
      "    hum_baytown_tx  wind_baytown_tx  precip_baytown_tx  temp_houston_tx  \\\n",
      "0             82.0             1.86                0.0             26.9   \n",
      "1             83.0             1.84                0.0             26.6   \n",
      "2             84.0             1.77                0.0             26.3   \n",
      "3             86.0             1.26                0.0             26.0   \n",
      "4             88.0             1.42                0.0             25.7   \n",
      "5             89.0             1.10                0.0             25.5   \n",
      "6             92.0             1.39                0.0             25.2   \n",
      "7             93.0             1.44                0.0             25.1   \n",
      "8             87.0             1.53                0.0             26.3   \n",
      "9             76.0             2.06                0.0             27.7   \n",
      "10            69.0             2.01                0.0             28.9   \n",
      "11            61.0             1.27                0.0             30.1   \n",
      "12            56.0             1.00                0.0             31.4   \n",
      "13            52.0             1.30                0.0             32.8   \n",
      "14            53.0             2.01                0.0             34.2   \n",
      "15            47.0             3.37                0.0             35.3   \n",
      "16            44.0             3.83                0.0             36.4   \n",
      "17            43.0             2.11                0.0             36.8   \n",
      "18            45.0             2.39                0.0             34.5   \n",
      "19            63.0             3.58                0.0             32.0   \n",
      "20            67.0             3.22                0.0             30.2   \n",
      "21            72.0             3.04                0.0             29.5   \n",
      "22            75.0             2.00                0.0             29.4   \n",
      "23            76.0             2.02                0.0             28.1   \n",
      "\n",
      "    hum_houston_tx  wind_houston_tx  precip_houston_tx  \n",
      "0             82.0             2.06                0.0  \n",
      "1             83.0             2.02                0.0  \n",
      "2             85.0             1.53                0.0  \n",
      "3             88.0             1.58                0.0  \n",
      "4             89.0             1.08                0.0  \n",
      "5             90.0             0.71                0.0  \n",
      "6             91.0             0.70                0.0  \n",
      "7             93.0             1.58                0.0  \n",
      "8             94.0             1.21                0.0  \n",
      "9             91.0             1.58                0.0  \n",
      "10            83.0             1.88                0.0  \n",
      "11            77.0             1.75                0.0  \n",
      "12            71.0             1.35                0.0  \n",
      "13            61.0             0.82                0.0  \n",
      "14            53.0             0.81                0.0  \n",
      "15            48.0             0.22                0.0  \n",
      "16            44.0             0.61                0.0  \n",
      "17            38.0             0.82                0.0  \n",
      "18            49.0             4.14                0.0  \n",
      "19            55.0             2.50                0.0  \n",
      "20            64.0             2.84                0.0  \n",
      "21            67.0             1.17                0.0  \n",
      "22            68.0             0.36                0.0  \n",
      "23            73.0             2.28                0.0  \n",
      "\n",
      "[24 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "SELECT\n",
    "    lf.*,\n",
    "\n",
    "    -- calendar features from the date\n",
    "    EXTRACT(year  FROM lf.OperatingDTM)::INT  AS cal_year,\n",
    "    EXTRACT(month FROM lf.OperatingDTM)::INT  AS cal_month,\n",
    "    EXTRACT(dow   FROM lf.OperatingDTM)::INT  AS cal_dow,    -- 0=Sunday .. 6=Saturday\n",
    "    CASE WHEN EXTRACT(dow FROM lf.OperatingDTM) IN (0,6) THEN 1 ELSE 0 END AS cal_is_weekend,\n",
    "\n",
    "    -- hour features from Interval (since OperatingDTM is a date)\n",
    "    lf.Interval AS cal_hour,\n",
    "    ((EXTRACT(dow FROM lf.OperatingDTM)::INT) * 24 + (lf.Interval - 1)) AS cal_hour_of_week,\n",
    "\n",
    "    -- optional cyclic encodings\n",
    "    SIN(2*PI() * lf.Interval/24.0)                        AS cal_sin_hour,\n",
    "    COS(2*PI() * lf.Interval/24.0)                        AS cal_cos_hour,\n",
    "    SIN(2*PI() * EXTRACT(dow FROM lf.OperatingDTM)/7.0)   AS cal_sin_dow,\n",
    "    COS(2*PI() * EXTRACT(dow FROM lf.OperatingDTM)/7.0)   AS cal_cos_dow,\n",
    "\n",
    "    -- forecast weather columns (already pivoted wide). Exclude join keys.\n",
    "    wfc.* EXCLUDE (OperatingDTM, \"timestamp\", \"interval\")\n",
    "\n",
    "FROM vw_loadforecast_by_zone lf\n",
    "LEFT JOIN vw_forecast_weather_by_city wfc\n",
    "  ON wfc.OperatingDTM = lf.OperatingDTM\n",
    " AND wfc.\"interval\"   = lf.Interval\n",
    "\n",
    "WHERE lf.OperatingDTM = DATE '2025-08-18'\n",
    "ORDER BY lf.Interval;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b46cf",
   "metadata": {},
   "source": [
    "good, now lets review historic weather data before joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ba166d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               timestamp  interval  temp_the_woodlands_tx  \\\n",
      "0    2025-08-16 20:00:00        20                   30.0   \n",
      "1    2025-08-16 19:00:00        19                   31.1   \n",
      "2    2025-08-16 18:00:00        18                   31.7   \n",
      "3    2025-08-16 17:00:00        17                   29.4   \n",
      "4    2025-08-16 16:00:00        16                   27.8   \n",
      "...                  ...       ...                    ...   \n",
      "1995 2025-05-25 17:00:00        17                   33.3   \n",
      "1996 2025-05-25 16:00:00        16                   33.9   \n",
      "1997 2025-05-25 15:00:00        15                   33.3   \n",
      "1998 2025-05-25 14:00:00        14                   32.2   \n",
      "1999 2025-05-25 13:00:00        13                   32.2   \n",
      "\n",
      "      hum_the_woodlands_tx  wind_the_woodlands_tx  precip_the_woodlands_tx  \\\n",
      "0                     72.0               1.388889                      0.0   \n",
      "1                     66.0               1.944444                      0.2   \n",
      "2                     68.0               1.666667                      0.3   \n",
      "3                     77.0               1.666667                      0.7   \n",
      "4                     82.0               0.833333                      0.0   \n",
      "...                    ...                    ...                      ...   \n",
      "1995                  51.0               4.611111                      0.0   \n",
      "1996                  50.0               3.611111                      0.0   \n",
      "1997                  52.0               3.611111                      0.0   \n",
      "1998                  56.0               4.111111                      0.0   \n",
      "1999                  58.0               4.111111                      0.0   \n",
      "\n",
      "      temp_katy_tx  hum_katy_tx  wind_katy_tx  precip_katy_tx  ...  \\\n",
      "0             32.0         71.0      2.777778             0.0  ...   \n",
      "1             32.0         71.0      2.222222             1.2  ...   \n",
      "2             32.0         71.0      2.500000             0.0  ...   \n",
      "3             32.0         67.0      1.944444             0.0  ...   \n",
      "4             29.0         84.0      1.388889             0.0  ...   \n",
      "...            ...          ...           ...             ...  ...   \n",
      "1995          35.0         44.0      7.694444             0.2  ...   \n",
      "1996          34.0         50.0      5.694444             0.0  ...   \n",
      "1997          34.0         50.0      8.194444             0.0  ...   \n",
      "1998          34.0         53.0      7.194444             0.0  ...   \n",
      "1999          33.0         56.0      6.694444             0.0  ...   \n",
      "\n",
      "      wind_friendswood_tx  precip_friendswood_tx  temp_baytown_tx  \\\n",
      "0                2.694444                    0.0             30.8   \n",
      "1                3.694444                    0.0             31.1   \n",
      "2                4.000000                    2.6             31.4   \n",
      "3                4.111111                    0.2             29.7   \n",
      "4                3.611111                    0.2             28.4   \n",
      "...                   ...                    ...              ...   \n",
      "1995             7.194444                    0.0             32.0   \n",
      "1996             6.694444                    0.0             32.0   \n",
      "1997             5.694444                    0.0             32.0   \n",
      "1998             6.194444                    0.0             32.0   \n",
      "1999             6.194444                    0.0             31.0   \n",
      "\n",
      "      hum_baytown_tx  wind_baytown_tx  precip_baytown_tx  temp_houston_tx  \\\n",
      "0               73.0         2.222222                0.0             30.7   \n",
      "1               74.0         1.666667                0.0             30.8   \n",
      "2               71.0         0.833333                0.5             31.8   \n",
      "3               78.0         0.000000                0.3             33.4   \n",
      "4               86.0         0.000000                0.0             34.2   \n",
      "...              ...              ...                ...              ...   \n",
      "1995            60.0         7.777778                1.0             31.0   \n",
      "1996            57.0         7.777778                1.4             32.0   \n",
      "1997            60.0         5.555556                0.9             32.0   \n",
      "1998            60.0         7.777778                0.0             31.0   \n",
      "1999            63.0         7.222222                0.0             31.0   \n",
      "\n",
      "      hum_houston_tx  wind_houston_tx  precip_houston_tx  \n",
      "0               69.0         3.611111                0.0  \n",
      "1               68.0         3.805556                0.3  \n",
      "2               62.0         4.500000                0.1  \n",
      "3               56.0         3.111111                0.4  \n",
      "4               55.0         2.611111                0.0  \n",
      "...              ...              ...                ...  \n",
      "1995            63.0         8.333333                0.3  \n",
      "1996            56.0         9.166667                0.0  \n",
      "1997            56.0         7.222222                0.1  \n",
      "1998            59.0         7.777778                0.0  \n",
      "1999            63.0         9.166667                0.0  \n",
      "\n",
      "[2000 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 select * from vw_historical_weather_by_city order by 1 desc,2 desc Limit 2000\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a74032",
   "metadata": {},
   "source": [
    "make the same update to historic weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10980914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View vw_historical_weather_by_city updated with OperatingDTM (hist_* columns).\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW vw_historical_weather_by_city AS\n",
    "WITH labeled AS (\n",
    "  SELECT\n",
    "      h.\"timestamp\",\n",
    "      h.\"interval\",\n",
    "      /* OperatingDTM logic:\n",
    "         - If time = 00:00:00, assign to prior operating day\n",
    "         - Else assign to same day\n",
    "      */\n",
    "      CASE\n",
    "        WHEN STRFTIME(h.\"timestamp\", '%H:%M:%S') = '00:00:00'\n",
    "          THEN CAST(DATE_TRUNC('day', h.\"timestamp\") - INTERVAL 1 DAY AS DATE)\n",
    "        ELSE CAST(DATE_TRUNC('day', h.\"timestamp\") AS DATE)\n",
    "      END AS OperatingDTM,\n",
    "      h.lat,\n",
    "      h.lon,\n",
    "      COALESCE(c.city, 'unmapped') AS city_name,\n",
    "      REGEXP_REPLACE(LOWER(COALESCE(c.city, 'unmapped')), '[^a-z0-9]+', '_') AS city_key,\n",
    "      h.temperature,\n",
    "      h.humidity,\n",
    "      h.windspeed,\n",
    "      h.precipitation\n",
    "  FROM historical_weather h\n",
    "  LEFT JOIN city_coords c\n",
    "    ON ROUND(h.lat, 4) = ROUND(c.lat, 4)\n",
    "   AND ROUND(h.lon, 4) = ROUND(c.lon, 4)\n",
    "),\n",
    "latest AS (\n",
    "  -- If duplicates exist per delivery hour & city, keep the most recent observation\n",
    "  SELECT *\n",
    "  FROM (\n",
    "    SELECT\n",
    "      labeled.*,\n",
    "      ROW_NUMBER() OVER (\n",
    "        PARTITION BY \"timestamp\", \"interval\", city_name\n",
    "        ORDER BY \"timestamp\" DESC\n",
    "      ) AS rn\n",
    "    FROM labeled\n",
    "  )\n",
    "  WHERE rn = 1\n",
    ")\n",
    "SELECT\n",
    "  OperatingDTM,\n",
    "  \"timestamp\",\n",
    "  \"interval\",\n",
    "\n",
    "  /* The_Woodlands_TX */\n",
    "  AVG(CASE WHEN city_name='The_Woodlands_TX' THEN temperature   END) AS hist_temp_the_woodlands_tx,\n",
    "  AVG(CASE WHEN city_name='The_Woodlands_TX' THEN humidity      END) AS hist_hum_the_woodlands_tx,\n",
    "  AVG(CASE WHEN city_name='The_Woodlands_TX' THEN windspeed     END) AS hist_wind_the_woodlands_tx,\n",
    "  AVG(CASE WHEN city_name='The_Woodlands_TX' THEN precipitation END) AS hist_precip_the_woodlands_tx,\n",
    "\n",
    "  /* Katy_TX */\n",
    "  AVG(CASE WHEN city_name='Katy_TX' THEN temperature   END) AS hist_temp_katy_tx,\n",
    "  AVG(CASE WHEN city_name='Katy_TX' THEN humidity      END) AS hist_hum_katy_tx,\n",
    "  AVG(CASE WHEN city_name='Katy_TX' THEN windspeed     END) AS hist_wind_katy_tx,\n",
    "  AVG(CASE WHEN city_name='Katy_TX' THEN precipitation END) AS hist_precip_katy_tx,\n",
    "\n",
    "  /* Friendswood_TX */\n",
    "  AVG(CASE WHEN city_name='Friendswood_TX' THEN temperature   END) AS hist_temp_friendswood_tx,\n",
    "  AVG(CASE WHEN city_name='Friendswood_TX' THEN humidity      END) AS hist_hum_friendswood_tx,\n",
    "  AVG(CASE WHEN city_name='Friendswood_TX' THEN windspeed     END) AS hist_wind_friendswood_tx,\n",
    "  AVG(CASE WHEN city_name='Friendswood_TX' THEN precipitation END) AS hist_precip_friendswood_tx,\n",
    "\n",
    "  /* Baytown_TX */\n",
    "  AVG(CASE WHEN city_name='Baytown_TX' THEN temperature   END) AS hist_temp_baytown_tx,\n",
    "  AVG(CASE WHEN city_name='Baytown_TX' THEN humidity      END) AS hist_hum_baytown_tx,\n",
    "  AVG(CASE WHEN city_name='Baytown_TX' THEN windspeed     END) AS hist_wind_baytown_tx,\n",
    "  AVG(CASE WHEN city_name='Baytown_TX' THEN precipitation END) AS hist_precip_baytown_tx,\n",
    "\n",
    "  /* Houston_TX */\n",
    "  AVG(CASE WHEN city_name='Houston_TX' THEN temperature   END) AS hist_temp_houston_tx,\n",
    "  AVG(CASE WHEN city_name='Houston_TX' THEN humidity      END) AS hist_hum_houston_tx,\n",
    "  AVG(CASE WHEN city_name='Houston_TX' THEN windspeed     END) AS hist_wind_houston_tx,\n",
    "  AVG(CASE WHEN city_name='Houston_TX' THEN precipitation END) AS hist_precip_houston_tx\n",
    "\n",
    "FROM latest\n",
    "GROUP BY OperatingDTM, \"timestamp\", \"interval\"\n",
    "ORDER BY OperatingDTM, \"timestamp\", \"interval\";\n",
    "\"\"\")\n",
    "\n",
    "print(\"View vw_historical_weather_by_city updated with OperatingDTM (hist_* columns).\")\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f4ee436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OperatingDTM           timestamp  interval  hist_temp_the_woodlands_tx  \\\n",
      "0    2025-08-16 2025-08-16 20:00:00        20                        30.0   \n",
      "1    2025-08-16 2025-08-16 19:00:00        19                        31.1   \n",
      "2    2025-08-16 2025-08-16 18:00:00        18                        31.7   \n",
      "3    2025-08-16 2025-08-16 17:00:00        17                        29.4   \n",
      "4    2025-08-16 2025-08-16 16:00:00        16                        27.8   \n",
      "5    2025-08-16 2025-08-16 15:00:00        15                        26.1   \n",
      "6    2025-08-16 2025-08-16 14:00:00        14                        26.1   \n",
      "7    2025-08-16 2025-08-16 13:00:00        13                        26.7   \n",
      "8    2025-08-16 2025-08-16 12:00:00        12                        28.0   \n",
      "9    2025-08-16 2025-08-16 11:00:00        11                        31.1   \n",
      "10   2025-08-16 2025-08-16 10:00:00        10                        31.1   \n",
      "11   2025-08-16 2025-08-16 09:00:00         9                        27.2   \n",
      "12   2025-08-16 2025-08-16 08:00:00         8                        26.0   \n",
      "13   2025-08-16 2025-08-16 07:00:00         7                        24.0   \n",
      "14   2025-08-16 2025-08-16 06:00:00         6                        24.0   \n",
      "15   2025-08-16 2025-08-16 05:00:00         5                        23.0   \n",
      "16   2025-08-16 2025-08-16 04:00:00         4                        24.0   \n",
      "17   2025-08-16 2025-08-16 03:00:00         3                        24.0   \n",
      "18   2025-08-16 2025-08-16 02:00:00         2                        25.0   \n",
      "19   2025-08-16 2025-08-16 01:00:00         1                        25.6   \n",
      "20   2025-08-15 2025-08-16 00:00:00        24                        26.7   \n",
      "21   2025-08-15 2025-08-15 23:00:00        23                        27.2   \n",
      "22   2025-08-15 2025-08-15 22:00:00        22                        28.3   \n",
      "23   2025-08-15 2025-08-15 21:00:00        21                        29.4   \n",
      "24   2025-08-15 2025-08-15 20:00:00        20                        31.1   \n",
      "\n",
      "    hist_hum_the_woodlands_tx  hist_wind_the_woodlands_tx  \\\n",
      "0                        72.0                    1.388889   \n",
      "1                        66.0                    1.944444   \n",
      "2                        68.0                    1.666667   \n",
      "3                        77.0                    1.666667   \n",
      "4                        82.0                    0.833333   \n",
      "5                        94.0                    1.111111   \n",
      "6                        94.0                    0.833333   \n",
      "7                        90.0                    0.833333   \n",
      "8                        89.0                    1.111111   \n",
      "9                        70.0                    0.000000   \n",
      "10                       75.0                    1.111111   \n",
      "11                       86.0                    1.694444   \n",
      "12                       90.0                    1.666667   \n",
      "13                       97.0                    1.666667   \n",
      "14                      100.0                    0.000000   \n",
      "15                      100.0                    0.000000   \n",
      "16                      100.0                    0.000000   \n",
      "17                       96.0                    0.000000   \n",
      "18                       94.0                    0.000000   \n",
      "19                       90.0                    0.000000   \n",
      "20                       85.0                    2.111111   \n",
      "21                       82.0                    1.500000   \n",
      "22                       74.0                    3.111111   \n",
      "23                       74.0                    3.611111   \n",
      "24                       73.0                    4.611111   \n",
      "\n",
      "    hist_precip_the_woodlands_tx  hist_temp_katy_tx  hist_hum_katy_tx  \\\n",
      "0                            0.0               32.0              71.0   \n",
      "1                            0.2               32.0              71.0   \n",
      "2                            0.3               32.0              71.0   \n",
      "3                            0.7               32.0              67.0   \n",
      "4                            0.0               29.0              84.0   \n",
      "5                            0.0               30.0              75.0   \n",
      "6                            0.0               32.0              67.0   \n",
      "7                            0.0               33.0              63.0   \n",
      "8                            0.0               34.0              63.0   \n",
      "9                            0.2               31.0              79.0   \n",
      "10                           0.0               30.0              84.0   \n",
      "11                           0.0               28.0              94.0   \n",
      "12                           0.0               26.0             100.0   \n",
      "13                           0.0               25.0             100.0   \n",
      "14                           0.4               25.0             100.0   \n",
      "15                           0.0               26.0             100.0   \n",
      "16                           0.0               26.0             100.0   \n",
      "17                           0.0               27.0              94.0   \n",
      "18                           0.0               27.0              94.0   \n",
      "19                           0.0               27.0              89.0   \n",
      "20                           0.0               28.0              84.0   \n",
      "21                           0.0               29.0              79.0   \n",
      "22                           0.0               30.0              75.0   \n",
      "23                           NaN               31.0              70.0   \n",
      "24                           NaN               32.0              67.0   \n",
      "\n",
      "    hist_wind_katy_tx  ...  hist_wind_friendswood_tx  \\\n",
      "0            2.777778  ...                  2.694444   \n",
      "1            2.222222  ...                  3.694444   \n",
      "2            2.500000  ...                  4.000000   \n",
      "3            1.944444  ...                  4.111111   \n",
      "4            1.388889  ...                  3.611111   \n",
      "5            2.222222  ...                  3.388889   \n",
      "6            0.000000  ...                  2.111111   \n",
      "7            1.944444  ...                  3.305556   \n",
      "8            1.944444  ...                  4.388889   \n",
      "9            3.611111  ...                  3.500000   \n",
      "10           2.500000  ...                  2.611111   \n",
      "11           1.666667  ...                  2.500000   \n",
      "12           1.944444  ...                  2.111111   \n",
      "13           1.944444  ...                  1.500000   \n",
      "14           1.666667  ...                  2.500000   \n",
      "15           1.666667  ...                  2.888889   \n",
      "16           1.500000  ...                  3.388889   \n",
      "17           2.111111  ...                  3.805556   \n",
      "18           2.611111  ...                  3.305556   \n",
      "19           2.111111  ...                  3.388889   \n",
      "20           3.111111  ...                  4.000000   \n",
      "21           3.111111  ...                  4.111111   \n",
      "22           5.694444  ...                  4.000000   \n",
      "23           5.111111  ...                  4.111111   \n",
      "24           5.111111  ...                  4.305556   \n",
      "\n",
      "    hist_precip_friendswood_tx  hist_temp_baytown_tx  hist_hum_baytown_tx  \\\n",
      "0                          0.0                  30.8                 73.0   \n",
      "1                          0.0                  31.1                 74.0   \n",
      "2                          2.6                  31.4                 71.0   \n",
      "3                          0.2                  29.7                 78.0   \n",
      "4                          0.2                  28.4                 86.0   \n",
      "5                          0.0                  27.7                 82.0   \n",
      "6                          0.0                  28.9                 81.0   \n",
      "7                          1.3                  26.6                 88.0   \n",
      "8                          0.5                  31.5                 68.0   \n",
      "9                          0.0                  31.0                 67.0   \n",
      "10                         0.0                  32.0                 71.0   \n",
      "11                         0.0                  29.0                 81.0   \n",
      "12                         0.0                  26.0                 93.0   \n",
      "13                         0.0                  26.0                 92.0   \n",
      "14                         0.0                  27.0                 87.0   \n",
      "15                         2.3                  28.0                 81.0   \n",
      "16                         0.0                  28.0                 81.0   \n",
      "17                         0.0                  29.0                 77.0   \n",
      "18                         0.0                  29.0                 76.0   \n",
      "19                         0.0                  29.0                 77.0   \n",
      "20                         0.0                  29.0                 76.0   \n",
      "21                         0.0                  30.0                 72.0   \n",
      "22                         0.0                  30.0                 72.0   \n",
      "23                         0.0                  30.0                 70.0   \n",
      "24                         0.0                  31.0                 65.0   \n",
      "\n",
      "    hist_wind_baytown_tx  hist_precip_baytown_tx  hist_temp_houston_tx  \\\n",
      "0               2.222222                     0.0                  30.7   \n",
      "1               1.666667                     0.0                  30.8   \n",
      "2               0.833333                     0.5                  31.8   \n",
      "3               0.000000                     0.3                  33.4   \n",
      "4               0.000000                     0.0                  34.2   \n",
      "5               0.000000                     0.5                  33.4   \n",
      "6               0.000000                     0.2                  32.9   \n",
      "7               1.111111                     0.3                  31.8   \n",
      "8               2.222222                     0.1                  30.7   \n",
      "9               3.611111                     0.0                  29.6   \n",
      "10              4.166667                     0.0                  28.5   \n",
      "11              0.000000                     0.0                  27.1   \n",
      "12              0.000000                     0.0                  26.3   \n",
      "13              0.000000                     0.0                  26.5   \n",
      "14              0.000000                     0.0                  26.6   \n",
      "15              1.666667                     0.0                  26.7   \n",
      "16              2.500000                     0.0                  26.8   \n",
      "17              3.055556                     0.0                  28.0   \n",
      "18              3.611111                     0.0                  28.0   \n",
      "19              3.611111                     0.0                  28.0   \n",
      "20              2.500000                     0.0                  28.0   \n",
      "21              3.611111                     0.0                  29.0   \n",
      "22              5.277778                     0.0                  29.0   \n",
      "23              5.277778                     0.0                  30.0   \n",
      "24              4.722222                     0.0                  31.0   \n",
      "\n",
      "    hist_hum_houston_tx  hist_wind_houston_tx  hist_precip_houston_tx  \n",
      "0                  69.0              3.611111                     0.0  \n",
      "1                  68.0              3.805556                     0.3  \n",
      "2                  62.0              4.500000                     0.1  \n",
      "3                  56.0              3.111111                     0.4  \n",
      "4                  55.0              2.611111                     0.0  \n",
      "5                  59.0              2.194444                     0.0  \n",
      "6                  62.0              2.388889                     0.0  \n",
      "7                  66.0              3.194444                     0.1  \n",
      "8                  70.0              3.194444                     0.2  \n",
      "9                  74.0              3.000000                     0.0  \n",
      "10                 80.0              2.694444                     0.0  \n",
      "11                 87.0              1.694444                     0.0  \n",
      "12                 91.0              1.305556                     0.0  \n",
      "13                 91.0              0.805556                     0.0  \n",
      "14                 90.0              1.000000                     0.0  \n",
      "15                 90.0              1.111111                     0.0  \n",
      "16                 89.0              1.388889                     0.0  \n",
      "17                 84.0              1.944444                     0.0  \n",
      "18                 84.0              1.944444                     0.0  \n",
      "19                 84.0              2.500000                     0.0  \n",
      "20                 84.0              1.666667                     0.0  \n",
      "21                 74.0              3.055556                     0.0  \n",
      "22                 74.0              3.333333                     0.0  \n",
      "23                 70.0              3.333333                     0.0  \n",
      "24                 66.0              3.888889                     0.0  \n",
      "\n",
      "[25 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 select * from vw_historical_weather_by_city  order by 1 desc,2 desc Limit 25\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b7b304",
   "metadata": {},
   "source": [
    "Def a problem with the querying of the historic weather data. Runnign it through the ui is now throwing an error.\n",
    " SOlving that in a different workflow while pressing on with the join of this view to the master spine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d1b70ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OperatingDTM  Interval   Month DSTFlag  wz_southcentral    wz_east  \\\n",
      "0    2025-08-15         1  8-2025       N       10968.5996  2131.9399   \n",
      "1    2025-08-15         2  8-2025       N       10136.5000  1983.5100   \n",
      "2    2025-08-15         3  8-2025       N        9746.1602  1866.6100   \n",
      "3    2025-08-15         4  8-2025       N        9462.0498  1844.2700   \n",
      "4    2025-08-15         5  8-2025       N        9396.8799  1800.1600   \n",
      "5    2025-08-15         6  8-2025       N        9425.6602  1830.5500   \n",
      "6    2025-08-15         7  8-2025       N        9703.2002  1915.6700   \n",
      "7    2025-08-15         8  8-2025       N        9848.6104  1929.8101   \n",
      "8    2025-08-15         9  8-2025       N       10091.0000  2020.5000   \n",
      "9    2025-08-15        10  8-2025       N       11048.2998  2216.1599   \n",
      "10   2025-08-15        11  8-2025       N       11824.2002  2570.5300   \n",
      "11   2025-08-15        12  8-2025       N       12548.4004  2846.6499   \n",
      "12   2025-08-15        13  8-2025       N       13163.0996  2731.6899   \n",
      "13   2025-08-15        14  8-2025       N       13720.2998  2627.9399   \n",
      "14   2025-08-15        15  8-2025       N       14183.5000  2695.0901   \n",
      "15   2025-08-15        16  8-2025       N       13999.0000  2680.3601   \n",
      "16   2025-08-15        17  8-2025       N       13679.5000  2649.6299   \n",
      "17   2025-08-15        18  8-2025       N       13462.9004  2637.3701   \n",
      "18   2025-08-15        19  8-2025       N       13560.0996  2700.2100   \n",
      "19   2025-08-15        20  8-2025       N       13455.4004  2708.6899   \n",
      "20   2025-08-15        21  8-2025       N       12976.5996  2596.5100   \n",
      "21   2025-08-15        22  8-2025       N       12517.2002  2507.6799   \n",
      "22   2025-08-15        23  8-2025       N       11821.9004  2311.3401   \n",
      "23   2025-08-15        24  8-2025       N       11113.4004  2245.7300   \n",
      "\n",
      "      wz_west  wz_northcentral  wz_farwest   wz_north  ...  \\\n",
      "0   1768.1200       18768.4004   7770.4502  2419.8899  ...   \n",
      "1   1713.5100       17474.4004   7753.1401  2261.4500  ...   \n",
      "2   1666.6801       16863.9004   7695.2300  2192.5601  ...   \n",
      "3   1612.3800       16361.0000   7681.3901  2114.9900  ...   \n",
      "4   1584.2700       16212.7998   7590.6099  2125.5701  ...   \n",
      "5   1547.2700       16119.2002   7532.4399  2111.8701  ...   \n",
      "6   1596.2800       16397.3008   7584.8101  1811.0300  ...   \n",
      "7   1595.1300       16691.5996   7769.5098  1378.8600  ...   \n",
      "8   1676.9600       18035.4004   8237.0400  1674.5800  ...   \n",
      "9   1845.3700       20104.5000   8614.5898  1783.1700  ...   \n",
      "10  2211.8301       22226.0996   8768.9199  1969.6400  ...   \n",
      "11  2210.2900       22814.6992   8665.3799  1979.4301  ...   \n",
      "12  2223.9900       23620.3008   8369.4199  2109.6799  ...   \n",
      "13  2127.2800       24273.4004   8335.3096  2188.0500  ...   \n",
      "14  2177.9900       25398.5996   8022.7202  2264.7700  ...   \n",
      "15  2212.4500       26160.6992   7879.3599  2355.7500  ...   \n",
      "16  2304.6599       26608.9004   7852.9502  2425.2200  ...   \n",
      "17  2341.3201       26725.6992   8146.8301  2459.4900  ...   \n",
      "18  2270.4600       26199.1992   8139.9302  2383.2000  ...   \n",
      "19  2163.4500       25186.6992   8039.9399  2278.8301  ...   \n",
      "20  2042.4100       23850.5996   7514.6001  2139.3999  ...   \n",
      "21  1950.1300       22528.4004   7795.3101  2023.2200  ...   \n",
      "22  1903.6300       21380.5000   7479.2700  1954.6700  ...   \n",
      "23  1895.4800       20009.8008   7495.8101  1947.1200  ...   \n",
      "\n",
      "    hist_wind_friendswood_tx  hist_precip_friendswood_tx  \\\n",
      "0                   1.500000                         0.0   \n",
      "1                   2.111111                         0.0   \n",
      "2                   2.305556                         0.0   \n",
      "3                   2.000000                         0.0   \n",
      "4                   2.000000                         0.0   \n",
      "5                   1.694444                         0.0   \n",
      "6                   1.305556                         0.0   \n",
      "7                   2.611111                         0.0   \n",
      "8                   2.194444                         0.0   \n",
      "9                   4.111111                         0.0   \n",
      "10                  3.194444                         0.0   \n",
      "11                  3.888889                         0.2   \n",
      "12                  4.611111                         0.6   \n",
      "13                  3.305556                         0.6   \n",
      "14                  5.611111                         0.5   \n",
      "15                  6.500000                         1.6   \n",
      "16                  5.500000                         0.0   \n",
      "17                  5.388889                         0.0   \n",
      "18                  5.611111                         0.0   \n",
      "19                  4.305556                         0.0   \n",
      "20                  4.111111                         0.0   \n",
      "21                  4.000000                         0.0   \n",
      "22                  4.111111                         0.0   \n",
      "23                  4.000000                         0.0   \n",
      "\n",
      "    hist_temp_baytown_tx  hist_hum_baytown_tx  hist_wind_baytown_tx  \\\n",
      "0                   29.0                 79.0              3.055556   \n",
      "1                   29.0                 80.0              1.944444   \n",
      "2                   29.0                 81.0              3.055556   \n",
      "3                   28.0                 82.0              3.611111   \n",
      "4                   28.0                 83.0              1.944444   \n",
      "5                   28.0                 83.0              2.500000   \n",
      "6                   28.0                 83.0              1.944444   \n",
      "7                   26.0                 93.0              0.000000   \n",
      "8                   28.0                 87.0              0.000000   \n",
      "9                   32.0                 69.0              3.611111   \n",
      "10                  33.0                 65.0              3.611111   \n",
      "11                  33.0                 65.0              5.277778   \n",
      "12                  32.0                 68.0              5.277778   \n",
      "13                  33.0                 62.0              6.666667   \n",
      "14                  33.0                 63.0              6.111111   \n",
      "15                  34.0                 57.0              5.555556   \n",
      "16                  32.0                 66.0              5.277778   \n",
      "17                  32.0                 69.0              6.111111   \n",
      "18                  32.0                 59.0              7.222222   \n",
      "19                  31.0                 65.0              4.722222   \n",
      "20                  30.0                 70.0              5.277778   \n",
      "21                  30.0                 72.0              5.277778   \n",
      "22                  30.0                 72.0              3.611111   \n",
      "23                  29.0                 76.0              2.500000   \n",
      "\n",
      "    hist_precip_baytown_tx  hist_temp_houston_tx  hist_hum_houston_tx  \\\n",
      "0                      0.0                  28.0                 84.0   \n",
      "1                      0.0                  28.0                 84.0   \n",
      "2                      0.0                  28.0                 84.0   \n",
      "3                      0.0                  28.0                 84.0   \n",
      "4                      0.0                  28.0                 84.0   \n",
      "5                      0.0                  28.0                 89.0   \n",
      "6                      0.0                  28.0                 84.0   \n",
      "7                      0.0                  28.0                 84.0   \n",
      "8                      0.0                  28.0                 89.0   \n",
      "9                      0.0                  30.0                 75.0   \n",
      "10                     0.9                  32.0                 67.0   \n",
      "11                     2.5                  30.0                 75.0   \n",
      "12                     0.0                  32.0                 63.0   \n",
      "13                     0.1                  33.0                 56.0   \n",
      "14                     0.1                  32.2                 63.0   \n",
      "15                     0.0                  33.0                 56.0   \n",
      "16                     0.4                  33.0                 56.0   \n",
      "17                     0.0                  32.0                 59.0   \n",
      "18                     0.0                  32.0                 59.0   \n",
      "19                     0.0                  31.0                 66.0   \n",
      "20                     0.0                  30.0                 70.0   \n",
      "21                     0.0                  29.0                 74.0   \n",
      "22                     0.0                  29.0                 74.0   \n",
      "23                     0.0                  28.0                 84.0   \n",
      "\n",
      "    hist_wind_houston_tx  hist_precip_houston_tx  \n",
      "0               1.944444                     0.0  \n",
      "1               4.166667                     0.0  \n",
      "2               2.500000                     0.0  \n",
      "3               2.222222                     0.0  \n",
      "4               0.000000                     0.0  \n",
      "5               1.944444                     0.0  \n",
      "6               0.833333                     0.0  \n",
      "7               0.000000                     0.0  \n",
      "8               1.388889                     0.3  \n",
      "9               2.500000                     0.0  \n",
      "10              1.944444                     0.0  \n",
      "11              3.611111                     0.0  \n",
      "12              1.666667                     0.9  \n",
      "13              3.611111                     0.7  \n",
      "14              4.388889                     0.0  \n",
      "15              1.944444                     3.9  \n",
      "16              4.166667                     0.2  \n",
      "17              3.611111                     0.0  \n",
      "18              4.166667                     0.0  \n",
      "19              3.888889                     0.0  \n",
      "20              3.333333                     0.0  \n",
      "21              3.333333                     0.0  \n",
      "22              3.055556                     0.0  \n",
      "23              1.666667                     0.0  \n",
      "\n",
      "[24 rows x 66 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "SELECT\n",
    "    lf.*,\n",
    "\n",
    "    -- calendar features from the date\n",
    "    EXTRACT(year  FROM lf.OperatingDTM)::INT  AS cal_year,\n",
    "    EXTRACT(month FROM lf.OperatingDTM)::INT  AS cal_month,\n",
    "    EXTRACT(dow   FROM lf.OperatingDTM)::INT  AS cal_dow,    -- 0=Sunday .. 6=Saturday\n",
    "    CASE WHEN EXTRACT(dow FROM lf.OperatingDTM) IN (0,6) THEN 1 ELSE 0 END AS cal_is_weekend,\n",
    "\n",
    "    -- hour features from Interval (since OperatingDTM is a date)\n",
    "    lf.Interval AS cal_hour,\n",
    "    ((EXTRACT(dow FROM lf.OperatingDTM)::INT) * 24 + (lf.Interval - 1)) AS cal_hour_of_week,\n",
    "\n",
    "    -- optional cyclic encodings\n",
    "    SIN(2*PI() * lf.Interval/24.0)                        AS cal_sin_hour,\n",
    "    COS(2*PI() * lf.Interval/24.0)                        AS cal_cos_hour,\n",
    "    SIN(2*PI() * EXTRACT(dow FROM lf.OperatingDTM)/7.0)   AS cal_sin_dow,\n",
    "    COS(2*PI() * EXTRACT(dow FROM lf.OperatingDTM)/7.0)   AS cal_cos_dow,\n",
    "\n",
    "    -- forecast weather (future-known), exclude join keys\n",
    "    wfc.* EXCLUDE (OperatingDTM, \"timestamp\", \"interval\"),\n",
    "\n",
    "    -- historical weather (past-only), exclude join keys\n",
    "    whc.* EXCLUDE (OperatingDTM, \"timestamp\", \"interval\")\n",
    "\n",
    "FROM vw_loadforecast_by_zone lf\n",
    "LEFT JOIN vw_forecast_weather_by_city wfc\n",
    "  ON wfc.OperatingDTM = lf.OperatingDTM\n",
    " AND wfc.\"interval\"   = lf.Interval\n",
    "LEFT JOIN vw_historical_weather_by_city whc\n",
    "  ON whc.OperatingDTM = lf.OperatingDTM\n",
    " AND whc.\"interval\"   = lf.Interval\n",
    "\n",
    "WHERE lf.OperatingDTM = DATE '2025-08-15'\n",
    "ORDER BY lf.Interval;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26818a3",
   "metadata": {},
   "source": [
    "checking prices one last time before joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef9f22bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OperatingDTM  Interval  hb_hubavg  hb_west  hb_north  hb_south  hb_houston  \\\n",
      "0    2025-08-18        24      46.10    49.61     44.36     45.52       44.92   \n",
      "1    2025-08-18        23      75.00    78.87     74.11     73.12       73.90   \n",
      "2    2025-08-18        22     136.14   141.17    136.00    132.82      134.57   \n",
      "3    2025-08-18        21     218.71   227.49    221.14    210.66      215.58   \n",
      "4    2025-08-18        20     228.82   241.16    234.59    215.28      224.23   \n",
      "5    2025-08-18        19     116.74   121.95    115.00    116.83      113.16   \n",
      "6    2025-08-18        18      83.97    83.56     74.97     96.08       81.29   \n",
      "7    2025-08-18        17      67.79    63.56     55.56     85.93       66.11   \n",
      "8    2025-08-18        16      54.50    48.02     41.33     74.15       54.52   \n",
      "9    2025-08-18        15      50.26    43.28     39.33     67.55       50.88   \n",
      "10   2025-08-18        14      42.72    36.68     33.58     57.31       43.30   \n",
      "11   2025-08-18        13      34.68    29.82     28.90     44.95       35.06   \n",
      "12   2025-08-18        12      28.54    25.12     25.43     34.52       29.08   \n",
      "13   2025-08-18        11      22.61    20.66     21.60     25.22       22.96   \n",
      "14   2025-08-18        10      20.71    20.18     20.29     21.59       20.77   \n",
      "15   2025-08-18         9      23.44    23.61     23.14     23.65       23.34   \n",
      "16   2025-08-18         8      32.59    34.15     31.73     32.43       32.06   \n",
      "17   2025-08-18         7      34.77    37.23     33.45     34.48       33.92   \n",
      "18   2025-08-18         6      29.44    31.72     28.54     28.85       28.66   \n",
      "19   2025-08-18         5      27.78    30.65     26.70     26.97       26.80   \n",
      "20   2025-08-18         4      26.86    30.30     25.53     25.91       25.68   \n",
      "21   2025-08-18         3      28.46    32.31     26.89     27.48       27.16   \n",
      "22   2025-08-18         2      29.90    34.29     27.98     28.91       28.42   \n",
      "23   2025-08-18         1      32.21    37.29     29.89     31.13       30.52   \n",
      "24   2025-08-17        24      36.57    41.38     34.39     35.48       35.03   \n",
      "\n",
      "    hb_pan  \n",
      "0    41.46  \n",
      "1    69.11  \n",
      "2   128.70  \n",
      "3   213.78  \n",
      "4   232.15  \n",
      "5   114.19  \n",
      "6    77.69  \n",
      "7    59.82  \n",
      "8    45.25  \n",
      "9    41.64  \n",
      "10   35.37  \n",
      "11   28.53  \n",
      "12   24.08  \n",
      "13   19.94  \n",
      "14   19.90  \n",
      "15   23.02  \n",
      "16   31.30  \n",
      "17   32.96  \n",
      "18   28.14  \n",
      "19   26.44  \n",
      "20   25.08  \n",
      "21   25.91  \n",
      "22   26.24  \n",
      "23   26.73  \n",
      "24   30.30  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to your DuckDB file\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# Run query and fetch results\n",
    "df = con.execute(\"\"\"\n",
    "                 select * from vw_prices_by_hub   order by 1 desc,2 desc Limit 25\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc6c43",
   "metadata": {},
   "source": [
    "great now lets join the prices on to the master spine view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5b0051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OperatingDTM  Interval   Month DSTFlag  wz_southcentral    wz_east  \\\n",
      "0    2025-08-16         1  8-2025       N       10012.4004  2085.3000   \n",
      "1    2025-08-16         2  8-2025       N        9431.2402  1985.4600   \n",
      "2    2025-08-16         3  8-2025       N        8878.2305  1870.9000   \n",
      "3    2025-08-16         4  8-2025       N        8570.9004  1847.8400   \n",
      "4    2025-08-16         5  8-2025       N        8444.1797  1770.3000   \n",
      "5    2025-08-16         6  8-2025       N        8272.6104  1753.5100   \n",
      "6    2025-08-16         7  8-2025       N        8256.2197  1747.1600   \n",
      "7    2025-08-16         8  8-2025       N        8603.3496  1798.7300   \n",
      "8    2025-08-16         9  8-2025       N        8905.4502  1916.1801   \n",
      "9    2025-08-16        10  8-2025       N        9686.9004  2184.7300   \n",
      "10   2025-08-16        11  8-2025       N       10357.2002  2301.9700   \n",
      "11   2025-08-16        12  8-2025       N       10838.9004  2404.2600   \n",
      "12   2025-08-16        13  8-2025       N       11347.0000  2489.0701   \n",
      "13   2025-08-16        14  8-2025       N       11917.9004  2607.2800   \n",
      "14   2025-08-16        15  8-2025       N       12151.0996  2643.5901   \n",
      "15   2025-08-16        16  8-2025       N       12502.7002  2708.0400   \n",
      "16   2025-08-16        17  8-2025       N       12772.0996  2711.1599   \n",
      "17   2025-08-16        18  8-2025       N       13284.5996  2733.9900   \n",
      "18   2025-08-16        19  8-2025       N       13258.0996  2711.3701   \n",
      "19   2025-08-16        20  8-2025       N       13323.5996  2565.2900   \n",
      "20   2025-08-16        21  8-2025       N       12713.2998  2499.5801   \n",
      "21   2025-08-16        22  8-2025       N       12217.4004  2401.5000   \n",
      "22   2025-08-16        23  8-2025       N       11679.7002  2307.2800   \n",
      "23   2025-08-16        24  8-2025       N       11048.0000  2169.8401   \n",
      "\n",
      "      wz_west  wz_northcentral  wz_farwest   wz_north  ...  \\\n",
      "0   1744.8600       18433.8008   7703.1899  1858.9800  ...   \n",
      "1   1685.4800       17524.4004   7684.8999  1731.2200  ...   \n",
      "2   1607.1700       16570.3008   7669.0098  1692.9500  ...   \n",
      "3   1578.4900       15904.2998   7639.5498  1738.6200  ...   \n",
      "4   1533.3900       15456.7998   7722.5298  1689.7100  ...   \n",
      "5   1482.6500       15204.5000   7576.2700  1819.6500  ...   \n",
      "6   1458.7100       15096.2998   7631.2202  1863.7700  ...   \n",
      "7   1464.8900       15460.4004   7693.5898  1866.9301  ...   \n",
      "8   1576.8400       16575.8008   8275.6699  2081.2600  ...   \n",
      "9   1812.4399       18335.1992   8638.9199  2074.4700  ...   \n",
      "10  1874.3800       19340.8008   8720.7100  2140.6399  ...   \n",
      "11  1829.2900       20420.1992   8479.9902  2226.9800  ...   \n",
      "12  1860.7700       21777.6992   8543.0098  2479.1499  ...   \n",
      "13  1904.5500       23098.6992   8408.0596  2541.4500  ...   \n",
      "14  1986.3101       23696.0000   8073.7402  2617.9099  ...   \n",
      "15  2051.9399       24182.8008   8107.8101  2647.1399  ...   \n",
      "16  2097.7600       24520.5996   8174.7700  2764.0500  ...   \n",
      "17  2139.5500       24615.6992   8151.1001  2800.7600  ...   \n",
      "18  2118.5801       24065.9004   7811.9302  2847.6699  ...   \n",
      "19  2026.5300       22887.4004   7450.0400  2696.2200  ...   \n",
      "20  1973.0100       21969.0000   7446.2598  2568.7500  ...   \n",
      "21  1929.2000       21256.4004   7457.3301  2498.3899  ...   \n",
      "22  1885.5500       20330.9004   7490.1001  2456.3101  ...   \n",
      "23  1810.7200       19086.9004   7588.8398  2363.5701  ...   \n",
      "\n",
      "    hist_temp_houston_tx  hist_hum_houston_tx  hist_wind_houston_tx  \\\n",
      "0                   28.0                 84.0              2.500000   \n",
      "1                   28.0                 84.0              1.944444   \n",
      "2                   28.0                 84.0              1.944444   \n",
      "3                   26.8                 89.0              1.388889   \n",
      "4                   26.7                 90.0              1.111111   \n",
      "5                   26.6                 90.0              1.000000   \n",
      "6                   26.5                 91.0              0.805556   \n",
      "7                   26.3                 91.0              1.305556   \n",
      "8                   27.1                 87.0              1.694444   \n",
      "9                   28.5                 80.0              2.694444   \n",
      "10                  29.6                 74.0              3.000000   \n",
      "11                  30.7                 70.0              3.194444   \n",
      "12                  31.8                 66.0              3.194444   \n",
      "13                  32.9                 62.0              2.388889   \n",
      "14                  33.4                 59.0              2.194444   \n",
      "15                  34.2                 55.0              2.611111   \n",
      "16                  33.4                 56.0              3.111111   \n",
      "17                  31.8                 62.0              4.500000   \n",
      "18                  30.8                 68.0              3.805556   \n",
      "19                  30.7                 69.0              3.611111   \n",
      "20                   NaN                  NaN                   NaN   \n",
      "21                   NaN                  NaN                   NaN   \n",
      "22                   NaN                  NaN                   NaN   \n",
      "23                   NaN                  NaN                   NaN   \n",
      "\n",
      "    hist_precip_houston_tx  hb_hubavg  hb_west  hb_north  hb_south  \\\n",
      "0                      0.0      28.18    33.99     24.92     27.52   \n",
      "1                      0.0      25.63    30.94     22.70     25.04   \n",
      "2                      0.0      23.87    28.32     21.48     23.31   \n",
      "3                      0.0      23.51    27.73     21.40     22.85   \n",
      "4                      0.0      23.68    27.47     21.70     23.16   \n",
      "5                      0.0      24.79    28.13     22.95     24.43   \n",
      "6                      0.0      27.12    30.56     25.21     26.76   \n",
      "7                      0.0      24.62    26.44     23.59     24.48   \n",
      "8                      0.0      19.46    19.79     19.24     19.48   \n",
      "9                      0.0      18.41    18.37     18.20     18.73   \n",
      "10                     0.0      20.00    19.69     20.00     20.21   \n",
      "11                     0.2      23.96    23.66     23.84     23.82   \n",
      "12                     0.1      27.38    25.78     25.60     29.59   \n",
      "13                     0.0      29.42    26.89     26.76     32.96   \n",
      "14                     0.0      32.11    28.83     27.96     37.68   \n",
      "15                     0.0      34.14    31.17     28.60     41.72   \n",
      "16                     0.4      37.14    35.98     31.84     44.63   \n",
      "17                     0.1      40.00    39.33     32.89     49.68   \n",
      "18                     0.3      51.48    54.93     49.51     52.04   \n",
      "19                     0.0      69.90    76.10     71.40     64.21   \n",
      "20                     NaN      61.17    66.11     61.35     57.33   \n",
      "21                     NaN      48.71    53.55     47.25     46.69   \n",
      "22                     NaN      38.55    43.46     36.37     37.18   \n",
      "23                     NaN      32.14    36.90     30.02     30.94   \n",
      "\n",
      "    hb_houston  hb_pan  \n",
      "0        26.29   19.54  \n",
      "1        23.86   18.76  \n",
      "2        22.35   18.82  \n",
      "3        22.07   18.95  \n",
      "4        22.39   18.68  \n",
      "5        23.65   19.66  \n",
      "6        25.93   21.64  \n",
      "7        23.99   21.56  \n",
      "8        19.33   18.64  \n",
      "9        18.35   18.00  \n",
      "10       20.11   19.04  \n",
      "11       24.52   22.88  \n",
      "12       28.54   24.95  \n",
      "13       31.06   25.68  \n",
      "14       33.97   27.19  \n",
      "15       35.07   29.10  \n",
      "16       36.11   31.19  \n",
      "17       38.10   33.85  \n",
      "18       49.45   48.52  \n",
      "19       67.90   68.05  \n",
      "20       59.90   56.56  \n",
      "21       47.36   41.35  \n",
      "22       37.18   30.24  \n",
      "23       30.69   25.68  \n",
      "\n",
      "[24 rows x 72 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "# --- Minimal: add only Houston hub price ---\n",
    "df = con.execute(\"\"\"\n",
    "SELECT\n",
    "    lf.*,\n",
    "\n",
    "    -- calendar features from the date\n",
    "    EXTRACT(year  FROM lf.OperatingDTM)::INT  AS cal_year,\n",
    "    EXTRACT(month FROM lf.OperatingDTM)::INT  AS cal_month,\n",
    "    EXTRACT(dow   FROM lf.OperatingDTM)::INT  AS cal_dow,    -- 0=Sun..6=Sat\n",
    "    CASE WHEN EXTRACT(dow FROM lf.OperatingDTM) IN (0,6) THEN 1 ELSE 0 END AS cal_is_weekend,\n",
    "\n",
    "    -- hour features from Interval (since OperatingDTM is a date)\n",
    "    lf.Interval AS cal_hour,\n",
    "    ((EXTRACT(dow FROM lf.OperatingDTM)::INT) * 24 + (lf.Interval - 1)) AS cal_hour_of_week,\n",
    "\n",
    "    -- optional cyclic encodings\n",
    "    SIN(2*PI() * lf.Interval/24.0)                      AS cal_sin_hour,\n",
    "    COS(2*PI() * lf.Interval/24.0)                      AS cal_cos_hour,\n",
    "    SIN(2*PI() * EXTRACT(dow FROM lf.OperatingDTM)/7.0) AS cal_sin_dow,\n",
    "    COS(2*PI() * EXTRACT(dow FROM lf.OperatingDTM)/7.0) AS cal_cos_dow,\n",
    "\n",
    "    -- forecast weather (exclude join keys)\n",
    "    wfc.* EXCLUDE (OperatingDTM, \"timestamp\", \"interval\"),\n",
    "\n",
    "    -- historical weather (exclude join keys)\n",
    "    whc.* EXCLUDE (OperatingDTM, \"timestamp\", \"interval\"),\n",
    "\n",
    "    -- prices (join on OperatingDTM + Interval)\n",
    "    p.* EXCLUDE (OperatingDTM, Interval)\n",
    "\n",
    "FROM vw_loadforecast_by_zone lf\n",
    "LEFT JOIN vw_forecast_weather_by_city    wfc\n",
    "  ON wfc.OperatingDTM = lf.OperatingDTM\n",
    " AND wfc.\"interval\"   = lf.Interval\n",
    "LEFT JOIN vw_historical_weather_by_city  whc\n",
    "  ON whc.OperatingDTM = lf.OperatingDTM\n",
    " AND whc.\"interval\"   = lf.Interval\n",
    "LEFT JOIN vw_prices_by_hub               p\n",
    "  ON p.OperatingDTM   = lf.OperatingDTM\n",
    " AND p.Interval       = lf.Interval\n",
    "\n",
    "WHERE lf.OperatingDTM = DATE '2025-08-16'\n",
    "ORDER BY lf.Interval;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b8b60",
   "metadata": {},
   "source": [
    "Final Query for training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28fc9c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created view vw_master_spine\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW vw_master_spine AS\n",
    "SELECT\n",
    "    lf.*,\n",
    "\n",
    "    -- calendar features from the date\n",
    "    EXTRACT(year  FROM lf.OperatingDTM)::INT  AS cal_year,\n",
    "    EXTRACT(month FROM lf.OperatingDTM)::INT  AS cal_month,\n",
    "    EXTRACT(dow   FROM lf.OperatingDTM)::INT  AS cal_dow,    -- 0=Sun..6=Sat\n",
    "    CASE WHEN EXTRACT(dow FROM lf.OperatingDTM) IN (0,6) THEN 1 ELSE 0 END AS cal_is_weekend,\n",
    "\n",
    "    -- hour features from Interval (since OperatingDTM is a date)\n",
    "    lf.Interval AS cal_hour,\n",
    "    ((EXTRACT(dow FROM lf.OperatingDTM)::INT) * 24 + (lf.Interval - 1)) AS cal_hour_of_week,\n",
    "\n",
    "    -- optional cyclic encodings\n",
    "    SIN(2*PI() * lf.Interval/24.0)                      AS cal_sin_hour,\n",
    "    COS(2*PI() * lf.Interval/24.0)                      AS cal_cos_hour,\n",
    "    SIN(2*PI() * EXTRACT(dow FROM lf.OperatingDTM)/7.0) AS cal_sin_dow,\n",
    "    COS(2*PI() * EXTRACT(dow FROM lf.OperatingDTM)/7.0) AS cal_cos_dow,\n",
    "\n",
    "    -- forecast weather (exclude join keys)\n",
    "    wfc.* EXCLUDE (OperatingDTM, \"timestamp\", \"interval\"),\n",
    "\n",
    "    -- historical weather (exclude join keys)\n",
    "    whc.* EXCLUDE (OperatingDTM, \"timestamp\", \"interval\"),\n",
    "\n",
    "    -- prices (exclude join keys; brings all hub/zone price columns)\n",
    "    p.* EXCLUDE (OperatingDTM, Interval)\n",
    "\n",
    "FROM vw_loadforecast_by_zone lf\n",
    "LEFT JOIN vw_forecast_weather_by_city    wfc\n",
    "  ON wfc.OperatingDTM = lf.OperatingDTM\n",
    " AND wfc.\"interval\"   = lf.Interval\n",
    "LEFT JOIN vw_historical_weather_by_city  whc\n",
    "  ON whc.OperatingDTM = lf.OperatingDTM\n",
    " AND whc.\"interval\"   = lf.Interval\n",
    "LEFT JOIN vw_prices_by_hub               p\n",
    "  ON p.OperatingDTM   = lf.OperatingDTM\n",
    " AND p.Interval       = lf.Interval;\n",
    "\"\"\")\n",
    "\n",
    "print(\"Created view vw_master_spine\")\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1e289",
   "metadata": {},
   "source": [
    "single day query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c80524e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OperatingDTM  Interval   Month DSTFlag  wz_southcentral    wz_east  \\\n",
      "0    2025-08-16         1  8-2025       N       10012.4004  2085.3000   \n",
      "1    2025-08-16         2  8-2025       N        9431.2402  1985.4600   \n",
      "2    2025-08-16         3  8-2025       N        8878.2305  1870.9000   \n",
      "3    2025-08-16         4  8-2025       N        8570.9004  1847.8400   \n",
      "4    2025-08-16         5  8-2025       N        8444.1797  1770.3000   \n",
      "5    2025-08-16         6  8-2025       N        8272.6104  1753.5100   \n",
      "6    2025-08-16         7  8-2025       N        8256.2197  1747.1600   \n",
      "7    2025-08-16         8  8-2025       N        8603.3496  1798.7300   \n",
      "8    2025-08-16         9  8-2025       N        8905.4502  1916.1801   \n",
      "9    2025-08-16        10  8-2025       N        9686.9004  2184.7300   \n",
      "10   2025-08-16        11  8-2025       N       10357.2002  2301.9700   \n",
      "11   2025-08-16        12  8-2025       N       10838.9004  2404.2600   \n",
      "12   2025-08-16        13  8-2025       N       11347.0000  2489.0701   \n",
      "13   2025-08-16        14  8-2025       N       11917.9004  2607.2800   \n",
      "14   2025-08-16        15  8-2025       N       12151.0996  2643.5901   \n",
      "15   2025-08-16        16  8-2025       N       12502.7002  2708.0400   \n",
      "16   2025-08-16        17  8-2025       N       12772.0996  2711.1599   \n",
      "17   2025-08-16        18  8-2025       N       13284.5996  2733.9900   \n",
      "18   2025-08-16        19  8-2025       N       13258.0996  2711.3701   \n",
      "19   2025-08-16        20  8-2025       N       13323.5996  2565.2900   \n",
      "20   2025-08-16        21  8-2025       N       12713.2998  2499.5801   \n",
      "21   2025-08-16        22  8-2025       N       12217.4004  2401.5000   \n",
      "22   2025-08-16        23  8-2025       N       11679.7002  2307.2800   \n",
      "23   2025-08-16        24  8-2025       N       11048.0000  2169.8401   \n",
      "\n",
      "      wz_west  wz_northcentral  wz_farwest   wz_north  ...  \\\n",
      "0   1744.8600       18433.8008   7703.1899  1858.9800  ...   \n",
      "1   1685.4800       17524.4004   7684.8999  1731.2200  ...   \n",
      "2   1607.1700       16570.3008   7669.0098  1692.9500  ...   \n",
      "3   1578.4900       15904.2998   7639.5498  1738.6200  ...   \n",
      "4   1533.3900       15456.7998   7722.5298  1689.7100  ...   \n",
      "5   1482.6500       15204.5000   7576.2700  1819.6500  ...   \n",
      "6   1458.7100       15096.2998   7631.2202  1863.7700  ...   \n",
      "7   1464.8900       15460.4004   7693.5898  1866.9301  ...   \n",
      "8   1576.8400       16575.8008   8275.6699  2081.2600  ...   \n",
      "9   1812.4399       18335.1992   8638.9199  2074.4700  ...   \n",
      "10  1874.3800       19340.8008   8720.7100  2140.6399  ...   \n",
      "11  1829.2900       20420.1992   8479.9902  2226.9800  ...   \n",
      "12  1860.7700       21777.6992   8543.0098  2479.1499  ...   \n",
      "13  1904.5500       23098.6992   8408.0596  2541.4500  ...   \n",
      "14  1986.3101       23696.0000   8073.7402  2617.9099  ...   \n",
      "15  2051.9399       24182.8008   8107.8101  2647.1399  ...   \n",
      "16  2097.7600       24520.5996   8174.7700  2764.0500  ...   \n",
      "17  2139.5500       24615.6992   8151.1001  2800.7600  ...   \n",
      "18  2118.5801       24065.9004   7811.9302  2847.6699  ...   \n",
      "19  2026.5300       22887.4004   7450.0400  2696.2200  ...   \n",
      "20  1973.0100       21969.0000   7446.2598  2568.7500  ...   \n",
      "21  1929.2000       21256.4004   7457.3301  2498.3899  ...   \n",
      "22  1885.5500       20330.9004   7490.1001  2456.3101  ...   \n",
      "23  1810.7200       19086.9004   7588.8398  2363.5701  ...   \n",
      "\n",
      "    hist_temp_houston_tx  hist_hum_houston_tx  hist_wind_houston_tx  \\\n",
      "0                   28.0                 84.0              2.500000   \n",
      "1                   28.0                 84.0              1.944444   \n",
      "2                   28.0                 84.0              1.944444   \n",
      "3                   26.8                 89.0              1.388889   \n",
      "4                   26.7                 90.0              1.111111   \n",
      "5                   26.6                 90.0              1.000000   \n",
      "6                   26.5                 91.0              0.805556   \n",
      "7                   26.3                 91.0              1.305556   \n",
      "8                   27.1                 87.0              1.694444   \n",
      "9                   28.5                 80.0              2.694444   \n",
      "10                  29.6                 74.0              3.000000   \n",
      "11                  30.7                 70.0              3.194444   \n",
      "12                  31.8                 66.0              3.194444   \n",
      "13                  32.9                 62.0              2.388889   \n",
      "14                  33.4                 59.0              2.194444   \n",
      "15                  34.2                 55.0              2.611111   \n",
      "16                  33.4                 56.0              3.111111   \n",
      "17                  31.8                 62.0              4.500000   \n",
      "18                  30.8                 68.0              3.805556   \n",
      "19                  30.7                 69.0              3.611111   \n",
      "20                   NaN                  NaN                   NaN   \n",
      "21                   NaN                  NaN                   NaN   \n",
      "22                   NaN                  NaN                   NaN   \n",
      "23                   NaN                  NaN                   NaN   \n",
      "\n",
      "    hist_precip_houston_tx  hb_hubavg  hb_west  hb_north  hb_south  \\\n",
      "0                      0.0      28.18    33.99     24.92     27.52   \n",
      "1                      0.0      25.63    30.94     22.70     25.04   \n",
      "2                      0.0      23.87    28.32     21.48     23.31   \n",
      "3                      0.0      23.51    27.73     21.40     22.85   \n",
      "4                      0.0      23.68    27.47     21.70     23.16   \n",
      "5                      0.0      24.79    28.13     22.95     24.43   \n",
      "6                      0.0      27.12    30.56     25.21     26.76   \n",
      "7                      0.0      24.62    26.44     23.59     24.48   \n",
      "8                      0.0      19.46    19.79     19.24     19.48   \n",
      "9                      0.0      18.41    18.37     18.20     18.73   \n",
      "10                     0.0      20.00    19.69     20.00     20.21   \n",
      "11                     0.2      23.96    23.66     23.84     23.82   \n",
      "12                     0.1      27.38    25.78     25.60     29.59   \n",
      "13                     0.0      29.42    26.89     26.76     32.96   \n",
      "14                     0.0      32.11    28.83     27.96     37.68   \n",
      "15                     0.0      34.14    31.17     28.60     41.72   \n",
      "16                     0.4      37.14    35.98     31.84     44.63   \n",
      "17                     0.1      40.00    39.33     32.89     49.68   \n",
      "18                     0.3      51.48    54.93     49.51     52.04   \n",
      "19                     0.0      69.90    76.10     71.40     64.21   \n",
      "20                     NaN      61.17    66.11     61.35     57.33   \n",
      "21                     NaN      48.71    53.55     47.25     46.69   \n",
      "22                     NaN      38.55    43.46     36.37     37.18   \n",
      "23                     NaN      32.14    36.90     30.02     30.94   \n",
      "\n",
      "    hb_houston  hb_pan  \n",
      "0        26.29   19.54  \n",
      "1        23.86   18.76  \n",
      "2        22.35   18.82  \n",
      "3        22.07   18.95  \n",
      "4        22.39   18.68  \n",
      "5        23.65   19.66  \n",
      "6        25.93   21.64  \n",
      "7        23.99   21.56  \n",
      "8        19.33   18.64  \n",
      "9        18.35   18.00  \n",
      "10       20.11   19.04  \n",
      "11       24.52   22.88  \n",
      "12       28.54   24.95  \n",
      "13       31.06   25.68  \n",
      "14       33.97   27.19  \n",
      "15       35.07   29.10  \n",
      "16       36.11   31.19  \n",
      "17       38.10   33.85  \n",
      "18       49.45   48.52  \n",
      "19       67.90   68.05  \n",
      "20       59.90   56.56  \n",
      "21       47.36   41.35  \n",
      "22       37.18   30.24  \n",
      "23       30.69   25.68  \n",
      "\n",
      "[24 rows x 72 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "SELECT * FROM vw_master_spine\n",
    "WHERE OperatingDTM = DATE '2025-08-16'\n",
    "ORDER BY Interval;\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2cd3c",
   "metadata": {},
   "source": [
    "last X Op Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54736e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    OperatingDTM  Interval   Month DSTFlag  wz_southcentral    wz_east  \\\n",
      "0     2025-08-04         1  8-2025       N       10777.4004  1983.6000   \n",
      "1     2025-08-04         2  8-2025       N        9904.2803  1771.8500   \n",
      "2     2025-08-04         3  8-2025       N        9372.2900  1682.3199   \n",
      "3     2025-08-04         4  8-2025       N        9024.0596  1620.0400   \n",
      "4     2025-08-04         5  8-2025       N        8870.4102  1622.1500   \n",
      "..           ...       ...     ...     ...              ...        ...   \n",
      "475   2025-08-23        20  8-2025       N       12319.8994  2547.4299   \n",
      "476   2025-08-23        21  8-2025       N       11860.6748  2438.8701   \n",
      "477   2025-08-23        22  8-2025       N       11418.9746  2320.7075   \n",
      "478   2025-08-23        23  8-2025       N       10797.6992  2187.1775   \n",
      "479   2025-08-23        24  8-2025       N       10105.1426  2033.3350   \n",
      "\n",
      "       wz_west  wz_northcentral  wz_farwest   wz_north  ...  \\\n",
      "0    1642.8500       16235.0996   7639.3398  2011.7500  ...   \n",
      "1    1676.9301       15198.2998   7594.1001  2100.3899  ...   \n",
      "2    1569.7000       14655.7002   7868.1201  1915.2100  ...   \n",
      "3    1467.4800       14254.5000   7734.3398  1868.1801  ...   \n",
      "4    1453.2200       14230.5000   7463.6499  1819.7600  ...   \n",
      "..         ...              ...         ...        ...  ...   \n",
      "475  1751.6400       21937.4258   7929.2900  2461.0977  ...   \n",
      "476  1687.8900       20975.9746   7657.3301  2337.6174  ...   \n",
      "477  1637.4351       20113.9238   7670.5352  2298.1899  ...   \n",
      "478  1535.3601       18937.5488   7716.1201  2187.9351  ...   \n",
      "479  1456.1176       17665.5742   7745.1724  2098.2500  ...   \n",
      "\n",
      "     hist_temp_houston_tx  hist_hum_houston_tx  hist_wind_houston_tx  \\\n",
      "0                    28.0                 62.0              0.000000   \n",
      "1                    28.0                 66.0              0.000000   \n",
      "2                    28.0                 74.0              0.000000   \n",
      "3                    28.0                 79.0              1.666667   \n",
      "4                    27.0                 79.0              1.944444   \n",
      "..                    ...                  ...                   ...   \n",
      "475                   NaN                  NaN                   NaN   \n",
      "476                   NaN                  NaN                   NaN   \n",
      "477                   NaN                  NaN                   NaN   \n",
      "478                   NaN                  NaN                   NaN   \n",
      "479                   NaN                  NaN                   NaN   \n",
      "\n",
      "     hist_precip_houston_tx  hb_hubavg  hb_west  hb_north  hb_south  \\\n",
      "0                       0.0      25.57    29.23     24.01     24.70   \n",
      "1                       0.0      25.24    28.70     23.80     24.38   \n",
      "2                       0.0      24.25    27.13     23.18     23.42   \n",
      "3                       0.0      25.13    28.17     24.09     24.18   \n",
      "4                       0.0      27.02    29.71     26.10     26.16   \n",
      "..                      ...        ...      ...       ...       ...   \n",
      "475                     NaN        NaN      NaN       NaN       NaN   \n",
      "476                     NaN        NaN      NaN       NaN       NaN   \n",
      "477                     NaN        NaN      NaN       NaN       NaN   \n",
      "478                     NaN        NaN      NaN       NaN       NaN   \n",
      "479                     NaN        NaN      NaN       NaN       NaN   \n",
      "\n",
      "     hb_houston  hb_pan  \n",
      "0         24.36   22.85  \n",
      "1         24.08   23.02  \n",
      "2         23.27   23.21  \n",
      "3         24.10   24.53  \n",
      "4         26.10   26.60  \n",
      "..          ...     ...  \n",
      "475         NaN     NaN  \n",
      "476         NaN     NaN  \n",
      "477         NaN     NaN  \n",
      "478         NaN     NaN  \n",
      "479         NaN     NaN  \n",
      "\n",
      "[480 rows x 72 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "SELECT ms.*\n",
    "FROM vw_master_spine ms\n",
    "JOIN (\n",
    "  SELECT DISTINCT OperatingDTM\n",
    "  FROM vw_master_spine\n",
    "  ORDER BY OperatingDTM DESC\n",
    "  LIMIT 20\n",
    ") d USING (OperatingDTM)\n",
    "ORDER BY ms.OperatingDTM, ms.Interval;\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "con.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bca957",
   "metadata": {},
   "source": [
    "Now lets make a lagged and rolled price view adding a timestamp back to the master view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5cb37c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    OperatingDTM  Interval   Month DSTFlag  wz_southcentral    wz_east  \\\n",
      "0     2025-08-04         1  8-2025       N       10777.4004  1983.6000   \n",
      "1     2025-08-04         2  8-2025       N        9904.2803  1771.8500   \n",
      "2     2025-08-04         3  8-2025       N        9372.2900  1682.3199   \n",
      "3     2025-08-04         4  8-2025       N        9024.0596  1620.0400   \n",
      "4     2025-08-04         5  8-2025       N        8870.4102  1622.1500   \n",
      "..           ...       ...     ...     ...              ...        ...   \n",
      "475   2025-08-23        20  8-2025       N       12319.8994  2547.4299   \n",
      "476   2025-08-23        21  8-2025       N       11860.6748  2438.8701   \n",
      "477   2025-08-23        22  8-2025       N       11418.9746  2320.7075   \n",
      "478   2025-08-23        23  8-2025       N       10797.6992  2187.1775   \n",
      "479   2025-08-23        24  8-2025       N       10105.1426  2033.3350   \n",
      "\n",
      "       wz_west  wz_northcentral  wz_farwest   wz_north  ...  \\\n",
      "0    1642.8500       16235.0996   7639.3398  2011.7500  ...   \n",
      "1    1676.9301       15198.2998   7594.1001  2100.3899  ...   \n",
      "2    1569.7000       14655.7002   7868.1201  1915.2100  ...   \n",
      "3    1467.4800       14254.5000   7734.3398  1868.1801  ...   \n",
      "4    1453.2200       14230.5000   7463.6499  1819.7600  ...   \n",
      "..         ...              ...         ...        ...  ...   \n",
      "475  1751.6400       21937.4258   7929.2900  2461.0977  ...   \n",
      "476  1687.8900       20975.9746   7657.3301  2337.6174  ...   \n",
      "477  1637.4351       20113.9238   7670.5352  2298.1899  ...   \n",
      "478  1535.3601       18937.5488   7716.1201  2187.9351  ...   \n",
      "479  1456.1176       17665.5742   7745.1724  2098.2500  ...   \n",
      "\n",
      "     hist_temp_houston_tx  hist_hum_houston_tx  hist_wind_houston_tx  \\\n",
      "0                    28.0                 62.0              0.000000   \n",
      "1                    28.0                 66.0              0.000000   \n",
      "2                    28.0                 74.0              0.000000   \n",
      "3                    28.0                 79.0              1.666667   \n",
      "4                    27.0                 79.0              1.944444   \n",
      "..                    ...                  ...                   ...   \n",
      "475                   NaN                  NaN                   NaN   \n",
      "476                   NaN                  NaN                   NaN   \n",
      "477                   NaN                  NaN                   NaN   \n",
      "478                   NaN                  NaN                   NaN   \n",
      "479                   NaN                  NaN                   NaN   \n",
      "\n",
      "     hist_precip_houston_tx  hb_hubavg  hb_west  hb_north  hb_south  \\\n",
      "0                       0.0      25.57    29.23     24.01     24.70   \n",
      "1                       0.0      25.24    28.70     23.80     24.38   \n",
      "2                       0.0      24.25    27.13     23.18     23.42   \n",
      "3                       0.0      25.13    28.17     24.09     24.18   \n",
      "4                       0.0      27.02    29.71     26.10     26.16   \n",
      "..                      ...        ...      ...       ...       ...   \n",
      "475                     NaN        NaN      NaN       NaN       NaN   \n",
      "476                     NaN        NaN      NaN       NaN       NaN   \n",
      "477                     NaN        NaN      NaN       NaN       NaN   \n",
      "478                     NaN        NaN      NaN       NaN       NaN   \n",
      "479                     NaN        NaN      NaN       NaN       NaN   \n",
      "\n",
      "     hb_houston  hb_pan  \n",
      "0         24.36   22.85  \n",
      "1         24.08   23.02  \n",
      "2         23.27   23.21  \n",
      "3         24.10   24.53  \n",
      "4         26.10   26.60  \n",
      "..          ...     ...  \n",
      "475         NaN     NaN  \n",
      "476         NaN     NaN  \n",
      "477         NaN     NaN  \n",
      "478         NaN     NaN  \n",
      "479         NaN     NaN  \n",
      "\n",
      "[480 rows x 72 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW vw_master_spine_ts AS\n",
    "SELECT\n",
    "  ms.*,\n",
    "  CAST(ms.OperatingDTM AS TIMESTAMP) + (ms.Interval - 1) * INTERVAL 1 HOUR AS ts\n",
    "FROM vw_master_spine ms;\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(df)\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b92b49d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    OperatingDTM  Interval   Month DSTFlag  wz_southcentral    wz_east  \\\n",
      "0     2025-08-04         1  8-2025       N       10777.4004  1983.6000   \n",
      "1     2025-08-04         2  8-2025       N        9904.2803  1771.8500   \n",
      "2     2025-08-04         3  8-2025       N        9372.2900  1682.3199   \n",
      "3     2025-08-04         4  8-2025       N        9024.0596  1620.0400   \n",
      "4     2025-08-04         5  8-2025       N        8870.4102  1622.1500   \n",
      "..           ...       ...     ...     ...              ...        ...   \n",
      "475   2025-08-23        20  8-2025       N       12319.8994  2547.4299   \n",
      "476   2025-08-23        21  8-2025       N       11860.6748  2438.8701   \n",
      "477   2025-08-23        22  8-2025       N       11418.9746  2320.7075   \n",
      "478   2025-08-23        23  8-2025       N       10797.6992  2187.1775   \n",
      "479   2025-08-23        24  8-2025       N       10105.1426  2033.3350   \n",
      "\n",
      "       wz_west  wz_northcentral  wz_farwest   wz_north  ...  \\\n",
      "0    1642.8500       16235.0996   7639.3398  2011.7500  ...   \n",
      "1    1676.9301       15198.2998   7594.1001  2100.3899  ...   \n",
      "2    1569.7000       14655.7002   7868.1201  1915.2100  ...   \n",
      "3    1467.4800       14254.5000   7734.3398  1868.1801  ...   \n",
      "4    1453.2200       14230.5000   7463.6499  1819.7600  ...   \n",
      "..         ...              ...         ...        ...  ...   \n",
      "475  1751.6400       21937.4258   7929.2900  2461.0977  ...   \n",
      "476  1687.8900       20975.9746   7657.3301  2337.6174  ...   \n",
      "477  1637.4351       20113.9238   7670.5352  2298.1899  ...   \n",
      "478  1535.3601       18937.5488   7716.1201  2187.9351  ...   \n",
      "479  1456.1176       17665.5742   7745.1724  2098.2500  ...   \n",
      "\n",
      "     hist_temp_houston_tx  hist_hum_houston_tx  hist_wind_houston_tx  \\\n",
      "0                    28.0                 62.0              0.000000   \n",
      "1                    28.0                 66.0              0.000000   \n",
      "2                    28.0                 74.0              0.000000   \n",
      "3                    28.0                 79.0              1.666667   \n",
      "4                    27.0                 79.0              1.944444   \n",
      "..                    ...                  ...                   ...   \n",
      "475                   NaN                  NaN                   NaN   \n",
      "476                   NaN                  NaN                   NaN   \n",
      "477                   NaN                  NaN                   NaN   \n",
      "478                   NaN                  NaN                   NaN   \n",
      "479                   NaN                  NaN                   NaN   \n",
      "\n",
      "     hist_precip_houston_tx  hb_hubavg  hb_west  hb_north  hb_south  \\\n",
      "0                       0.0      25.57    29.23     24.01     24.70   \n",
      "1                       0.0      25.24    28.70     23.80     24.38   \n",
      "2                       0.0      24.25    27.13     23.18     23.42   \n",
      "3                       0.0      25.13    28.17     24.09     24.18   \n",
      "4                       0.0      27.02    29.71     26.10     26.16   \n",
      "..                      ...        ...      ...       ...       ...   \n",
      "475                     NaN        NaN      NaN       NaN       NaN   \n",
      "476                     NaN        NaN      NaN       NaN       NaN   \n",
      "477                     NaN        NaN      NaN       NaN       NaN   \n",
      "478                     NaN        NaN      NaN       NaN       NaN   \n",
      "479                     NaN        NaN      NaN       NaN       NaN   \n",
      "\n",
      "     hb_houston  hb_pan  \n",
      "0         24.36   22.85  \n",
      "1         24.08   23.02  \n",
      "2         23.27   23.21  \n",
      "3         24.10   24.53  \n",
      "4         26.10   26.60  \n",
      "..          ...     ...  \n",
      "475         NaN     NaN  \n",
      "476         NaN     NaN  \n",
      "477         NaN     NaN  \n",
      "478         NaN     NaN  \n",
      "479         NaN     NaN  \n",
      "\n",
      "[480 rows x 72 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW vw_master_spine_lags AS\n",
    "SELECT\n",
    "  t.*,\n",
    "  LAG(hb_houston, 1)   OVER (ORDER BY ts) AS p_lag1,\n",
    "  LAG(hb_houston, 2)   OVER (ORDER BY ts) AS p_lag2,\n",
    "  LAG(hb_houston, 3)   OVER (ORDER BY ts) AS p_lag3,\n",
    "  LAG(hb_houston, 6)   OVER (ORDER BY ts) AS p_lag6,\n",
    "  LAG(hb_houston, 12)  OVER (ORDER BY ts) AS p_lag12,\n",
    "  LAG(hb_houston, 24)  OVER (ORDER BY ts) AS p_lag24,\n",
    "  LAG(hb_houston, 48)  OVER (ORDER BY ts) AS p_lag48,\n",
    "  LAG(hb_houston, 72)  OVER (ORDER BY ts) AS p_lag72,\n",
    "  LAG(hb_houston, 168) OVER (ORDER BY ts) AS p_lag168,\n",
    "  hb_houston - LAG(hb_houston, 1)  OVER (ORDER BY ts) AS dp1,\n",
    "  hb_houston - LAG(hb_houston, 24) OVER (ORDER BY ts) AS dp24,\n",
    "  AVG(hb_houston) OVER (ORDER BY ts ROWS BETWEEN 23 PRECEDING AND CURRENT ROW)  AS p_roll24_mean,\n",
    "  STDDEV_SAMP(hb_houston) OVER (ORDER BY ts ROWS BETWEEN 23 PRECEDING AND CURRENT ROW) AS p_roll24_std,\n",
    "  AVG(hb_houston) OVER (ORDER BY ts ROWS BETWEEN 71 PRECEDING AND CURRENT ROW)  AS p_roll72_mean,\n",
    "  AVG(hb_houston) OVER (ORDER BY ts ROWS BETWEEN 167 PRECEDING AND CURRENT ROW) AS p_roll168_mean\n",
    "FROM vw_master_spine_ts t\n",
    "ORDER BY ts;\n",
    "\"\"\")\n",
    "\n",
    "print(df)\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3751599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    OperatingDTM   n\n",
      "0     2025-08-23  24\n",
      "1     2025-08-22  24\n",
      "2     2025-08-21  24\n",
      "3     2025-08-20  24\n",
      "4     2025-08-19  24\n",
      "..           ...  ..\n",
      "961   2023-01-05  24\n",
      "962   2023-01-04  24\n",
      "963   2023-01-03  24\n",
      "964   2023-01-02  24\n",
      "965   2023-01-01  24\n",
      "\n",
      "[966 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#########           sanity checks\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "-- 24 rows per day?\n",
    "SELECT OperatingDTM, COUNT(*) AS n\n",
    "FROM vw_master_spine_ts\n",
    "GROUP BY 1 ORDER BY 1 DESC;\n",
    "\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "con.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63e8aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OperatingDTM  Interval  n\n",
      "0   2024-11-03         2  2\n",
      "1   2023-11-05         2  2\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "-- 1 row per (OperatingDTM, Interval)?\n",
    "SELECT OperatingDTM, Interval, COUNT(*) AS n\n",
    "FROM vw_master_spine_ts\n",
    "GROUP BY 1,2 HAVING COUNT(*)>1;\n",
    "\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e98801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count_star()\n",
      "0             0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "\n",
    "-- any null prices inside the historical period?\n",
    "SELECT COUNT(*) FROM vw_master_spine_ts WHERE hb_houston IS NULL AND ts <= (SELECT MAX(ts) FROM vw_master_spine_ts WHERE hb_houston IS NOT NULL);\n",
    "\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "print(df)\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f312e1",
   "metadata": {},
   "source": [
    "Now we have two new additons to the master spine view: \n",
    "# vw_master_spine_ts \n",
    "# vw_master_spine_lags "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b28dbf",
   "metadata": {},
   "source": [
    "Next, lets build horizon-specific datasets (future-known covariates shifted by H):\n",
    "\n",
    "For each horizon H = 1..24, we’ll:\n",
    "\n",
    "Use recency features at time t from vw_master_spine_lags (b).\n",
    "\n",
    "Join future-known features (load forecast + forecast weather + calendar at the delivery hour) from the same view shifted to t+H (f).\n",
    "\n",
    "Create the label y_hH = LEAD(hb_houston, H)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc5e7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "\n",
    "# columns to fetch as FUTURE covariates (from 'f' alias; adjust to your exact columns)\n",
    "FUTURE_LOAD_COLS = [\n",
    "    \"wz_southcentral\",\"wz_east\",\"wz_west\",\"wz_northcentral\",\"wz_farwest\",\"wz_north\",\"wz_southern\",\"wz_coast\",\n",
    "    \"lz_north\",\"lz_west\",\"lz_south\",\"lz_houston\"\n",
    "]\n",
    "FUTURE_WX_COLS = [\n",
    "    \"temp_the_woodlands_tx\",\"hum_the_woodlands_tx\",\"wind_the_woodlands_tx\",\"precip_the_woodlands_tx\",\n",
    "    \"temp_katy_tx\",\"hum_katy_tx\",\"wind_katy_tx\",\"precip_katy_tx\",\n",
    "    \"temp_friendswood_tx\",\"hum_friendswood_tx\",\"wind_friendswood_tx\",\"precip_friendswood_tx\",\n",
    "    \"temp_baytown_tx\",\"hum_baytown_tx\",\"wind_baytown_tx\",\"precip_baytown_tx\",\n",
    "    \"temp_houston_tx\",\"hum_houston_tx\",\"wind_houston_tx\",\"precip_houston_tx\"\n",
    "]\n",
    "FUTURE_CAL_COLS = [\"cal_hour\",\"cal_dow\",\"cal_is_weekend\",\"cal_sin_hour\",\"cal_cos_hour\",\"cal_sin_dow\",\"cal_cos_dow\"]\n",
    "\n",
    "RECENCY_COLS = [\n",
    "    \"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\",\n",
    "    # historical weather recency (at t)\n",
    "] + [c for c in FUTURE_WX_COLS if c.startswith(\"hist_\")]  # if you created hist_* columns in master\n",
    "\n",
    "def fetch_horizon_df(h: int) -> pd.DataFrame:\n",
    "    con = duckdb.connect(DB)\n",
    "    # Build select lists\n",
    "    future_select = \",\\n       \".join([f\"f.{c} AS {c}_f\" for c in FUTURE_LOAD_COLS + FUTURE_WX_COLS + FUTURE_CAL_COLS])\n",
    "    recency_select = \",\\n       \".join([f\"b.{c}\" for c in RECENCY_COLS])\n",
    "    q = f\"\"\"\n",
    "    WITH base AS (\n",
    "      SELECT *\n",
    "      FROM vw_master_spine_lags\n",
    "    ),\n",
    "    future AS (\n",
    "      SELECT *\n",
    "      FROM vw_master_spine_ts\n",
    "    )\n",
    "    SELECT\n",
    "      b.ts, b.OperatingDTM, b.Interval,\n",
    "      b.hb_houston,            -- current observed price (for reference)\n",
    "      {recency_select},\n",
    "      {future_select},\n",
    "      LEAD(b.hb_houston, {h}) OVER (ORDER BY b.ts) AS target\n",
    "    FROM base b\n",
    "    LEFT JOIN future f\n",
    "      ON f.ts = b.ts + INTERVAL {h} HOUR\n",
    "    ORDER BY b.ts;\n",
    "    \"\"\"\n",
    "    df = con.execute(q).df()\n",
    "    con.close()\n",
    "    # Drop rows with missing target or missing future features\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9193f7",
   "metadata": {},
   "source": [
    "finally, we're ready to do introductory ML baselining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "\n",
    "def eval_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    try:\n",
    "        rmse = root_mean_squared_error(y_true, y_pred)  # newer sklearn\n",
    "    except TypeError:\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))        # fallback\n",
    "    mape = float(np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-6, None))) * 100)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE%\": mape, \"R2\": r2}\n",
    "\n",
    "MODELS = {\n",
    "    \"Linear\": LinearRegression(),\n",
    "    \"Ridge\":  Ridge(alpha=5.0),\n",
    "    \"Lasso\":  Lasso(alpha=0.001, max_iter=20000),\n",
    "    \"XGB\":    XGBRegressor(n_estimators=800, learning_rate=0.05, max_depth=6,\n",
    "                           subsample=0.9, colsample_bytree=0.9, random_state=42),\n",
    "    \"LGBM\":   LGBMRegressor(n_estimators=1200, learning_rate=0.05, num_leaves=127,\n",
    "                            subsample=0.9, colsample_bytree=0.9, random_state=42),\n",
    "}\n",
    "\n",
    "def train_horizon(h: int):\n",
    "    df = fetch_horizon_df(h)\n",
    "    # Feature matrix\n",
    "    drop_cols = [\"ts\",\"OperatingDTM\",\"Interval\",\"hb_houston\",\"target\"]\n",
    "    X = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    y = df[\"target\"].values\n",
    "\n",
    "    # Walk-forward split (3 folds example)\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    results = {}\n",
    "    for name, mdl in MODELS.items():\n",
    "        fold_scores = []\n",
    "        for tr_idx, te_idx in tscv.split(X):\n",
    "            Xtr, Xte = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "            ytr, yte = y[tr_idx], y[te_idx]\n",
    "\n",
    "            if name in (\"Linear\",\"Ridge\",\"Lasso\"):\n",
    "                scaler = StandardScaler()\n",
    "                Xtr2 = scaler.fit_transform(Xtr)\n",
    "                Xte2 = scaler.transform(Xte)\n",
    "                mdl.fit(Xtr2, ytr)\n",
    "                yhat = mdl.predict(Xte2)\n",
    "            else:\n",
    "                mdl.fit(Xtr, ytr)\n",
    "                yhat = mdl.predict(Xte)\n",
    "\n",
    "            fold_scores.append(eval_metrics(yte, yhat))\n",
    "        # average across folds\n",
    "        results[name] = {\n",
    "            k: float(np.mean([fs[k] for fs in fold_scores])) for k in fold_scores[0].keys()\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# Run for horizons 1..24\n",
    "all_metrics = {h: train_horizon(h) for h in range(1,25)}\n",
    "for h in range(1,25):\n",
    "    best = min(all_metrics[h].items(), key=lambda kv: kv[1][\"RMSE\"])\n",
    "    print(f\"H{h:02d} best: {best[0]}  →  {best[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a10481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [ts, OperatingDTM, Interval, f_operatingdtm, f_interval, f_cal_hour]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "-- For a chosen horizon H (say H=6), verify that future calendar aligns\n",
    "WITH b AS (\n",
    "  SELECT ts, OperatingDTM, Interval\n",
    "  FROM vw_master_spine_lags\n",
    "  WHERE hb_houston IS NOT NULL\n",
    "  ORDER BY ts\n",
    "  LIMIT 200\n",
    "),\n",
    "f AS (\n",
    "  SELECT ts, OperatingDTM AS f_operatingdtm, Interval AS f_interval, cal_hour AS f_cal_hour\n",
    "  FROM vw_master_spine_ts\n",
    ")\n",
    "SELECT\n",
    "  b.ts,\n",
    "  b.OperatingDTM, b.Interval,\n",
    "  f.f_operatingdtm, f.f_interval, f.f_cal_hour\n",
    "FROM b\n",
    "LEFT JOIN f\n",
    "  ON f.ts = b.ts + INTERVAL 6 HOUR\n",
    "WHERE (b.Interval + 6 - 1) % 24 + 1 <> f.f_cal_hour\n",
    "LIMIT 5;\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "print(df)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b26674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "\n",
    "CREATE OR REPLACE VIEW load_fcst_enh AS\n",
    "WITH z AS (\n",
    "  SELECT\n",
    "    OperatingDTM, Interval,\n",
    "    (wz_southcentral + wz_east + wz_west + wz_northcentral + wz_farwest + wz_north + wz_southern + wz_coast) AS net_wz,\n",
    "    (lz_north + lz_west + lz_south + lz_houston) AS net_lz,\n",
    "    GREATEST(wz_southcentral, wz_east, wz_west, wz_northcentral, wz_farwest, wz_north, wz_southern, wz_coast) AS wz_max,\n",
    "    LEAST(  wz_southcentral, wz_east, wz_west, wz_northcentral, wz_farwest, wz_north, wz_southern, wz_coast) AS wz_min\n",
    "  FROM vw_loadforecast_by_zone\n",
    ")\n",
    "SELECT\n",
    "  *,\n",
    "  net_wz - LAG(net_wz) OVER (ORDER BY OperatingDTM, Interval) AS net_wz_ramp1,\n",
    "  net_lz - LAG(net_lz) OVER (ORDER BY OperatingDTM, Interval) AS net_lz_ramp1,\n",
    "  (wz_max - wz_min) AS wz_spread\n",
    "FROM z;\n",
    "\"\"\").fetchdf()\n",
    "print(df)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f4f79c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      OperatingDTM  Interval      net_wz        net_lz      wz_max     wz_min  \\\n",
      "0       2023-01-01         1  35811.1029  35811.102844   9965.3701   758.7730   \n",
      "1       2023-01-01         2  35059.0604  35059.060486   9831.7500   761.5800   \n",
      "2       2023-01-01         3  34451.1034  34451.103333   9737.5303   728.3730   \n",
      "3       2023-01-01         4  34032.5793  34032.579346   9606.1904   724.6090   \n",
      "4       2023-01-01         5  33818.4683  33818.468384   9510.8096   753.9290   \n",
      "...            ...       ...         ...           ...         ...        ...   \n",
      "23178   2025-08-23        20  72789.6773  72789.677368  21937.4258  1751.6400   \n",
      "23179   2025-08-23        21  70209.0098  70209.009888  20975.9746  1687.8900   \n",
      "23180   2025-08-23        22  68042.1196  68042.119629  20113.9238  1637.4351   \n",
      "23181   2025-08-23        23  64982.1933  64982.193359  18937.5488  1535.3601   \n",
      "23182   2025-08-23        24  61709.0737  61709.073608  17665.5742  1456.1176   \n",
      "\n",
      "       net_wz_ramp1  net_lz_ramp1   wz_spread  \n",
      "0               NaN           NaN   9206.5971  \n",
      "1         -752.0425   -752.042358   9070.1700  \n",
      "2         -607.9570   -607.957153   9009.1573  \n",
      "3         -418.5241   -418.523987   8881.5814  \n",
      "4         -214.1110   -214.110962   8756.8806  \n",
      "...             ...           ...         ...  \n",
      "23178    -2910.6261  -2910.626099  20185.7858  \n",
      "23179    -2580.6675  -2580.667480  19288.0846  \n",
      "23180    -2166.8902  -2166.890259  18476.4887  \n",
      "23181    -3059.9263  -3059.926270  17402.1887  \n",
      "23182    -3273.1196  -3273.119751  16209.4566  \n",
      "\n",
      "[23183 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "\n",
    "select * from load_fcst_enh\n",
    "\"\"\").fetchdf()\n",
    "print(df)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f78313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Day-ahead block backtest (uses your existing fetch_horizon_df) ---\n",
    "import duckdb, pandas as pd, numpy as np\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "\n",
    "def day_ahead_backtest(models_by_h, scalers_by_h=None):\n",
    "    con = duckdb.connect(DB)\n",
    "    # Base times = end-of-day hours where we have price and a full next day available\n",
    "    base_df = con.execute(\"\"\"\n",
    "        WITH t AS (\n",
    "          SELECT ts, OperatingDTM, Interval, hb_houston\n",
    "          FROM vw_master_spine_lags\n",
    "          WHERE hb_houston IS NOT NULL\n",
    "        )\n",
    "        SELECT *\n",
    "        FROM t\n",
    "        WHERE Interval = 24\n",
    "          AND ts + INTERVAL 24 HOUR <= (SELECT MAX(ts) FROM t)  -- ensure tomorrow exists\n",
    "        ORDER BY ts\n",
    "    \"\"\").df()\n",
    "    con.close()\n",
    "\n",
    "    records = []\n",
    "    for _, row in base_df.iterrows():\n",
    "        ts0 = row[\"ts\"]  # end of day\n",
    "        preds, acts = [], []\n",
    "        for h in range(1, 25):\n",
    "            # build one-row features for base ts0 and delivery ts0+h\n",
    "            # (same logic as in fetch_horizon_df but filtered to ts==ts0)\n",
    "            q = f\"\"\"\n",
    "            WITH b AS (\n",
    "              SELECT *\n",
    "              FROM vw_master_spine_lags\n",
    "              WHERE ts = TIMESTAMP '{ts0}'\n",
    "            ),\n",
    "            f AS (\n",
    "              SELECT *\n",
    "              FROM vw_master_spine_ts\n",
    "              WHERE ts = TIMESTAMP '{ts0}' + INTERVAL {h} HOUR\n",
    "            )\n",
    "            SELECT\n",
    "              b.ts, b.OperatingDTM, b.Interval,\n",
    "              b.hb_houston,\n",
    "              -- recency features at t\n",
    "              p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "              dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean,\n",
    "              -- (optional) hist_* recency if you added them:\n",
    "              {\", \".join(\n",
    "                  [c for c in [\n",
    "                      \"hist_temp_the_woodlands_tx\",\"hist_hum_the_woodlands_tx\",\n",
    "                      \"hist_wind_the_woodlands_tx\",\"hist_precip_the_woodlands_tx\",\n",
    "                      \"hist_temp_katy_tx\",\"hist_hum_katy_tx\",\"hist_wind_katy_tx\",\"hist_precip_katy_tx\",\n",
    "                      \"hist_temp_friendswood_tx\",\"hist_hum_friendswood_tx\",\"hist_wind_friendswood_tx\",\"hist_precip_friendswood_tx\",\n",
    "                      \"hist_temp_baytown_tx\",\"hist_hum_baytown_tx\",\"hist_wind_baytown_tx\",\"hist_precip_baytown_tx\",\n",
    "                      \"hist_temp_houston_tx\",\"hist_hum_houston_tx\",\"hist_wind_houston_tx\",\"hist_precip_houston_tx\"\n",
    "                  ] if c in pd.read_sql(\"SELECT * FROM vw_master_spine_lags LIMIT 1\", duckdb.connect(DB)).columns]\n",
    "              ) or \"/* no hist_* in view */\"} \n",
    "              ,\n",
    "              -- future-known covariates at t+h\n",
    "              f.wz_southcentral AS wz_southcentral_f, f.wz_east AS wz_east_f, f.wz_west AS wz_west_f,\n",
    "              f.wz_northcentral AS wz_northcentral_f, f.wz_farwest AS wz_farwest_f, f.wz_north AS wz_north_f,\n",
    "              f.wz_southern AS wz_southern_f, f.wz_coast AS wz_coast_f,\n",
    "              f.lz_north AS lz_north_f, f.lz_west AS lz_west_f, f.lz_south AS lz_south_f, f.lz_houston AS lz_houston_f,\n",
    "              f.temp_the_woodlands_tx AS temp_the_woodlands_tx_f, f.hum_the_woodlands_tx AS hum_the_woodlands_tx_f,\n",
    "              f.wind_the_woodlands_tx AS wind_the_woodlands_tx_f, f.precip_the_woodlands_tx AS precip_the_woodlands_tx_f,\n",
    "              f.temp_katy_tx AS temp_katy_tx_f, f.hum_katy_tx AS hum_katy_tx_f, f.wind_katy_tx AS wind_katy_tx_f, f.precip_katy_tx AS precip_katy_tx_f,\n",
    "              f.temp_friendswood_tx AS temp_friendswood_tx_f, f.hum_friendswood_tx AS hum_friendswood_tx_f,\n",
    "              f.wind_friendswood_tx AS wind_friendswood_tx_f, f.precip_friendswood_tx AS precip_friendswood_tx_f,\n",
    "              f.temp_baytown_tx AS temp_baytown_tx_f, f.hum_baytown_tx AS hum_baytown_tx_f, f.wind_baytown_tx AS wind_baytown_tx_f, f.precip_baytown_tx AS precip_baytown_tx_f,\n",
    "              f.temp_houston_tx AS temp_houston_tx_f, f.hum_houston_tx AS hum_houston_tx_f, f.wind_houston_tx AS wind_houston_tx_f, f.precip_houston_tx AS precip_houston_tx_f,\n",
    "              f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "              f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f, f.cal_sin_dow AS cal_sin_dow_f, f.cal_cos_dow AS cal_cos_dow_f,\n",
    "              -- label: actual price at t+h\n",
    "              (SELECT hb_houston FROM vw_master_spine_ts WHERE ts = TIMESTAMP '{ts0}' + INTERVAL {h} HOUR) AS target\n",
    "            FROM b\n",
    "            LEFT JOIN f ON TRUE\n",
    "            \"\"\"\n",
    "            con = duckdb.connect(DB)\n",
    "            feat = con.execute(q).df()\n",
    "            con.close()\n",
    "\n",
    "            # prune non-feature columns\n",
    "            X = feat.drop(columns=[c for c in [\"ts\",\"OperatingDTM\",\"Interval\",\"hb_houston\",\"target\"] if c in feat.columns], errors=\"ignore\")\n",
    "            mdl = models_by_h[h][\"model\"]\n",
    "            scaler = models_by_h[h].get(\"scaler\")\n",
    "\n",
    "            if scaler is not None:\n",
    "                yhat = mdl.predict(scaler.transform(X))\n",
    "            else:\n",
    "                yhat = mdl.predict(X)\n",
    "\n",
    "            preds.append(float(yhat[0]))\n",
    "            acts.append(float(feat[\"target\"].iloc[0]))\n",
    "\n",
    "        # day metrics\n",
    "        mae = float(np.mean(np.abs(np.array(acts) - np.array(preds))))\n",
    "        rmse = float(np.sqrt(np.mean((np.array(acts) - np.array(preds))**2)))\n",
    "        records.append({\"base_ts\": ts0, \"day_mae\": mae, \"day_rmse\": rmse})\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "645e8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tomorrow(models_by_h):\n",
    "    con = duckdb.connect(DB)\n",
    "    # last hour with a known price (end of today in your data)\n",
    "    t0 = con.execute(\"\"\"\n",
    "        SELECT MAX(ts) AS t0\n",
    "        FROM vw_master_spine_ts\n",
    "        WHERE hb_houston IS NOT NULL\n",
    "    \"\"\").fetchone()[0]\n",
    "\n",
    "    rows = []\n",
    "    for h in range(1, 25):\n",
    "        q = f\"\"\"\n",
    "        WITH b AS (SELECT * FROM vw_master_spine_lags WHERE ts = TIMESTAMP '{t0}'),\n",
    "             f AS (SELECT * FROM vw_master_spine_ts   WHERE ts = TIMESTAMP '{t0}' + INTERVAL {h} HOUR)\n",
    "        SELECT\n",
    "          b.OperatingDTM AS base_operatingdtm, b.Interval AS base_interval,\n",
    "          f.OperatingDTM AS delivery_operatingdtm, f.Interval AS delivery_interval,\n",
    "          -- features\n",
    "          p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "          dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean,\n",
    "          f.wz_southcentral AS wz_southcentral_f, f.wz_east AS wz_east_f, f.wz_west AS wz_west_f,\n",
    "          f.wz_northcentral AS wz_northcentral_f, f.wz_farwest AS wz_farwest_f, f.wz_north AS wz_north_f,\n",
    "          f.wz_southern AS wz_southern_f, f.wz_coast AS wz_coast_f,\n",
    "          f.lz_north AS lz_north_f, f.lz_west AS lz_west_f, f.lz_south AS lz_south_f, f.lz_houston AS lz_houston_f,\n",
    "          f.temp_the_woodlands_tx AS temp_the_woodlands_tx_f, f.hum_the_woodlands_tx AS hum_the_woodlands_tx_f,\n",
    "          f.wind_the_woodlands_tx AS wind_the_woodlands_tx_f, f.precip_the_woodlands_tx AS precip_the_woodlands_tx_f,\n",
    "          f.temp_katy_tx AS temp_katy_tx_f, f.hum_katy_tx AS hum_katy_tx_f, f.wind_katy_tx AS wind_katy_tx_f, f.precip_katy_tx AS precip_katy_tx_f,\n",
    "          f.temp_friendswood_tx AS temp_friendswood_tx_f, f.hum_friendswood_tx AS hum_friendswood_tx_f,\n",
    "          f.wind_friendswood_tx AS wind_friendswood_tx_f, f.precip_friendswood_tx AS precip_friendswood_tx_f,\n",
    "          f.temp_baytown_tx AS temp_baytown_tx_f, f.hum_baytown_tx AS hum_baytown_tx_f, f.wind_baytown_tx AS wind_baytown_tx_f, f.precip_baytown_tx AS precip_baytown_tx_f,\n",
    "          f.temp_houston_tx AS temp_houston_tx_f, f.hum_houston_tx AS hum_houston_tx_f, f.wind_houston_tx AS wind_houston_tx_f, f.precip_houston_tx AS precip_houston_tx_f,\n",
    "          f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "          f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f, f.cal_sin_dow AS cal_sin_dow_f, f.cal_cos_dow AS cal_cos_dow_f\n",
    "        FROM b LEFT JOIN f ON TRUE\n",
    "        \"\"\"\n",
    "        feat = con.execute(q).df()\n",
    "        X = feat.drop(columns=[c for c in feat.columns if c.endswith(\"_operatingdtm\") or c.endswith(\"_interval\")], errors=\"ignore\")\n",
    "        mdl = models_by_h[h][\"model\"]\n",
    "        scaler = models_by_h[h].get(\"scaler\")\n",
    "        yhat = mdl.predict(scaler.transform(X) if scaler is not None else X)[0]\n",
    "        rows.append({\n",
    "            \"OperatingDTM\": feat[\"delivery_operatingdtm\"].iloc[0],\n",
    "            \"Interval\": int(feat[\"delivery_interval\"].iloc[0]),\n",
    "            \"hb_houston_pred\": float(yhat)\n",
    "        })\n",
    "    con.close()\n",
    "    out = pd.DataFrame(rows).sort_values([\"OperatingDTM\",\"Interval\"]).reset_index(drop=True)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dec1693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_rows\n",
      "0   23063\n",
      "   n_eod\n",
      "0    961\n",
      "       last_priced_ts\n",
      "0 2025-08-18 23:00:00\n",
      "   tomorrow_hours\n",
      "0              24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "SELECT COUNT(*) AS n_rows\n",
    "FROM vw_master_spine_ts\n",
    "WHERE hb_houston IS NOT NULL;\n",
    "    \"\"\").fetchdf()       \n",
    "\n",
    "df1 = con.execute(\"\"\"      \n",
    "SELECT COUNT(*) AS n_eod\n",
    "FROM vw_master_spine_ts\n",
    "WHERE hb_houston IS NOT NULL\n",
    "  AND Interval = 24;\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "df2 = con.execute(\"\"\"      \n",
    "SELECT MAX(ts) AS last_priced_ts\n",
    "FROM vw_master_spine_ts\n",
    "WHERE hb_houston IS NOT NULL;\n",
    "\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "df3 = con.execute(\"\"\"      \n",
    "WITH last AS (\n",
    "  SELECT DATE(MAX(ts)) AS d\n",
    "  FROM vw_master_spine_ts\n",
    "  WHERE hb_houston IS NOT NULL\n",
    ")\n",
    "SELECT COUNT(*) AS tomorrow_hours\n",
    "FROM vw_master_spine_ts f\n",
    "JOIN last l ON f.OperatingDTM = l.d + INTERVAL 1 DAY;\n",
    "\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(df)\n",
    "print(df1)\n",
    "print(df2)\n",
    "print(df3)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "159a3810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                base_ts  base_date delivery_date  delivery_interval  \\\n",
      "0   2023-01-01 23:00:00 2023-01-01    2023-01-02                  1   \n",
      "1   2023-01-01 23:00:00 2023-01-01    2023-01-02                  2   \n",
      "2   2023-01-01 23:00:00 2023-01-01    2023-01-02                  3   \n",
      "3   2023-01-01 23:00:00 2023-01-01    2023-01-02                  4   \n",
      "4   2023-01-01 23:00:00 2023-01-01    2023-01-02                  5   \n",
      "..                  ...        ...           ...                ...   \n",
      "115 2023-01-05 23:00:00 2023-01-05    2023-01-06                 20   \n",
      "116 2023-01-05 23:00:00 2023-01-05    2023-01-06                 21   \n",
      "117 2023-01-05 23:00:00 2023-01-05    2023-01-06                 22   \n",
      "118 2023-01-05 23:00:00 2023-01-05    2023-01-06                 23   \n",
      "119 2023-01-05 23:00:00 2023-01-05    2023-01-06                 24   \n",
      "\n",
      "     target_price  wz_southcentral  wz_east    wz_west  wz_northcentral  \\\n",
      "0            9.31        5729.4399  1212.96  1037.0500        9694.4102   \n",
      "1            4.98        5824.6499  1193.15  1035.4000        9357.8701   \n",
      "2            3.83        5698.0801  1179.40   971.2230        9188.5400   \n",
      "3            3.71        5611.3501  1129.63   960.4850        9054.4902   \n",
      "4            7.35        5589.4302  1154.87   989.7360        9147.0703   \n",
      "..            ...              ...      ...        ...              ...   \n",
      "115         20.50        7110.0200  1484.25  1118.9500       12109.7002   \n",
      "116         19.11        6874.1499  1433.16  1163.9399       11732.5996   \n",
      "117         19.52        6737.1001  1394.14  1086.8500       11518.4004   \n",
      "118         18.33        6518.4600  1313.11  1104.2500       11269.4004   \n",
      "119         17.84        6178.3799  1273.28  1026.8101       10709.7998   \n",
      "\n",
      "     wz_farwest  ...  hum_baytown_tx  wind_baytown_tx  precip_baytown_tx  \\\n",
      "0     5073.7598  ...             NaN              NaN                NaN   \n",
      "1     5033.2500  ...             NaN              NaN                NaN   \n",
      "2     5126.4502  ...             NaN              NaN                NaN   \n",
      "3     5085.7500  ...             NaN              NaN                NaN   \n",
      "4     5099.9800  ...             NaN              NaN                NaN   \n",
      "..          ...  ...             ...              ...                ...   \n",
      "115   4918.1499  ...             NaN              NaN                NaN   \n",
      "116   5396.6802  ...             NaN              NaN                NaN   \n",
      "117   5622.1401  ...             NaN              NaN                NaN   \n",
      "118   5401.3101  ...             NaN              NaN                NaN   \n",
      "119   5225.0400  ...             NaN              NaN                NaN   \n",
      "\n",
      "     temp_houston_tx  hum_houston_tx  wind_houston_tx  precip_houston_tx  \\\n",
      "0                NaN             NaN              NaN                NaN   \n",
      "1                NaN             NaN              NaN                NaN   \n",
      "2                NaN             NaN              NaN                NaN   \n",
      "3                NaN             NaN              NaN                NaN   \n",
      "4                NaN             NaN              NaN                NaN   \n",
      "..               ...             ...              ...                ...   \n",
      "115              NaN             NaN              NaN                NaN   \n",
      "116              NaN             NaN              NaN                NaN   \n",
      "117              NaN             NaN              NaN                NaN   \n",
      "118              NaN             NaN              NaN                NaN   \n",
      "119              NaN             NaN              NaN                NaN   \n",
      "\n",
      "     cal_hour_f  cal_dow_f  cal_is_weekend_f  \n",
      "0             1          1                 0  \n",
      "1             2          1                 0  \n",
      "2             3          1                 0  \n",
      "3             4          1                 0  \n",
      "4             5          1                 0  \n",
      "..          ...        ...               ...  \n",
      "115          20          5                 0  \n",
      "116          21          5                 0  \n",
      "117          22          5                 0  \n",
      "118          23          5                 0  \n",
      "119          24          5                 0  \n",
      "\n",
      "[120 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "-- Inspect: day-ahead pairs (base end-of-day -> next day 24 hours)\n",
    "WITH base AS (\n",
    "  SELECT ts AS base_ts, OperatingDTM AS base_date, Interval\n",
    "  FROM vw_master_spine_lags\n",
    "  WHERE hb_houston IS NOT NULL\n",
    "    AND Interval = 24\n",
    "),\n",
    "future AS (\n",
    "  SELECT *\n",
    "  FROM vw_master_spine_ts\n",
    ")\n",
    "SELECT\n",
    "  b.base_ts,\n",
    "  b.base_date,\n",
    "  f.OperatingDTM   AS delivery_date,       -- should be base_date + 1 day\n",
    "  f.Interval       AS delivery_interval,   -- 1..24\n",
    "  f.hb_houston     AS target_price,        -- for evaluation\n",
    "  -- recency at base time (add whatever you computed in *_lags)\n",
    "  -- example:\n",
    "  -- lags\n",
    "  -- (You can include more recency cols from vw_master_spine_lags by joining b->lags on base_ts)\n",
    "  -- future-known covariates at delivery hour (already in future view)\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.temp_the_woodlands_tx, f.hum_the_woodlands_tx, f.wind_the_woodlands_tx, f.precip_the_woodlands_tx,\n",
    "  f.temp_katy_tx,          f.hum_katy_tx,          f.wind_katy_tx,          f.precip_katy_tx,\n",
    "  f.temp_friendswood_tx,   f.hum_friendswood_tx,   f.wind_friendswood_tx,   f.precip_friendswood_tx,\n",
    "  f.temp_baytown_tx,       f.hum_baytown_tx,       f.wind_baytown_tx,       f.precip_baytown_tx,\n",
    "  f.temp_houston_tx,       f.hum_houston_tx,       f.wind_houston_tx,       f.precip_houston_tx,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f\n",
    "FROM base b\n",
    "JOIN future f\n",
    "  ON f.OperatingDTM = b.base_date + INTERVAL 1 DAY\n",
    "ORDER BY b.base_ts, f.Interval\n",
    "LIMIT 120;  -- just to preview\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "print(df)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b9b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import duckdb, pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "\n",
    "def predict_tomorrow_simple(models_by_h):\n",
    "    con = duckdb.connect(DB)\n",
    "\n",
    "    # Last priced hour (base time)\n",
    "    t0 = con.execute(\"\"\"\n",
    "        SELECT MAX(ts) FROM vw_master_spine_ts\n",
    "        WHERE hb_houston IS NOT NULL\n",
    "    \"\"\").fetchone()[0]\n",
    "    if t0 is None:\n",
    "        con.close()\n",
    "        raise RuntimeError(\"No priced rows found in vw_master_spine_ts.\")\n",
    "\n",
    "    # Base recency features at t0 (one row)\n",
    "    b = con.execute(\"\"\"\n",
    "        SELECT *\n",
    "        FROM vw_master_spine_lags\n",
    "        WHERE ts = ?\n",
    "    \"\"\", [t0]).df()\n",
    "\n",
    "    # Delivery day = DATE(t0) + 1 day\n",
    "    base_date = pd.to_datetime(t0).date()\n",
    "    delivery_date = pd.Timestamp(base_date) + pd.Timedelta(days=1)\n",
    "\n",
    "    # Future-known features for the 24 delivery hours\n",
    "    f = con.execute(\"\"\"\n",
    "        SELECT *\n",
    "        FROM vw_master_spine_ts\n",
    "        WHERE OperatingDTM = ?\n",
    "        ORDER BY Interval\n",
    "    \"\"\", [delivery_date]).df()\n",
    "\n",
    "    con.close()\n",
    "\n",
    "    if f.empty:\n",
    "        raise RuntimeError(f\"No future rows for delivery_date={delivery_date}. \"\n",
    "                           \"Check that vw_master_spine_ts has tomorrow's spine rows.\")\n",
    "\n",
    "    rows = []\n",
    "    for h in range(1, 25):\n",
    "        feat = pd.concat([b.reset_index(drop=True), f.iloc[[h-1]].reset_index(drop=True)], axis=1)\n",
    "        X = feat.drop(columns=[c for c in feat.columns\n",
    "                               if c in (\"ts\",\"OperatingDTM\",\"Interval\",\"hb_houston\")\n",
    "                               or c.endswith(\"_x\") or c.endswith(\"_y\")], errors=\"ignore\")\n",
    "        mdl = models_by_h[h][\"model\"]\n",
    "        scaler = models_by_h[h].get(\"scaler\")\n",
    "        yhat = mdl.predict(scaler.transform(X) if scaler is not None else X)[0]\n",
    "        rows.append({\"OperatingDTM\": delivery_date.date(), \"Interval\": h, \"hb_houston_pred\": float(yhat)})\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b3ac45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_rows</th>\n",
       "      <th>eod_rows</th>\n",
       "      <th>priced_rows</th>\n",
       "      <th>priced_eod_rows</th>\n",
       "      <th>tomorrow_spine_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23183</td>\n",
       "      <td>966</td>\n",
       "      <td>23063</td>\n",
       "      <td>961</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_rows  eod_rows  priced_rows  priced_eod_rows  tomorrow_spine_rows\n",
       "0       23183       966        23063              961                   24"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "WITH last AS (\n",
    "  SELECT DATE(MAX(ts)) AS d\n",
    "  FROM vw_master_spine_ts\n",
    "  WHERE hb_houston IS NOT NULL\n",
    ")\n",
    "SELECT\n",
    "  (SELECT COUNT(*) FROM vw_master_spine_ts)                                                     AS total_rows,\n",
    "  (SELECT COUNT(*) FROM vw_master_spine_ts WHERE Interval = 24)                                 AS eod_rows,\n",
    "  (SELECT COUNT(*) FROM vw_master_spine_ts WHERE hb_houston IS NOT NULL)                        AS priced_rows,\n",
    "  (SELECT COUNT(*) FROM vw_master_spine_ts WHERE hb_houston IS NOT NULL AND Interval = 24)      AS priced_eod_rows,\n",
    "  (SELECT COUNT(*) FROM vw_master_spine_ts f, last l\n",
    "     WHERE f.OperatingDTM = l.d + INTERVAL 1 DAY)                                               AS tomorrow_spine_rows;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "con.close()\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d468558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last priced: {'last_priced_ts': Timestamp('2025-08-18 23:00:00'), 'last_priced_date': Timestamp('2025-08-18 00:00:00')}\n",
      "Tomorrow spine rows after last priced day: 24\n",
      "Top 5 day-ahead pairs (for evaluation):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_date</th>\n",
       "      <th>base_ts</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>nextday_price_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>2025-08-18 23:00:00</td>\n",
       "      <td>2025-08-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>2025-08-17 23:00:00</td>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>2025-08-16 23:00:00</td>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>2025-08-15 23:00:00</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>2025-08-14 23:00:00</td>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   base_date             base_ts delivery_date  nextday_price_hours\n",
       "0 2025-08-18 2025-08-18 23:00:00    2025-08-19                    0\n",
       "1 2025-08-17 2025-08-17 23:00:00    2025-08-18                   24\n",
       "2 2025-08-16 2025-08-16 23:00:00    2025-08-17                   24\n",
       "3 2025-08-15 2025-08-15 23:00:00    2025-08-16                   24\n",
       "4 2025-08-14 2025-08-14 23:00:00    2025-08-15                   24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid day-ahead evaluation days (full next-day actuals): 955\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "con = duckdb.connect(DB)\n",
    "\n",
    "# Last priced timestamp and date\n",
    "last_priced = con.execute(\"\"\"\n",
    "SELECT\n",
    "  MAX(ts) AS last_priced_ts,\n",
    "  DATE(MAX(ts)) AS last_priced_date\n",
    "FROM vw_master_spine_ts\n",
    "WHERE hb_houston IS NOT NULL\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "# How many tomorrow spine rows exist after that date?\n",
    "tomorrow_cnt = con.execute(\"\"\"\n",
    "WITH last AS (\n",
    "  SELECT DATE(MAX(ts)) AS d\n",
    "  FROM vw_master_spine_ts\n",
    "  WHERE hb_houston IS NOT NULL\n",
    ")\n",
    "SELECT COUNT(*) AS tomorrow_spine_rows\n",
    "FROM vw_master_spine_ts f, last l\n",
    "WHERE f.OperatingDTM = l.d + INTERVAL 1 DAY\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "# Build a list of base days that have Interval=24 price AND the next day has 24 actual prices\n",
    "day_pairs = con.execute(\"\"\"\n",
    "WITH base AS (\n",
    "  SELECT DATE(ts) AS base_date, ts AS base_ts\n",
    "  FROM vw_master_spine_ts\n",
    "  WHERE hb_houston IS NOT NULL AND Interval = 24\n",
    "),\n",
    "nextday_prices AS (\n",
    "  SELECT OperatingDTM AS delivery_date, COUNT(*) AS n_price_hours\n",
    "  FROM vw_master_spine_ts\n",
    "  WHERE hb_houston IS NOT NULL\n",
    "  GROUP BY OperatingDTM\n",
    ")\n",
    "SELECT\n",
    "  b.base_date,\n",
    "  b.base_ts,\n",
    "  (b.base_date + INTERVAL 1 DAY) AS delivery_date,\n",
    "  COALESCE(n.n_price_hours, 0)   AS nextday_price_hours\n",
    "FROM base b\n",
    "LEFT JOIN nextday_prices n\n",
    "  ON n.delivery_date = b.base_date + INTERVAL 1 DAY\n",
    "ORDER BY b.base_date DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(\"Last priced:\", last_priced.iloc[0].to_dict())\n",
    "print(\"Tomorrow spine rows after last priced day:\", int(tomorrow_cnt.iloc[0,0]))\n",
    "print(\"Top 5 day-ahead pairs (for evaluation):\")\n",
    "display(day_pairs.head())\n",
    "\n",
    "valid_eval_days = day_pairs[day_pairs[\"nextday_price_hours\"] == 24]\n",
    "print(\"Valid day-ahead evaluation days (full next-day actuals):\", len(valid_eval_days))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbd81f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick refit per horizon on all available rows (dropna) — Ridge baseline\n",
    "import duckdb, pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "\n",
    "def refit_ridge_models():\n",
    "    models_by_h = {}\n",
    "    con = duckdb.connect(DB)\n",
    "    for h in range(1, 25):\n",
    "        # Build horizon dataset: recency at t + future-known covariates at t+h + label y_h=h\n",
    "        df = con.execute(f\"\"\"\n",
    "            WITH b AS (\n",
    "              SELECT *,\n",
    "                     LEAD(hb_houston, {h}) OVER (ORDER BY ts) AS target\n",
    "              FROM vw_master_spine_lags\n",
    "            ),\n",
    "            f AS (\n",
    "              SELECT *\n",
    "              FROM vw_master_spine_ts\n",
    "            )\n",
    "            SELECT\n",
    "              b.*,  -- base recency & current price (hb_houston)\n",
    "              -- future-known covariates at t+h (join by ts + h)\n",
    "              f.wz_southcentral AS wz_southcentral_f, f.wz_east AS wz_east_f, f.wz_west AS wz_west_f,\n",
    "              f.wz_northcentral AS wz_northcentral_f, f.wz_farwest AS wz_farwest_f, f.wz_north AS wz_north_f,\n",
    "              f.wz_southern AS wz_southern_f, f.wz_coast AS wz_coast_f,\n",
    "              f.lz_north AS lz_north_f, f.lz_west AS lz_west_f, f.lz_south AS lz_south_f, f.lz_houston AS lz_houston_f,\n",
    "              f.temp_the_woodlands_tx AS temp_the_woodlands_tx_f, f.hum_the_woodlands_tx AS hum_the_woodlands_tx_f,\n",
    "              f.wind_the_woodlands_tx AS wind_the_woodlands_tx_f, f.precip_the_woodlands_tx AS precip_the_woodlands_tx_f,\n",
    "              f.temp_katy_tx AS temp_katy_tx_f, f.hum_katy_tx AS hum_katy_tx_f, f.wind_katy_tx AS wind_katy_tx_f, f.precip_katy_tx AS precip_katy_tx_f,\n",
    "              f.temp_friendswood_tx AS temp_friendswood_tx_f, f.hum_friendswood_tx AS hum_friendswood_tx_f,\n",
    "              f.wind_friendswood_tx AS wind_friendswood_tx_f, f.precip_friendswood_tx AS precip_friendswood_tx_f,\n",
    "              f.temp_baytown_tx AS temp_baytown_tx_f, f.hum_baytown_tx AS hum_baytown_tx_f, f.wind_baytown_tx AS wind_baytown_tx_f, f.precip_baytown_tx AS precip_baytown_tx_f,\n",
    "              f.temp_houston_tx AS temp_houston_tx_f, f.hum_houston_tx AS hum_houston_tx_f, f.wind_houston_tx AS wind_houston_tx_f, f.precip_houston_tx AS precip_houston_tx_f,\n",
    "              f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "              f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f, f.cal_sin_dow AS cal_sin_dow_f, f.cal_cos_dow AS cal_cos_dow_f\n",
    "            FROM b\n",
    "            LEFT JOIN f ON f.ts = b.ts + INTERVAL {h} HOUR\n",
    "            ORDER BY b.ts\n",
    "        \"\"\").fetchdf()\n",
    "        con.close() if h == 24 else None\n",
    "\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        y = df[\"target\"].values\n",
    "        X = df.drop(columns=[c for c in [\"ts\",\"OperatingDTM\",\"Interval\",\"hb_houston\",\"target\"] if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        Xs = scaler.fit_transform(X)\n",
    "        model = Ridge(alpha=5.0)\n",
    "        model.fit(Xs, y)\n",
    "        models_by_h[h] = {\"model\": model, \"scaler\": scaler}\n",
    "        con = duckdb.connect(DB)  # reopen for next loop\n",
    "\n",
    "    con.close()\n",
    "    return models_by_h\n",
    "\n",
    "# models_by_h = refit_ridge_models()\n",
    "# forecast_df = predict_tomorrow(models_by_h)\n",
    "# display(forecast_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "\n",
    "def refit_ridge_models():\n",
    "    models_by_h = {}\n",
    "    con = duckdb.connect(DB)\n",
    "    for h in range(1, 25):\n",
    "        df = con.execute(f\"\"\"\n",
    "            WITH b AS (\n",
    "              SELECT *,\n",
    "                     LEAD(hb_houston, {h}) OVER (ORDER BY ts) AS target\n",
    "              FROM vw_master_spine_lags\n",
    "            ),\n",
    "            f AS (\n",
    "              SELECT ts, OperatingDTM, Interval,\n",
    "                     wz_southcentral, wz_east, wz_west, wz_northcentral, wz_farwest, wz_north, wz_southern, wz_coast,\n",
    "                     lz_north, lz_west, lz_south, lz_houston,\n",
    "                     temp_the_woodlands_tx, hum_the_woodlands_tx, wind_the_woodlands_tx, precip_the_woodlands_tx,\n",
    "                     temp_katy_tx,          hum_katy_tx,          wind_katy_tx,          precip_katy_tx,\n",
    "                     temp_friendswood_tx,   hum_friendswood_tx,   wind_friendswood_tx,   precip_friendswood_tx,\n",
    "                     temp_baytown_tx,       hum_baytown_tx,       wind_baytown_tx,       precip_baytown_tx,\n",
    "                     temp_houston_tx,       hum_houston_tx,       wind_houston_tx,       precip_houston_tx,\n",
    "                     cal_hour, cal_dow, cal_is_weekend, cal_sin_hour, cal_cos_hour, cal_sin_dow, cal_cos_dow\n",
    "              FROM vw_master_spine_ts\n",
    "            )\n",
    "            SELECT\n",
    "              b.*,  -- recency + target\n",
    "              f.wz_southcentral AS wz_southcentral_f, f.wz_east AS wz_east_f, f.wz_west AS wz_west_f,\n",
    "              f.wz_northcentral AS wz_northcentral_f, f.wz_farwest AS wz_farwest_f, f.wz_north AS wz_north_f,\n",
    "              f.wz_southern AS wz_southern_f, f.wz_coast AS wz_coast_f,\n",
    "              f.lz_north AS lz_north_f, f.lz_west AS lz_west_f, f.lz_south AS lz_south_f, f.lz_houston AS lz_houston_f,\n",
    "              f.temp_the_woodlands_tx AS temp_the_woodlands_tx_f, f.hum_the_woodlands_tx AS hum_the_woodlands_tx_f,\n",
    "              f.wind_the_woodlands_tx AS wind_the_woodlands_tx_f, f.precip_the_woodlands_tx AS precip_the_woodlands_tx_f,\n",
    "              f.temp_katy_tx AS temp_katy_tx_f, f.hum_katy_tx AS hum_katy_tx_f, f.wind_katy_tx AS wind_katy_tx_f, f.precip_katy_tx AS precip_katy_tx_f,\n",
    "              f.temp_friendswood_tx AS temp_friendswood_tx_f, f.hum_friendswood_tx AS hum_friendswood_tx_f,\n",
    "              f.wind_friendswood_tx AS wind_friendswood_tx_f, f.precip_friendswood_tx AS precip_friendswood_tx_f,\n",
    "              f.temp_baytown_tx AS temp_baytown_tx_f, f.hum_baytown_tx AS hum_baytown_tx_f, f.wind_baytown_tx AS wind_baytown_tx_f, f.precip_baytown_tx AS precip_baytown_tx_f,\n",
    "              f.temp_houston_tx AS temp_houston_tx_f, f.hum_houston_tx AS hum_houston_tx_f, f.wind_houston_tx AS wind_houston_tx_f, f.precip_houston_tx AS precip_houston_tx_f,\n",
    "              f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "              f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f, f.cal_sin_dow AS cal_sin_dow_f, f.cal_cos_dow AS cal_cos_dow_f\n",
    "            FROM b\n",
    "            LEFT JOIN f ON f.ts = b.ts + INTERVAL {h} HOUR\n",
    "            ORDER BY b.ts\n",
    "        \"\"\").fetchdf()\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        y = df[\"target\"].values\n",
    "        X = df.drop(columns=[c for c in [\"ts\",\"OperatingDTM\",\"Interval\",\"hb_houston\",\"target\"] if c in df.columns], errors=\"ignore\")\n",
    "        scaler, model = StandardScaler(), Ridge(alpha=5.0)\n",
    "        Xs = scaler.fit_transform(X); model.fit(Xs, y)\n",
    "        models_by_h[h] = {\"model\": model, \"scaler\": scaler}\n",
    "    con.close()\n",
    "    return models_by_h\n",
    "\n",
    "def predict_tomorrow(models_by_h):\n",
    "    con = duckdb.connect(DB)\n",
    "    t0 = con.execute(\"\"\"\n",
    "        SELECT MAX(ts) FROM vw_master_spine_ts\n",
    "        WHERE hb_houston IS NOT NULL\n",
    "    \"\"\").fetchone()[0]\n",
    "    b = con.execute(\"SELECT * FROM vw_master_spine_lags WHERE ts = ?\", [t0]).fetchdf()\n",
    "    f = con.execute(\"\"\"\n",
    "        SELECT * FROM vw_master_spine_ts\n",
    "        WHERE OperatingDTM = DATE(?::TIMESTAMP) + INTERVAL 1 DAY\n",
    "        ORDER BY Interval\n",
    "    \"\"\", [t0]).fetchdf()\n",
    "    con.close()\n",
    "    if f.empty or len(f) < 24:\n",
    "        raise RuntimeError(\"Tomorrow is not fully present in the spine (need 24 rows).\")\n",
    "    rows = []\n",
    "    for h in range(1, 25):\n",
    "        feat = pd.concat([b.reset_index(drop=True), f.iloc[[h-1]].reset_index(drop=True)], axis=1)\n",
    "        X = feat.drop(columns=[c for c in [\"ts\",\"OperatingDTM\",\"Interval\",\"hb_houston\"] if c in feat.columns], errors=\"ignore\")\n",
    "        mdl = models_by_h[h][\"model\"]; scaler = models_by_h[h].get(\"scaler\")\n",
    "        yhat = mdl.predict(scaler.transform(X) if scaler is not None else X)[0]\n",
    "        rows.append({\"OperatingDTM\": f.loc[h-1, \"OperatingDTM\"], \"Interval\": int(f.loc[h-1, \"Interval\"]), \"hb_houston_pred\": float(yhat)})\n",
    "    return pd.DataFrame(rows).sort_values([\"OperatingDTM\",\"Interval\"]).reset_index(drop=True)\n",
    "\n",
    "# Use existing models_by_h if you already trained; else auto-fit Ridge baselines:\n",
    "if 'models_by_h' not in globals():\n",
    "    models_by_h = refit_ridge_models()\n",
    "\n",
    "forecast_df = predict_tomorrow(models_by_h)\n",
    "forecast_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0478ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_days': 60, 'rows': 1440, 'MAE': 32.765872300955444, 'RMSE': 44.48798502742817, 'R2': -1.525670042704765}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['temp_the_woodlands_tx' 'hum_the_woodlands_tx' 'wind_the_woodlands_tx'\n",
      " 'precip_the_woodlands_tx' 'temp_katy_tx' 'hum_katy_tx' 'wind_katy_tx'\n",
      " 'precip_katy_tx' 'temp_friendswood_tx' 'hum_friendswood_tx'\n",
      " 'wind_friendswood_tx' 'precip_friendswood_tx' 'temp_baytown_tx'\n",
      " 'hum_baytown_tx' 'wind_baytown_tx' 'precip_baytown_tx' 'temp_houston_tx'\n",
      " 'hum_houston_tx' 'wind_houston_tx' 'precip_houston_tx']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['temp_the_woodlands_tx' 'hum_the_woodlands_tx' 'wind_the_woodlands_tx'\n",
      " 'precip_the_woodlands_tx' 'temp_katy_tx' 'hum_katy_tx' 'wind_katy_tx'\n",
      " 'precip_katy_tx' 'temp_friendswood_tx' 'hum_friendswood_tx'\n",
      " 'wind_friendswood_tx' 'precip_friendswood_tx' 'temp_baytown_tx'\n",
      " 'hum_baytown_tx' 'wind_baytown_tx' 'precip_baytown_tx' 'temp_houston_tx'\n",
      " 'hum_houston_tx' 'wind_houston_tx' 'precip_houston_tx']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['temp_the_woodlands_tx' 'hum_the_woodlands_tx' 'wind_the_woodlands_tx'\n",
      " 'precip_the_woodlands_tx' 'temp_katy_tx' 'hum_katy_tx' 'wind_katy_tx'\n",
      " 'precip_katy_tx' 'temp_friendswood_tx' 'hum_friendswood_tx'\n",
      " 'wind_friendswood_tx' 'precip_friendswood_tx' 'temp_baytown_tx'\n",
      " 'hum_baytown_tx' 'wind_baytown_tx' 'precip_baytown_tx' 'temp_houston_tx'\n",
      " 'hum_houston_tx' 'wind_houston_tx' 'precip_houston_tx']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OperatingDTM</th>\n",
       "      <th>Interval</th>\n",
       "      <th>hb_houston_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>1</td>\n",
       "      <td>70.918419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>2</td>\n",
       "      <td>56.588500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>3</td>\n",
       "      <td>42.603458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>4</td>\n",
       "      <td>34.488922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>5</td>\n",
       "      <td>30.233206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>6</td>\n",
       "      <td>31.682249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>7</td>\n",
       "      <td>34.279342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>8</td>\n",
       "      <td>35.460637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>9</td>\n",
       "      <td>27.451566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>10</td>\n",
       "      <td>29.284216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>11</td>\n",
       "      <td>45.630472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>12</td>\n",
       "      <td>65.501261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>13</td>\n",
       "      <td>84.200071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>14</td>\n",
       "      <td>103.330077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>15</td>\n",
       "      <td>120.344578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>16</td>\n",
       "      <td>133.429777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>17</td>\n",
       "      <td>140.997644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>18</td>\n",
       "      <td>144.420063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>19</td>\n",
       "      <td>141.346126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>20</td>\n",
       "      <td>136.442469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>21</td>\n",
       "      <td>127.777626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>22</td>\n",
       "      <td>112.183882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>23</td>\n",
       "      <td>91.357381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>24</td>\n",
       "      <td>70.812466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OperatingDTM  Interval  hb_houston_pred\n",
       "0    2025-08-18         1        70.918419\n",
       "1    2025-08-18         2        56.588500\n",
       "2    2025-08-18         3        42.603458\n",
       "3    2025-08-18         4        34.488922\n",
       "4    2025-08-18         5        30.233206\n",
       "5    2025-08-18         6        31.682249\n",
       "6    2025-08-18         7        34.279342\n",
       "7    2025-08-18         8        35.460637\n",
       "8    2025-08-18         9        27.451566\n",
       "9    2025-08-18        10        29.284216\n",
       "10   2025-08-18        11        45.630472\n",
       "11   2025-08-18        12        65.501261\n",
       "12   2025-08-18        13        84.200071\n",
       "13   2025-08-18        14       103.330077\n",
       "14   2025-08-18        15       120.344578\n",
       "15   2025-08-18        16       133.429777\n",
       "16   2025-08-18        17       140.997644\n",
       "17   2025-08-18        18       144.420063\n",
       "18   2025-08-18        19       141.346126\n",
       "19   2025-08-18        20       136.442469\n",
       "20   2025-08-18        21       127.777626\n",
       "21   2025-08-18        22       112.183882\n",
       "22   2025-08-18        23        91.357381\n",
       "23   2025-08-18        24        70.812466"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb, pandas as pd, numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()  # <- change if needed\n",
    "\n",
    "con = duckdb.connect(DB)\n",
    "\n",
    "# Day-ahead dataset: base=end-of-day with price/recency, future=next day 24h exogenous + label (if known)\n",
    "day_ahead_sql = \"\"\"\n",
    "WITH base_days AS (\n",
    "  SELECT DATE(ts) AS base_date, ts AS base_ts\n",
    "  FROM vw_master_spine_lags\n",
    "  WHERE hb_houston IS NOT NULL AND Interval = 24\n",
    "),\n",
    "base_feats AS (\n",
    "  SELECT\n",
    "    ts AS base_ts,\n",
    "    hb_houston                                  AS price_t,\n",
    "    p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "    dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "  FROM vw_master_spine_lags\n",
    "),\n",
    "future_24 AS (\n",
    "  SELECT\n",
    "    OperatingDTM    AS delivery_date,\n",
    "    Interval        AS delivery_interval,\n",
    "    hb_houston      AS target,                           -- may be NULL for the newest day\n",
    "    wz_southcentral, wz_east, wz_west, wz_northcentral, wz_farwest, wz_north, wz_southern, wz_coast,\n",
    "    lz_north, lz_west, lz_south, lz_houston,\n",
    "    temp_the_woodlands_tx, hum_the_woodlands_tx, wind_the_woodlands_tx, precip_the_woodlands_tx,\n",
    "    temp_katy_tx,          hum_katy_tx,          wind_katy_tx,          precip_katy_tx,\n",
    "    temp_friendswood_tx,   hum_friendswood_tx,   wind_friendswood_tx,   precip_friendswood_tx,\n",
    "    temp_baytown_tx,       hum_baytown_tx,       wind_baytown_tx,       precip_baytown_tx,\n",
    "    temp_houston_tx,       hum_houston_tx,       wind_houston_tx,       precip_houston_tx,\n",
    "    cal_hour AS cal_hour_f, cal_dow AS cal_dow_f, cal_is_weekend AS cal_is_weekend_f,\n",
    "    cal_sin_hour AS cal_sin_hour_f, cal_cos_hour AS cal_cos_hour_f,\n",
    "    cal_sin_dow  AS cal_sin_dow_f,  cal_cos_dow  AS cal_cos_dow_f\n",
    "  FROM vw_master_spine_ts\n",
    ")\n",
    "SELECT\n",
    "  b.base_date,\n",
    "  b.base_date + INTERVAL 1 DAY        AS delivery_date,\n",
    "  f.delivery_interval,\n",
    "  bf.price_t,\n",
    "  bf.p_lag1,bf.p_lag2,bf.p_lag3,bf.p_lag6,bf.p_lag12,bf.p_lag24,bf.p_lag48,bf.p_lag72,bf.p_lag168,\n",
    "  bf.dp1,bf.dp24,bf.p_roll24_mean,bf.p_roll24_std,bf.p_roll72_mean,bf.p_roll168_mean,\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.temp_the_woodlands_tx, f.hum_the_woodlands_tx, f.wind_the_woodlands_tx, f.precip_the_woodlands_tx,\n",
    "  f.temp_katy_tx,          f.hum_katy_tx,          f.wind_katy_tx,          f.precip_katy_tx,\n",
    "  f.temp_friendswood_tx,   f.hum_friendswood_tx,   f.wind_friendswood_tx,   f.precip_friendswood_tx,\n",
    "  f.temp_baytown_tx,       f.hum_baytown_tx,       f.wind_baytown_tx,       f.precip_baytown_tx,\n",
    "  f.temp_houston_tx,       f.hum_houston_tx,       f.wind_houston_tx,       f.precip_houston_tx,\n",
    "  f.cal_hour_f, f.cal_dow_f, f.cal_is_weekend_f, f.cal_sin_hour_f, f.cal_cos_hour_f, f.cal_sin_dow_f, f.cal_cos_dow_f,\n",
    "  f.target\n",
    "FROM base_days b\n",
    "JOIN base_feats bf\n",
    "  ON bf.base_ts = b.base_ts\n",
    "JOIN future_24 f\n",
    "  ON f.delivery_date = b.base_date + INTERVAL 1 DAY\n",
    "ORDER BY b.base_date, f.delivery_interval;\n",
    "\"\"\"\n",
    "\n",
    "data = con.execute(day_ahead_sql).fetchdf()\n",
    "con.close()\n",
    "\n",
    "# Ensure types\n",
    "data[\"delivery_date\"] = pd.to_datetime(data[\"delivery_date\"]).dt.date\n",
    "data[\"delivery_interval\"] = data[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# --- Warm-up filter: keep rows with sufficient history to avoid NaNs in recency features\n",
    "required_recency = [\"p_lag24\",\"p_lag72\",\"p_lag168\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"]\n",
    "mask_warmup = data[required_recency].notna().all(axis=1)\n",
    "data = data[mask_warmup].reset_index(drop=True)\n",
    "\n",
    "# Train rows = where target is known\n",
    "train_data = data[~data[\"target\"].isna()].copy()\n",
    "\n",
    "# Hold out last 60 delivery days (if available) for evaluation\n",
    "if not train_data.empty:\n",
    "    last_day = max(train_data[\"delivery_date\"])\n",
    "    recent_cut = pd.Timestamp(last_day) - pd.Timedelta(days=60)\n",
    "    train_mask = pd.to_datetime(train_data[\"delivery_date\"]) <= recent_cut\n",
    "    has_eval = (~train_mask).any()\n",
    "else:\n",
    "    raise RuntimeError(\"No rows with known targets found after warm-up. Expand history or relax filters.\")\n",
    "\n",
    "# Features/target\n",
    "drop_non_features = {\"base_date\",\"delivery_date\",\"target\"}\n",
    "X_cols = [c for c in train_data.columns if c not in drop_non_features]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\",  StandardScaler()),\n",
    "    (\"model\",   Ridge(alpha=5.0))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "X_train = train_data.loc[train_mask, X_cols]\n",
    "y_train = train_data.loc[train_mask, \"target\"].values\n",
    "if X_train.shape[0] == 0:\n",
    "    # fallback: if the 60-day holdout leaves no training, train on all\n",
    "    X_train = train_data[X_cols]\n",
    "    y_train = train_data[\"target\"].values\n",
    "    has_eval = False\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate (optional)\n",
    "if has_eval:\n",
    "    X_eval = train_data.loc[~train_mask, X_cols]\n",
    "    y_eval = train_data.loc[~train_mask, \"target\"].values\n",
    "    y_pred = pipe.predict(X_eval)\n",
    "    mae = float(mean_absolute_error(y_eval, y_pred))\n",
    "    rmse = float(np.sqrt(np.mean((y_eval - y_pred)**2)))\n",
    "    r2   = float(r2_score(y_eval, y_pred))\n",
    "    print({\"eval_days\": int(pd.Series(train_data.loc[~train_mask, \"delivery_date\"]).nunique()),\n",
    "           \"rows\": int(len(y_eval)), \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
    "else:\n",
    "    print(\"No eval holdout (trained on all rows).\")\n",
    "\n",
    "# Predict the 24 prices for the requested delivery date\n",
    "pred_rows = data[(data[\"delivery_date\"] == DELIVERY_DATE)].copy()\n",
    "if pred_rows.empty or pred_rows[\"delivery_interval\"].nunique() < 24:\n",
    "    raise RuntimeError(f\"Spine missing for {DELIVERY_DATE} (need 24 intervals).\")\n",
    "\n",
    "X_pred = pred_rows[X_cols]\n",
    "yhat = pipe.predict(X_pred)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*24,\n",
    "    \"Interval\": pred_rows[\"delivery_interval\"].tolist(),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "}).sort_values(\"Interval\").reset_index(drop=True)\n",
    "\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843e5ac",
   "metadata": {},
   "source": [
    "Okay, finally some results after better defining the 24 hour set of expected results and confirming data was properly aligned.\n",
    "\n",
    "# LETS ENHANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "967b017a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['temp_the_woodlands_tx' 'hum_the_woodlands_tx' 'wind_the_woodlands_tx'\n",
      " 'precip_the_woodlands_tx' 'temp_katy_tx' 'hum_katy_tx' 'wind_katy_tx'\n",
      " 'precip_katy_tx' 'temp_friendswood_tx' 'hum_friendswood_tx'\n",
      " 'wind_friendswood_tx' 'precip_friendswood_tx' 'temp_baytown_tx'\n",
      " 'hum_baytown_tx' 'wind_baytown_tx' 'precip_baytown_tx' 'temp_houston_tx'\n",
      " 'hum_houston_tx' 'wind_houston_tx' 'precip_houston_tx' 'temp_avg'\n",
      " 'temp_spread' 'temp_avg_ramp1' 'hum_avg' 'hum_spread' 'hum_avg_ramp1']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9038\n",
      "[LightGBM] [Info] Number of data points in the train set: 21435, number of used features: 43\n",
      "[LightGBM] [Info] Start training from score 22.680000\n",
      "{'model': 'LGBM', 'eval_days': 60, 'rows': 1440, 'MAE': 15.082037261885308, 'RMSE': 37.02211994018373, 'R2': -0.7490962479080749}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['temp_the_woodlands_tx' 'hum_the_woodlands_tx' 'wind_the_woodlands_tx'\n",
      " 'precip_the_woodlands_tx' 'temp_katy_tx' 'hum_katy_tx' 'wind_katy_tx'\n",
      " 'precip_katy_tx' 'temp_friendswood_tx' 'hum_friendswood_tx'\n",
      " 'wind_friendswood_tx' 'precip_friendswood_tx' 'temp_baytown_tx'\n",
      " 'hum_baytown_tx' 'wind_baytown_tx' 'precip_baytown_tx' 'temp_houston_tx'\n",
      " 'hum_houston_tx' 'wind_houston_tx' 'precip_houston_tx' 'temp_avg'\n",
      " 'temp_spread' 'temp_avg_ramp1' 'hum_avg' 'hum_spread' 'hum_avg_ramp1']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['temp_the_woodlands_tx' 'hum_the_woodlands_tx' 'wind_the_woodlands_tx'\n",
      " 'precip_the_woodlands_tx' 'temp_katy_tx' 'hum_katy_tx' 'wind_katy_tx'\n",
      " 'precip_katy_tx' 'temp_friendswood_tx' 'hum_friendswood_tx'\n",
      " 'wind_friendswood_tx' 'precip_friendswood_tx' 'temp_baytown_tx'\n",
      " 'hum_baytown_tx' 'wind_baytown_tx' 'precip_baytown_tx' 'temp_houston_tx'\n",
      " 'hum_houston_tx' 'wind_houston_tx' 'precip_houston_tx' 'temp_avg'\n",
      " 'temp_spread' 'temp_avg_ramp1' 'hum_avg' 'hum_spread' 'hum_avg_ramp1']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OperatingDTM</th>\n",
       "      <th>Interval</th>\n",
       "      <th>hb_houston_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>1</td>\n",
       "      <td>28.807174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>2</td>\n",
       "      <td>28.189162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>3</td>\n",
       "      <td>26.260620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>4</td>\n",
       "      <td>26.062219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>5</td>\n",
       "      <td>25.447546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>6</td>\n",
       "      <td>26.150197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>7</td>\n",
       "      <td>26.055271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>8</td>\n",
       "      <td>25.386501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>9</td>\n",
       "      <td>20.661650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>10</td>\n",
       "      <td>22.233936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>11</td>\n",
       "      <td>34.100285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>12</td>\n",
       "      <td>39.189913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>13</td>\n",
       "      <td>37.870202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>14</td>\n",
       "      <td>46.671327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>15</td>\n",
       "      <td>63.160764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>16</td>\n",
       "      <td>65.817155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>17</td>\n",
       "      <td>65.072856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>18</td>\n",
       "      <td>60.270349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>19</td>\n",
       "      <td>95.627167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>20</td>\n",
       "      <td>130.216117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>21</td>\n",
       "      <td>82.277077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>22</td>\n",
       "      <td>52.542436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>23</td>\n",
       "      <td>41.965660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-08-17</td>\n",
       "      <td>24</td>\n",
       "      <td>29.657427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OperatingDTM  Interval  hb_houston_pred\n",
       "0    2025-08-17         1        28.807174\n",
       "1    2025-08-17         2        28.189162\n",
       "2    2025-08-17         3        26.260620\n",
       "3    2025-08-17         4        26.062219\n",
       "4    2025-08-17         5        25.447546\n",
       "5    2025-08-17         6        26.150197\n",
       "6    2025-08-17         7        26.055271\n",
       "7    2025-08-17         8        25.386501\n",
       "8    2025-08-17         9        20.661650\n",
       "9    2025-08-17        10        22.233936\n",
       "10   2025-08-17        11        34.100285\n",
       "11   2025-08-17        12        39.189913\n",
       "12   2025-08-17        13        37.870202\n",
       "13   2025-08-17        14        46.671327\n",
       "14   2025-08-17        15        63.160764\n",
       "15   2025-08-17        16        65.817155\n",
       "16   2025-08-17        17        65.072856\n",
       "17   2025-08-17        18        60.270349\n",
       "18   2025-08-17        19        95.627167\n",
       "19   2025-08-17        20       130.216117\n",
       "20   2025-08-17        21        82.277077\n",
       "21   2025-08-17        22        52.542436\n",
       "22   2025-08-17        23        41.965660\n",
       "23   2025-08-17        24        29.657427"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === ONE-CELL DAY-AHEAD GBM WITH RAMP/SPREAD FEATURES ===\n",
    "import duckdb, pandas as pd, numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Prefer LightGBM; fallback to Ridge if not present\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    USE_LGBM = True\n",
    "except Exception:\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    USE_LGBM = False\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-17\").date()   # <--- change target date here if needed\n",
    "\n",
    "con = duckdb.connect(DB)\n",
    "\n",
    "# --- 1) Enhanced feature views: load ramps/spreads + weather ramps/spreads (future-known) ---\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW load_fcst_enh AS\n",
    "WITH z AS (\n",
    "  SELECT\n",
    "    OperatingDTM, Interval,\n",
    "    -- system-level aggregates\n",
    "    (wz_southcentral + wz_east + wz_west + wz_northcentral + wz_farwest + wz_north + wz_southern + wz_coast) AS net_wz,\n",
    "    (lz_north + lz_west + lz_south + lz_houston) AS net_lz,\n",
    "    -- spatial spreads\n",
    "    GREATEST(wz_southcentral, wz_east, wz_west, wz_northcentral, wz_farwest, wz_north, wz_southern, wz_coast) AS wz_max,\n",
    "    LEAST(  wz_southcentral, wz_east, wz_west, wz_northcentral, wz_farwest, wz_north, wz_southern, wz_coast) AS wz_min,\n",
    "    GREATEST(lz_north, lz_west, lz_south, lz_houston) AS lz_max,\n",
    "    LEAST(  lz_north, lz_west, lz_south, lz_houston) AS lz_min,\n",
    "    lz_houston\n",
    "  FROM vw_loadforecast_by_zone\n",
    ")\n",
    "SELECT\n",
    "  *,\n",
    "  -- 1h ramps\n",
    "  net_wz - LAG(net_wz) OVER (ORDER BY OperatingDTM, Interval) AS net_wz_ramp1,\n",
    "  net_lz - LAG(net_lz) OVER (ORDER BY OperatingDTM, Interval) AS net_lz_ramp1,\n",
    "  lz_houston - LAG(lz_houston) OVER (ORDER BY OperatingDTM, Interval) AS lz_houston_ramp1,\n",
    "  -- spreads\n",
    "  (wz_max - wz_min) AS wz_spread,\n",
    "  (lz_max - lz_min) AS lz_spread\n",
    "FROM z;\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW wx_fcst_enh AS\n",
    "WITH w AS (\n",
    "  SELECT\n",
    "    OperatingDTM, \"interval\" AS Interval,\n",
    "    -- temperatures & humidity for your 5 cities\n",
    "    temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx,\n",
    "    hum_the_woodlands_tx,  hum_katy_tx,  hum_friendswood_tx,  hum_baytown_tx,  hum_houston_tx\n",
    "  FROM vw_forecast_weather_by_city\n",
    "),\n",
    "agg AS (\n",
    "  SELECT\n",
    "    OperatingDTM, Interval,\n",
    "    -- averages\n",
    "    (temp_the_woodlands_tx + temp_katy_tx + temp_friendswood_tx + temp_baytown_tx + temp_houston_tx)/5.0 AS temp_avg,\n",
    "    (hum_the_woodlands_tx  + hum_katy_tx  + hum_friendswood_tx  + hum_baytown_tx  + hum_houston_tx)/5.0  AS hum_avg,\n",
    "    -- spreads\n",
    "    GREATEST(temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx)\n",
    "      - LEAST(temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx) AS temp_spread,\n",
    "    GREATEST(hum_the_woodlands_tx, hum_katy_tx, hum_friendswood_tx, hum_baytown_tx, hum_houston_tx)\n",
    "      - LEAST(hum_the_woodlands_tx, hum_katy_tx, hum_friendswood_tx, hum_baytown_tx, hum_houston_tx) AS hum_spread\n",
    "  FROM w\n",
    ")\n",
    "SELECT\n",
    "  a.*,\n",
    "  -- 1h ramps on aggregates\n",
    "  a.temp_avg - LAG(a.temp_avg) OVER (ORDER BY OperatingDTM, Interval) AS temp_avg_ramp1,\n",
    "  a.hum_avg  - LAG(a.hum_avg)  OVER (ORDER BY OperatingDTM, Interval) AS hum_avg_ramp1\n",
    "FROM agg a;\n",
    "\"\"\")\n",
    "\n",
    "# --- 2) Day-ahead dataset (base=end-of-day recency; future=next-day 24h features + enhanced joins) ---\n",
    "day_ahead_sql = \"\"\"\n",
    "WITH base_days AS (\n",
    "  SELECT DATE(ts) AS base_date, ts AS base_ts\n",
    "  FROM vw_master_spine_lags\n",
    "  WHERE hb_houston IS NOT NULL AND Interval = 24\n",
    "),\n",
    "base_feats AS (\n",
    "  SELECT\n",
    "    ts AS base_ts,\n",
    "    hb_houston                                  AS price_t,\n",
    "    p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "    dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "  FROM vw_master_spine_lags\n",
    "),\n",
    "future_24 AS (\n",
    "  SELECT\n",
    "    OperatingDTM    AS delivery_date,\n",
    "    Interval        AS delivery_interval,\n",
    "    hb_houston      AS target,                           -- may be NULL for the newest day\n",
    "    -- base future-known features\n",
    "    wz_southcentral, wz_east, wz_west, wz_northcentral, wz_farwest, wz_north, wz_southern, wz_coast,\n",
    "    lz_north, lz_west, lz_south, lz_houston,\n",
    "    temp_the_woodlands_tx, hum_the_woodlands_tx, wind_the_woodlands_tx, precip_the_woodlands_tx,\n",
    "    temp_katy_tx,          hum_katy_tx,          wind_katy_tx,          precip_katy_tx,\n",
    "    temp_friendswood_tx,   hum_friendswood_tx,   wind_friendswood_tx,   precip_friendswood_tx,\n",
    "    temp_baytown_tx,       hum_baytown_tx,       wind_baytown_tx,       precip_baytown_tx,\n",
    "    temp_houston_tx,       hum_houston_tx,       wind_houston_tx,       precip_houston_tx,\n",
    "    cal_hour AS cal_hour_f, cal_dow AS cal_dow_f, cal_is_weekend AS cal_is_weekend_f,\n",
    "    cal_sin_hour AS cal_sin_hour_f, cal_cos_hour AS cal_cos_hour_f,\n",
    "    cal_sin_dow  AS cal_sin_dow_f,  cal_cos_dow  AS cal_cos_dow_f\n",
    "  FROM vw_master_spine_ts\n",
    ")\n",
    "SELECT\n",
    "  b.base_date,\n",
    "  b.base_date + INTERVAL 1 DAY        AS delivery_date,\n",
    "  f.delivery_interval,\n",
    "\n",
    "  -- recency at base EOD\n",
    "  bf.price_t,\n",
    "  bf.p_lag1,bf.p_lag2,bf.p_lag3,bf.p_lag6,bf.p_lag12,bf.p_lag24,bf.p_lag48,bf.p_lag72,bf.p_lag168,\n",
    "  bf.dp1,bf.dp24,bf.p_roll24_mean,bf.p_roll24_std,bf.p_roll72_mean,bf.p_roll168_mean,\n",
    "\n",
    "  -- future-known baseline features at delivery hour\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.temp_the_woodlands_tx, f.hum_the_woodlands_tx, f.wind_the_woodlands_tx, f.precip_the_woodlands_tx,\n",
    "  f.temp_katy_tx,          f.hum_katy_tx,          f.wind_katy_tx,          f.precip_katy_tx,\n",
    "  f.temp_friendswood_tx,   f.hum_friendswood_tx,   f.wind_friendswood_tx,   f.precip_friendswood_tx,\n",
    "  f.temp_baytown_tx,       f.hum_baytown_tx,       f.wind_baytown_tx,       f.precip_baytown_tx,\n",
    "  f.temp_houston_tx,       f.hum_houston_tx,       f.wind_houston_tx,       f.precip_houston_tx,\n",
    "  f.cal_hour_f, f.cal_dow_f, f.cal_is_weekend_f, f.cal_sin_hour_f, f.cal_cos_hour_f, f.cal_sin_dow_f, f.cal_cos_dow_f,\n",
    "\n",
    "  -- enhanced load features at delivery hour\n",
    "  lfe.net_wz, lfe.net_lz, lfe.net_wz_ramp1, lfe.net_lz_ramp1, lfe.lz_houston_ramp1, lfe.wz_spread, lfe.lz_spread,\n",
    "\n",
    "  -- enhanced weather features at delivery hour\n",
    "  wfe.temp_avg, wfe.temp_spread, wfe.temp_avg_ramp1,\n",
    "  wfe.hum_avg,  wfe.hum_spread,  wfe.hum_avg_ramp1,\n",
    "\n",
    "  -- target (actual hb_houston at delivery hour; may be NULL for newest day)\n",
    "  f.target\n",
    "\n",
    "FROM base_days b\n",
    "JOIN base_feats bf\n",
    "  ON bf.base_ts = b.base_ts\n",
    "JOIN future_24 f\n",
    "  ON f.delivery_date = b.base_date + INTERVAL 1 DAY\n",
    "LEFT JOIN load_fcst_enh lfe\n",
    "  ON lfe.OperatingDTM = f.delivery_date AND lfe.Interval = f.delivery_interval\n",
    "LEFT JOIN wx_fcst_enh wfe\n",
    "  ON wfe.OperatingDTM = f.delivery_date AND wfe.Interval = f.delivery_interval\n",
    "ORDER BY b.base_date, f.delivery_interval;\n",
    "\"\"\"\n",
    "data = con.execute(day_ahead_sql).fetchdf()\n",
    "con.close()\n",
    "\n",
    "# --- 3) Prep data: warm-up filter (avoid lag NaNs), train/eval split ---\n",
    "data[\"delivery_date\"] = pd.to_datetime(data[\"delivery_date\"]).dt.date\n",
    "data[\"delivery_interval\"] = data[\"delivery_interval\"].astype(int)\n",
    "\n",
    "required_recency = [\"p_lag24\",\"p_lag72\",\"p_lag168\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"]\n",
    "data = data[data[required_recency].notna().all(axis=1)].reset_index(drop=True)\n",
    "\n",
    "train_data = data[~data[\"target\"].isna()].copy()\n",
    "if train_data.empty:\n",
    "    raise RuntimeError(\"No rows with known targets after warm-up. Expand history or relax filters.\")\n",
    "\n",
    "# Hold out the last 60 delivery days for eval (if available)\n",
    "last_day = max(train_data[\"delivery_date\"])\n",
    "recent_cut = pd.Timestamp(last_day) - pd.Timedelta(days=60)\n",
    "train_mask = pd.to_datetime(train_data[\"delivery_date\"]) <= recent_cut\n",
    "has_eval = (~train_mask).any()\n",
    "\n",
    "# Features/target\n",
    "DROP_NON_FEATURES = {\"base_date\",\"delivery_date\",\"target\"}\n",
    "X_cols = [c for c in train_data.columns if c not in DROP_NON_FEATURES]\n",
    "\n",
    "if USE_LGBM:\n",
    "    pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"model\", LGBMRegressor(\n",
    "            objective=\"mae\",     # robust to spikes\n",
    "            n_estimators=1200,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=127,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "else:\n",
    "    # Fallback to Ridge if LightGBM isn't available\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\",  StandardScaler()),\n",
    "        (\"model\",   Ridge(alpha=5.0))\n",
    "    ])\n",
    "\n",
    "# Train\n",
    "X_train = train_data.loc[train_mask, X_cols] if has_eval else train_data[X_cols]\n",
    "y_train = train_data.loc[train_mask, \"target\"].values if has_eval else train_data[\"target\"].values\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the holdout (optional)\n",
    "if has_eval:\n",
    "    X_eval = train_data.loc[~train_mask, X_cols]\n",
    "    y_eval = train_data.loc[~train_mask, \"target\"].values\n",
    "    y_pred = pipe.predict(X_eval)\n",
    "    mae = float(mean_absolute_error(y_eval, y_pred))\n",
    "    rmse = float(np.sqrt(np.mean((y_eval - y_pred)**2)))\n",
    "    r2   = float(r2_score(y_eval, y_pred))\n",
    "    print({\"model\": \"LGBM\" if USE_LGBM else \"Ridge\", \"eval_days\": int(pd.Series(train_data.loc[~train_mask, \"delivery_date\"]).nunique()),\n",
    "           \"rows\": int(len(y_eval)), \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
    "\n",
    "# --- 4) Predict the 24 prices for DELIVERY_DATE ---\n",
    "pred_rows = data[(data[\"delivery_date\"] == DELIVERY_DATE)].copy()\n",
    "if pred_rows.empty or pred_rows[\"delivery_interval\"].nunique() < 24:\n",
    "    raise RuntimeError(f\"Spine missing for {DELIVERY_DATE} (need 24 intervals).\")\n",
    "\n",
    "X_pred = pred_rows[X_cols]\n",
    "yhat = pipe.predict(X_pred)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*24,\n",
    "    \"Interval\": pred_rows[\"delivery_interval\"].tolist(),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "}).sort_values(\"Interval\").reset_index(drop=True)\n",
    "\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3a5be1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23183,)\n",
      "(192,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1144: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1149: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1169: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  train_loss=nan  val_loss=nan\n",
      "Epoch 02  train_loss=nan  val_loss=nan\n",
      "Epoch 03  train_loss=nan  val_loss=nan\n",
      "Epoch 04  train_loss=nan  val_loss=nan\n",
      "Epoch 05  train_loss=nan  val_loss=nan\n",
      "Epoch 06  train_loss=nan  val_loss=nan\n",
      "Epoch 07  train_loss=nan  val_loss=nan\n",
      "Epoch 08  train_loss=nan  val_loss=nan\n",
      "Epoch 09  train_loss=nan  val_loss=nan\n",
      "Epoch 10  train_loss=nan  val_loss=nan\n",
      "Epoch 11  train_loss=nan  val_loss=nan\n",
      "Epoch 12  train_loss=nan  val_loss=nan\n",
      "Epoch 13  train_loss=nan  val_loss=nan\n",
      "Epoch 14  train_loss=nan  val_loss=nan\n",
      "Epoch 15  train_loss=nan  val_loss=nan\n",
      "Epoch 16  train_loss=nan  val_loss=nan\n",
      "Epoch 17  train_loss=nan  val_loss=nan\n",
      "Epoch 18  train_loss=nan  val_loss=nan\n",
      "Epoch 19  train_loss=nan  val_loss=nan\n",
      "Epoch 20  train_loss=nan  val_loss=nan\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 318\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m xenc, xdec, y, y0 \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[32m    317\u001b[39m     xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     yhat_n = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxenc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxdec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_teacher\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_prob\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     y_n = y.cpu().numpy()\n\u001b[32m    320\u001b[39m     \u001b[38;5;66;03m# invert scale\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# ==== ONE-CELL PYTORCH SEQ2SEQ (ENCODER: past W hours, DECODER: next 24 hours) ====\n",
    "import duckdb, pandas as pd, numpy as np, math, datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ---- Config ----\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()  # change target date if you want\n",
    "W_PAST = 168          # encoder window (hours)\n",
    "H_FUT = 24            # decoder steps (hours)\n",
    "HOLDOUT_DAYS = 60     # last N delivery days for evaluation\n",
    "HIDDEN = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if __import__(\"torch\").cuda.is_available() else \"cpu\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---- Ensure enhanced views exist (idempotent) ----\n",
    "con = duckdb.connect(DB)\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW load_fcst_enh AS\n",
    "WITH z AS (\n",
    "  SELECT\n",
    "    OperatingDTM, Interval,\n",
    "    -- system aggregates\n",
    "    (wz_southcentral + wz_east + wz_west + wz_northcentral + wz_farwest + wz_north + wz_southern + wz_coast) AS net_wz,\n",
    "    (lz_north + lz_west + lz_south + lz_houston) AS net_lz,\n",
    "    -- spreads (wz + lz)\n",
    "    GREATEST(wz_southcentral, wz_east, wz_west, wz_northcentral, wz_farwest, wz_north, wz_southern, wz_coast) AS wz_max,\n",
    "    LEAST(  wz_southcentral, wz_east, wz_west, wz_northcentral, wz_farwest, wz_north, wz_southern, wz_coast) AS wz_min,\n",
    "    GREATEST(lz_north, lz_west, lz_south, lz_houston) AS lz_max,\n",
    "    LEAST(  lz_north, lz_west, lz_south, lz_houston) AS lz_min,\n",
    "    -- keep lz_houston for its own ramp\n",
    "    lz_houston\n",
    "  FROM vw_loadforecast_by_zone\n",
    ")\n",
    "SELECT\n",
    "  z.*,\n",
    "  -- 1h ramps\n",
    "  net_wz      - LAG(net_wz)      OVER (ORDER BY OperatingDTM, Interval) AS net_wz_ramp1,\n",
    "  net_lz      - LAG(net_lz)      OVER (ORDER BY OperatingDTM, Interval) AS net_lz_ramp1,\n",
    "  lz_houston  - LAG(lz_houston)  OVER (ORDER BY OperatingDTM, Interval) AS lz_houston_ramp1,\n",
    "  -- spreads\n",
    "  (wz_max - wz_min) AS wz_spread,\n",
    "  (lz_max - lz_min) AS lz_spread\n",
    "FROM z;\n",
    "\"\"\")\n",
    "\n",
    "# Weather enhanced view (unchanged)\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW wx_fcst_enh AS\n",
    "WITH w AS (\n",
    "  SELECT\n",
    "    OperatingDTM, \"interval\" AS Interval,\n",
    "    temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx,\n",
    "    hum_the_woodlands_tx,  hum_katy_tx,  hum_friendswood_tx,  hum_baytown_tx,  hum_houston_tx\n",
    "  FROM vw_forecast_weather_by_city\n",
    "),\n",
    "agg AS (\n",
    "  SELECT\n",
    "    OperatingDTM, Interval,\n",
    "    (temp_the_woodlands_tx + temp_katy_tx + temp_friendswood_tx + temp_baytown_tx + temp_houston_tx)/5.0 AS temp_avg,\n",
    "    (hum_the_woodlands_tx  + hum_katy_tx  + hum_friendswood_tx  + hum_baytown_tx  + hum_houston_tx)/5.0  AS hum_avg,\n",
    "    GREATEST(temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx)\n",
    "      - LEAST(temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx) AS temp_spread,\n",
    "    GREATEST(hum_the_woodlands_tx, hum_katy_tx, hum_friendswood_tx, hum_baytown_tx, hum_houston_tx)\n",
    "      - LEAST(hum_the_woodlands_tx, hum_katy_tx, hum_friendswood_tx, hum_baytown_tx, hum_houston_tx) AS hum_spread\n",
    "  FROM w\n",
    ")\n",
    "SELECT\n",
    "  a.*,\n",
    "  a.temp_avg - LAG(a.temp_avg) OVER (ORDER BY OperatingDTM, Interval) AS temp_avg_ramp1,\n",
    "  a.hum_avg  - LAG(a.hum_avg)  OVER (ORDER BY OperatingDTM, Interval) AS hum_avg_ramp1\n",
    "FROM agg a;\n",
    "\"\"\")\n",
    "\n",
    "# quick smoke test (optional)\n",
    "print(con.execute(\"SELECT COUNT(*) FROM load_fcst_enh\").fetchone())\n",
    "print(con.execute(\"SELECT COUNT(*) FROM wx_fcst_enh\").fetchone())\n",
    "\n",
    "\n",
    "# ---- Pull core tables to pandas (small enough to fit in memory) ----\n",
    "df_lags = con.execute(\"\"\"\n",
    "SELECT\n",
    "  ts, OperatingDTM, Interval, hb_houston,\n",
    "  p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "  dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "FROM vw_master_spine_lags\n",
    "ORDER BY ts\n",
    "\"\"\").df()\n",
    "df_lags[\"ts\"] = pd.to_datetime(df_lags[\"ts\"])\n",
    "\n",
    "df_future = con.execute(\"\"\"\n",
    "SELECT\n",
    "  f.OperatingDTM AS delivery_date,\n",
    "  f.Interval     AS delivery_interval,\n",
    "  f.hb_houston   AS target,\n",
    "  -- base future features\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.temp_the_woodlands_tx, f.hum_the_woodlands_tx, f.wind_the_woodlands_tx, f.precip_the_woodlands_tx,\n",
    "  f.temp_katy_tx, f.hum_katy_tx, f.wind_katy_tx, f.precip_katy_tx,\n",
    "  f.temp_friendswood_tx, f.hum_friendswood_tx, f.wind_friendswood_tx, f.precip_friendswood_tx,\n",
    "  f.temp_baytown_tx, f.hum_baytown_tx, f.wind_baytown_tx, f.precip_baytown_tx,\n",
    "  f.temp_houston_tx, f.hum_houston_tx, f.wind_houston_tx, f.precip_houston_tx,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "  f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f,\n",
    "  f.cal_sin_dow  AS cal_sin_dow_f,  f.cal_cos_dow  AS cal_cos_dow_f,\n",
    "  -- enhanced joins\n",
    "  l.net_wz, l.net_lz, l.net_wz_ramp1, l.net_lz_ramp1, l.lz_houston_ramp1, l.wz_spread, l.lz_spread,\n",
    "  w.temp_avg, w.temp_spread, w.temp_avg_ramp1, w.hum_avg, w.hum_spread, w.hum_avg_ramp1\n",
    "FROM vw_master_spine_ts f\n",
    "LEFT JOIN load_fcst_enh l\n",
    "  ON l.OperatingDTM = f.OperatingDTM AND l.Interval = f.Interval\n",
    "LEFT JOIN wx_fcst_enh w\n",
    "  ON w.OperatingDTM = f.OperatingDTM AND w.Interval = f.Interval\n",
    "ORDER BY f.OperatingDTM, f.Interval\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "df_future[\"delivery_date\"] = pd.to_datetime(df_future[\"delivery_date\"]).dt.date\n",
    "df_future[\"delivery_interval\"] = df_future[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# ---- Build list of training samples: base = EOD with price AND next-day has 24 actuals ----\n",
    "# base_ts are end-of-day rows (Interval=24) with price\n",
    "base_rows = df_lags[(df_lags[\"hb_houston\"].notna()) & (df_lags[\"Interval\"]==24)][[\"ts\",\"OperatingDTM\",\"hb_houston\"]].copy()\n",
    "base_rows[\"base_date\"] = pd.to_datetime(base_rows[\"OperatingDTM\"]).dt.date\n",
    "\n",
    "# find delivery days with 24 actuals\n",
    "has24 = df_future.groupby(\"delivery_date\")[\"target\"].apply(lambda s: s.notna().sum()==24)\n",
    "valid_delivery_dates = set(has24[has24].index)\n",
    "\n",
    "# keep those base rows whose next day has 24 targets\n",
    "base_rows[\"delivery_date\"] = base_rows[\"base_date\"] + pd.Timedelta(days=1)\n",
    "base_rows = base_rows[base_rows[\"delivery_date\"].isin(valid_delivery_dates)].reset_index(drop=True)\n",
    "\n",
    "# ---- Assemble sequences ----\n",
    "enc_cols = [\n",
    "    \"hb_houston\",\"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"\n",
    "]\n",
    "dec_future_cols = [\n",
    "    # calendar & exogenous for each delivery hour (future-known)\n",
    "    \"cal_hour_f\",\"cal_dow_f\",\"cal_is_weekend_f\",\"cal_sin_hour_f\",\"cal_cos_hour_f\",\"cal_sin_dow_f\",\"cal_cos_dow_f\",\n",
    "    \"net_wz\",\"net_lz\",\"net_wz_ramp1\",\"net_lz_ramp1\",\"lz_houston_ramp1\",\"wz_spread\",\"lz_spread\",\n",
    "    \"temp_avg\",\"temp_spread\",\"temp_avg_ramp1\",\"hum_avg\",\"hum_spread\",\"hum_avg_ramp1\",\n",
    "    # (you can add specific city-level features too)\n",
    "]\n",
    "\n",
    "# index df_lags by ts for fast slicing\n",
    "df_lags_idx = df_lags.set_index(\"ts\").sort_index()\n",
    "\n",
    "samples = []\n",
    "for _, r in base_rows.iterrows():\n",
    "    ts0 = r[\"ts\"]  # base end-of-day timestamp\n",
    "    # Encoder window\n",
    "    start = ts0 - pd.Timedelta(hours=W_PAST-1)\n",
    "    enc_df = df_lags_idx.loc[start:ts0][enc_cols]\n",
    "    if enc_df.shape[0] != W_PAST or enc_df.isna().any().any():\n",
    "        continue  # skip incomplete windows\n",
    "    # Decoder future 24 rows\n",
    "    fdf = df_future[(df_future[\"delivery_date\"]==r[\"delivery_date\"])].sort_values(\"delivery_interval\")\n",
    "    if fdf.shape[0] != H_FUT:\n",
    "        continue\n",
    "    X_enc = enc_df.values.astype(np.float32)                               # (W, E)\n",
    "    X_dec = fdf[dec_future_cols].values.astype(np.float32)                 # (24, D)\n",
    "    y     = fdf[\"target\"].values.astype(np.float32)                        # (24,)\n",
    "    # previous observed price at base (to start decoding)\n",
    "    y0    = np.float32(r[\"hb_houston\"])\n",
    "    samples.append((X_enc, X_dec, y, y0, r[\"delivery_date\"]))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No training samples assembled. Check warm-up length W_PAST and data availability.\")\n",
    "\n",
    "# ---- Train/val split by delivery_date (hold out last HOLDOUT_DAYS) ----\n",
    "dates = sorted({s[4] for s in samples})\n",
    "cut_date = dates[-1] - dt.timedelta(days=HOLDOUT_DAYS) if len(dates)>HOLDOUT_DAYS else dates[0]\n",
    "train_idx = [i for i,s in enumerate(samples) if s[4] <= cut_date]\n",
    "val_idx   = [i for i,s in enumerate(samples) if s[4] >  cut_date]\n",
    "\n",
    "# stack into arrays\n",
    "def stack(idxs):\n",
    "    Xe = np.stack([samples[i][0] for i in idxs], axis=0)   # (N, W, E)\n",
    "    Xd = np.stack([samples[i][1] for i in idxs], axis=0)   # (N, 24, D)\n",
    "    Y  = np.stack([samples[i][2] for i in idxs], axis=0)   # (N, 24)\n",
    "    Y0 = np.stack([samples[i][3] for i in idxs], axis=0)   # (N,)\n",
    "    return Xe, Xd, Y, Y0\n",
    "\n",
    "Xe_tr, Xd_tr, Y_tr, Y0_tr = stack(train_idx)\n",
    "Xe_va, Xd_va, Y_va, Y0_va = stack(val_idx) if len(val_idx)>0 else (None, None, None, None)\n",
    "\n",
    "# ---- Normalize features (standardize enc & dec features; standardize target for stability) ----\n",
    "enc_scaler = StandardScaler().fit(Xe_tr.reshape(len(train_idx)*W_PAST, -1))\n",
    "dec_scaler = StandardScaler().fit(Xd_tr.reshape(len(train_idx)*H_FUT, -1))\n",
    "y_scaler   = StandardScaler().fit(Y_tr.reshape(-1,1))\n",
    "\n",
    "Xe_tr_n = enc_scaler.transform(Xe_tr.reshape(len(train_idx)*W_PAST, -1)).reshape(Xe_tr.shape)\n",
    "Xd_tr_n = dec_scaler.transform(Xd_tr.reshape(len(train_idx)*H_FUT, -1)).reshape(Xd_tr.shape)\n",
    "Y_tr_n  = y_scaler.transform(Y_tr.reshape(-1,1)).reshape(Y_tr.shape)\n",
    "Y0_tr_n = y_scaler.transform(Y0_tr.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = enc_scaler.transform(Xe_va.reshape(len(val_idx)*W_PAST, -1)).reshape(Xe_va.shape)\n",
    "    Xd_va_n = dec_scaler.transform(Xd_va.reshape(len(val_idx)*H_FUT, -1)).reshape(Xd_va.shape)\n",
    "    Y_va_n  = y_scaler.transform(Y_va.reshape(-1,1)).reshape(Y_va.shape)\n",
    "    Y0_va_n = y_scaler.transform(Y0_va.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "# ---- PyTorch Dataset/Dataloader ----\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xe, Xd, Y, Y0):\n",
    "        self.Xe = torch.from_numpy(Xe).to(torch.float32)\n",
    "        self.Xd = torch.from_numpy(Xd).to(torch.float32)\n",
    "        self.Y  = torch.from_numpy(Y).to(torch.float32)\n",
    "        self.Y0 = torch.from_numpy(Y0).to(torch.float32)\n",
    "    def __len__(self): return self.Xe.shape[0]\n",
    "    def __getitem__(self, i): return self.Xe[i], self.Xd[i], self.Y[i], self.Y0[i]\n",
    "\n",
    "bs = 32\n",
    "train_loader = torch.utils.data.DataLoader(Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=bs, shuffle=True)\n",
    "val_loader   = None if Xe_va is None else torch.utils.data.DataLoader(Seq2SeqDataset(Xe_va_n, Xd_va_n, Y_va_n, Y0_va_n), batch_size=bs, shuffle=False)\n",
    "\n",
    "# ---- Model: GRU encoder-decoder with autoregressive decoding (teacher forcing p=0.5) ----\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_size=in_dim, hidden_size=hidden, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        _, h = self.rnn(x)   # h: (1, B, H)\n",
    "        return h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_size=in_dim+1, hidden_size=hidden, batch_first=True)  # +1 for prev y\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "    def forward(self, future_feats, h0, y0, teacher=None, tf_prob=0.5):\n",
    "        \"\"\"\n",
    "        future_feats: (B, 24, D)\n",
    "        y0: (B,) normalized last price at base time\n",
    "        teacher: (B, 24) normalized targets (optional, for teacher forcing)\n",
    "        \"\"\"\n",
    "        B, T, D = future_feats.shape\n",
    "        y_prev = y0.view(B,1)           # (B,1)\n",
    "        outputs = []\n",
    "        h = h0\n",
    "        for t in range(T):\n",
    "            x_t = torch.cat([future_feats[:,t,:], y_prev], dim=1).unsqueeze(1)  # (B,1,D+1)\n",
    "            out, h = self.rnn(x_t, h)\n",
    "            y_t = self.out(out[:, -1, :]).squeeze(1)   # (B,)\n",
    "            outputs.append(y_t)\n",
    "            if (teacher is not None) and (np.random.rand() < tf_prob):\n",
    "                y_prev = teacher[:,t].view(B,1)\n",
    "            else:\n",
    "                y_prev = y_t.view(B,1)\n",
    "        return torch.stack(outputs, dim=1)   # (B, 24)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(enc_in, hidden)\n",
    "        self.dec = Decoder(dec_in, hidden)\n",
    "    def forward(self, x_enc, x_dec, y0, y_teacher=None, tf_prob=0.5):\n",
    "        h = self.enc(x_enc)\n",
    "        y_hat = self.dec(x_dec, h, y0, y_teacher, tf_prob)\n",
    "        return y_hat\n",
    "\n",
    "enc_in = Xe_tr_n.shape[2]\n",
    "dec_in = Xd_tr_n.shape[2]\n",
    "model = Seq2Seq(enc_in, dec_in, HIDDEN).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.HuberLoss()  # robust to spikes\n",
    "\n",
    "# ---- Train ----\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in train_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=y, tf_prob=0.5)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    if val_loader is None: return None\n",
    "    model.eval()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(val_loader.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    if va is None:\n",
    "        print(f\"Epoch {ep:02d}  train_loss={tr:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {ep:02d}  train_loss={tr:.4f}  val_loss={va:.4f}\")\n",
    "\n",
    "# ---- Evaluate on holdout in real units (MAE) ----\n",
    "if val_loader is not None:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).cpu().numpy()\n",
    "        y_n = y.cpu().numpy()\n",
    "        # invert scale\n",
    "        yhat = y_scaler.inverse_transform(yhat_n.reshape(-1,1)).reshape(yhat_n.shape)\n",
    "        yt    = y_scaler.inverse_transform(y_n.reshape(-1,1)).reshape(y_n.shape)\n",
    "        preds.append(yhat); trues.append(yt)\n",
    "    preds = np.vstack(preds); trues = np.vstack(trues)\n",
    "    print({\"holdout_MAE\": float(mean_absolute_error(trues.flatten(), preds.flatten()))})\n",
    "\n",
    "# ---- Predict the 24 prices for DELIVERY_DATE ----\n",
    "# Find the base_ts (end of previous day) for this delivery date\n",
    "base_row = base_rows[base_rows[\"delivery_date\"]==DELIVERY_DATE]\n",
    "if base_row.empty:\n",
    "    raise RuntimeError(f\"No base EOD found for delivery_date={DELIVERY_DATE} (check that previous day has Interval=24 price).\")\n",
    "ts0 = base_row.iloc[0][\"ts\"]\n",
    "y0  = np.float32(base_row.iloc[0][\"hb_houston\"])\n",
    "\n",
    "# Encoder window\n",
    "df_win = df_lags_idx.loc[ts0 - pd.Timedelta(hours=W_PAST-1): ts0][enc_cols]\n",
    "if df_win.shape[0] != W_PAST:\n",
    "    raise RuntimeError(\"Insufficient history for encoder window.\")\n",
    "X_enc_pred = df_win.values.astype(np.float32)[None, ...]                 # (1,W,E)\n",
    "# Decoder future features (24 rows for delivery date)\n",
    "fpred = df_future[(df_future[\"delivery_date\"]==DELIVERY_DATE)].sort_values(\"delivery_interval\")\n",
    "if fpred.shape[0] != H_FUT:\n",
    "    raise RuntimeError(\"Spine missing 24 future rows for delivery date.\")\n",
    "X_dec_pred = fpred[dec_future_cols].values.astype(np.float32)[None, ...] # (1,24,D)\n",
    "\n",
    "# Normalize\n",
    "X_enc_pred_n = enc_scaler.transform(X_enc_pred.reshape(W_PAST, -1)).reshape(1, W_PAST, -1)\n",
    "X_dec_pred_n = dec_scaler.transform(X_dec_pred.reshape(H_FUT,  -1)).reshape(1, H_FUT,  -1)\n",
    "y0_n = y_scaler.transform(np.array([[y0]], dtype=np.float32)).reshape(-1)[0]\n",
    "\n",
    "# Run model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_n = model(torch.from_numpy(X_enc_pred_n).to(DEVICE),\n",
    "                   torch.from_numpy(X_dec_pred_n).to(DEVICE),\n",
    "                   torch.tensor([y0_n], dtype=torch.float32, device=DEVICE),\n",
    "                   y_teacher=None, tf_prob=0.0).cpu().numpy()[0]\n",
    "yhat = y_scaler.inverse_transform(yhat_n.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*H_FUT,\n",
    "    \"Interval\": list(range(1, H_FUT+1)),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "})\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31bdd9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1216: RuntimeWarning: All-NaN slice encountered\n",
      "  return fnb._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "C:\\Users\\db\\AppData\\Local\\Temp\\ipykernel_6672\\893039283.py:21: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(X2d, axis=0)\n",
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:2015: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  train_loss=nan  val_loss=nan\n",
      "Epoch 02  train_loss=nan  val_loss=nan\n",
      "Epoch 03  train_loss=nan  val_loss=nan\n",
      "Epoch 04  train_loss=nan  val_loss=nan\n",
      "Epoch 05  train_loss=nan  val_loss=nan\n",
      "Epoch 06  train_loss=nan  val_loss=nan\n",
      "Epoch 07  train_loss=nan  val_loss=nan\n",
      "Epoch 08  train_loss=nan  val_loss=nan\n",
      "Epoch 09  train_loss=nan  val_loss=nan\n",
      "Epoch 10  train_loss=nan  val_loss=nan\n",
      "Epoch 11  train_loss=nan  val_loss=nan\n",
      "Epoch 12  train_loss=nan  val_loss=nan\n",
      "Epoch 13  train_loss=nan  val_loss=nan\n",
      "Epoch 14  train_loss=nan  val_loss=nan\n",
      "Epoch 15  train_loss=nan  val_loss=nan\n",
      "Epoch 16  train_loss=nan  val_loss=nan\n",
      "Epoch 17  train_loss=nan  val_loss=nan\n",
      "Epoch 18  train_loss=nan  val_loss=nan\n",
      "Epoch 19  train_loss=nan  val_loss=nan\n",
      "Epoch 20  train_loss=nan  val_loss=nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 280\u001b[39m\n\u001b[32m    278\u001b[39m             preds.append(yhat); trues.append(yt)\n\u001b[32m    279\u001b[39m     preds = np.concatenate(preds); trues = np.concatenate(trues)\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[38;5;28mprint\u001b[39m({\u001b[33m\"\u001b[39m\u001b[33mholdout_MAE\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(\u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m)})\n\u001b[32m    282\u001b[39m \u001b[38;5;66;03m# ---- predict requested DELIVERY_DATE ----\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# find base row (previous day EOD with price)\u001b[39;00m\n\u001b[32m    284\u001b[39m prev_day = DELIVERY_DATE - dt.timedelta(days=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:284\u001b[39m, in \u001b[36mmean_absolute_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[32m    229\u001b[39m \n\u001b[32m    230\u001b[39m \u001b[33;03mThe mean absolute error is a non-negative floating point value, where best value\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    279\u001b[39m \u001b[33;03m0.85...\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    281\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    283\u001b[39m _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m )\n\u001b[32m    289\u001b[39m output_errors = _average(\n\u001b[32m    290\u001b[39m     xp.abs(y_pred - y_true), weights=sample_weight, axis=\u001b[32m0\u001b[39m, xp=xp\n\u001b[32m    291\u001b[39m )\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:209\u001b[39m, in \u001b[36m_check_reg_targets_with_floating_dtype\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Ensures y_true, y_pred, and sample_weight correspond to same regression task.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \u001b[33;03mExtends `_check_reg_targets` by automatically selecting a suitable floating-point\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    205\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    207\u001b[39m dtype_name = _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m y_type, y_true, y_pred, sample_weight, multioutput = \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y_type, y_true, y_pred, sample_weight, multioutput\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:116\u001b[39m, in \u001b[36m_check_reg_targets\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, dtype, xp)\u001b[39m\n\u001b[32m    114\u001b[39m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[32m    115\u001b[39m y_true = check_array(y_true, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m y_pred = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    118\u001b[39m     sample_weight = _check_sample_weight(sample_weight, y_true, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# ==== PyTorch seq2seq with robust preprocessing (median impute + safe standardize) ====\n",
    "import duckdb, pandas as pd, numpy as np, datetime as dt, math\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()   # change target date if needed\n",
    "W_PAST = 168\n",
    "H_FUT = 24\n",
    "HOLDOUT_DAYS = 60\n",
    "HIDDEN = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def fit_standardizer(X2d):\n",
    "    \"\"\"Return (mean,std) with std guarded (==0 -> 1). X2d: (N, F)\"\"\"\n",
    "    mu = np.nanmean(X2d, axis=0)\n",
    "    sd = np.nanstd(X2d, axis=0)\n",
    "    sd = np.where(sd <= 1e-8, 1.0, sd)\n",
    "    return mu, sd\n",
    "\n",
    "def apply_standardizer(X, mu, sd):\n",
    "    return (X - mu) / sd\n",
    "\n",
    "def col_median(X2d):\n",
    "    \"\"\"Median per feature, ignoring NaNs\"\"\"\n",
    "    return np.nanmedian(X2d, axis=0)\n",
    "\n",
    "# ---------- ensure enhanced views exist (already fixed earlier); skip if you ran that cell ----------\n",
    "\n",
    "# ---------- pull data ----------\n",
    "con = duckdb.connect(DB)\n",
    "df_lags = con.execute(\"\"\"\n",
    "SELECT\n",
    "  ts, OperatingDTM, Interval, hb_houston,\n",
    "  p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "  dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "FROM vw_master_spine_lags\n",
    "ORDER BY ts\n",
    "\"\"\").df()\n",
    "df_lags[\"ts\"] = pd.to_datetime(df_lags[\"ts\"])\n",
    "\n",
    "df_future = con.execute(\"\"\"\n",
    "SELECT\n",
    "  f.OperatingDTM AS delivery_date,\n",
    "  f.Interval     AS delivery_interval,\n",
    "  f.hb_houston   AS target,\n",
    "  -- base future features\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.temp_the_woodlands_tx, f.hum_the_woodlands_tx, f.wind_the_woodlands_tx, f.precip_the_woodlands_tx,\n",
    "  f.temp_katy_tx, f.hum_katy_tx, f.wind_katy_tx, f.precip_katy_tx,\n",
    "  f.temp_friendswood_tx, f.hum_friendswood_tx, f.wind_friendswood_tx, f.precip_friendswood_tx,\n",
    "  f.temp_baytown_tx, f.hum_baytown_tx, f.wind_baytown_tx, f.precip_baytown_tx,\n",
    "  f.temp_houston_tx, f.hum_houston_tx, f.wind_houston_tx, f.precip_houston_tx,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "  f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f,\n",
    "  f.cal_sin_dow  AS cal_sin_dow_f,  f.cal_cos_dow  AS cal_cos_dow_f,\n",
    "  -- enhanced joins\n",
    "  l.net_wz, l.net_lz, l.net_wz_ramp1, l.net_lz_ramp1, l.lz_houston_ramp1, l.wz_spread, l.lz_spread,\n",
    "  w.temp_avg, w.temp_spread, w.temp_avg_ramp1, w.hum_avg, w.hum_spread, w.hum_avg_ramp1\n",
    "FROM vw_master_spine_ts f\n",
    "LEFT JOIN load_fcst_enh l\n",
    "  ON l.OperatingDTM = f.OperatingDTM AND l.Interval = f.Interval\n",
    "LEFT JOIN wx_fcst_enh w\n",
    "  ON w.OperatingDTM = f.OperatingDTM AND w.Interval = f.Interval\n",
    "ORDER BY f.OperatingDTM, f.Interval\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "df_future[\"delivery_date\"] = pd.to_datetime(df_future[\"delivery_date\"]).dt.date\n",
    "df_future[\"delivery_interval\"] = df_future[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# ---------- assemble samples (day-ahead) ----------\n",
    "enc_cols = [\n",
    "    \"hb_houston\",\"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"\n",
    "]\n",
    "dec_future_cols = [\n",
    "    \"cal_hour_f\",\"cal_dow_f\",\"cal_is_weekend_f\",\"cal_sin_hour_f\",\"cal_cos_hour_f\",\"cal_sin_dow_f\",\"cal_cos_dow_f\",\n",
    "    \"net_wz\",\"net_lz\",\"net_wz_ramp1\",\"net_lz_ramp1\",\"lz_houston_ramp1\",\"wz_spread\",\"lz_spread\",\n",
    "    \"temp_avg\",\"temp_spread\",\"temp_avg_ramp1\",\"hum_avg\",\"hum_spread\",\"hum_avg_ramp1\"\n",
    "]\n",
    "\n",
    "# base rows = end-of-day with price\n",
    "base_rows = df_lags[(df_lags[\"hb_houston\"].notna()) & (df_lags[\"Interval\"]==24)][[\"ts\",\"OperatingDTM\",\"hb_houston\"]].copy()\n",
    "base_rows[\"base_date\"] = pd.to_datetime(base_rows[\"OperatingDTM\"]).dt.date\n",
    "base_rows[\"delivery_date\"] = base_rows[\"base_date\"] + dt.timedelta(days=1)\n",
    "\n",
    "# only use delivery days with 24 actuals for training/val\n",
    "has24 = df_future.groupby(\"delivery_date\")[\"target\"].apply(lambda s: s.notna().sum()==24)\n",
    "valid_delivery_dates = set(has24[has24].index)\n",
    "\n",
    "df_lags_idx = df_lags.set_index(\"ts\").sort_index()\n",
    "\n",
    "samples = []\n",
    "for _, r in base_rows.iterrows():\n",
    "    ddate = r[\"delivery_date\"]\n",
    "    ts0 = r[\"ts\"]\n",
    "    # encoder window\n",
    "    start = ts0 - pd.Timedelta(hours=W_PAST-1)\n",
    "    enc_df = df_lags_idx.loc[start:ts0][enc_cols]\n",
    "    if enc_df.shape[0] != W_PAST: \n",
    "        continue\n",
    "    if enc_df.isna().any().any():\n",
    "        continue  # strict: skip any encoder NaNs\n",
    "    # decoder future rows\n",
    "    fdf = df_future[(df_future[\"delivery_date\"]==ddate)].sort_values(\"delivery_interval\")\n",
    "    if fdf.shape[0] != H_FUT:\n",
    "        continue\n",
    "    X_enc = enc_df.values.astype(np.float32)\n",
    "    X_dec = fdf[dec_future_cols].values.astype(np.float32)\n",
    "    y     = fdf[\"target\"].values.astype(np.float32)\n",
    "    y0    = np.float32(r[\"hb_houston\"])\n",
    "    samples.append((X_enc, X_dec, y, y0, ddate))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples assembled; check data coverage or reduce W_PAST.\")\n",
    "\n",
    "# split by date\n",
    "dates = sorted({s[4] for s in samples})\n",
    "cut_date = dates[-1] - dt.timedelta(days=HOLDOUT_DAYS) if len(dates)>HOLDOUT_DAYS else dates[0]\n",
    "train_idx = [i for i,s in enumerate(samples) if s[4] <= cut_date and s[4] in valid_delivery_dates]\n",
    "val_idx   = [i for i,s in enumerate(samples) if s[4] >  cut_date and s[4] in valid_delivery_dates]\n",
    "\n",
    "def stack(idxs):\n",
    "    Xe = np.stack([samples[i][0] for i in idxs], axis=0)\n",
    "    Xd = np.stack([samples[i][1] for i in idxs], axis=0)\n",
    "    Y  = np.stack([samples[i][2] for i in idxs], axis=0)\n",
    "    Y0 = np.stack([samples[i][3] for i in idxs], axis=0)\n",
    "    return Xe, Xd, Y, Y0\n",
    "\n",
    "Xe_tr, Xd_tr, Y_tr, Y0_tr = stack(train_idx)\n",
    "Xe_va, Xd_va, Y_va, Y0_va = (None, None, None, None)\n",
    "if len(val_idx) > 0:\n",
    "    Xe_va, Xd_va, Y_va, Y0_va = stack(val_idx)\n",
    "\n",
    "# ---------- impute decoder NaNs with training medians ----------\n",
    "# (encoder already filtered for NaNs above; still guard just in case)\n",
    "enc_med = np.nanmedian(Xe_tr.reshape(-1, Xe_tr.shape[2]), axis=0)\n",
    "dec_med = np.nanmedian(Xd_tr.reshape(-1, Xd_tr.shape[2]), axis=0)\n",
    "\n",
    "def impute_inplace(X, med):\n",
    "    m = np.isnan(X)\n",
    "    if m.any():\n",
    "        X[m] = np.take(med, np.where(m)[2])\n",
    "\n",
    "impute_inplace(Xe_tr, enc_med)\n",
    "impute_inplace(Xd_tr, dec_med)\n",
    "if Xe_va is not None:\n",
    "    impute_inplace(Xe_va, enc_med)\n",
    "    impute_inplace(Xd_va, dec_med)\n",
    "\n",
    "# Targets should be complete for training/val; no imputation for y\n",
    "\n",
    "# ---------- safe standardization ----------\n",
    "enc_mu, enc_sd = fit_standardizer(Xe_tr.reshape(-1, Xe_tr.shape[2]))\n",
    "dec_mu, dec_sd = fit_standardizer(Xd_tr.reshape(-1, Xd_tr.shape[2]))\n",
    "y_mu,  y_sd    = fit_standardizer(Y_tr.reshape(-1,1))\n",
    "\n",
    "Xe_tr_n = apply_standardizer(Xe_tr, enc_mu, enc_sd)\n",
    "Xd_tr_n = apply_standardizer(Xd_tr, dec_mu, dec_sd)\n",
    "Y_tr_n  = apply_standardizer(Y_tr,  y_mu,  y_sd).reshape(Y_tr.shape)\n",
    "Y0_tr_n = apply_standardizer(Y0_tr.reshape(-1,1), y_mu, y_sd).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = apply_standardizer(Xe_va, enc_mu, enc_sd)\n",
    "    Xd_va_n = apply_standardizer(Xd_va, dec_mu, dec_sd)\n",
    "    Y_va_n  = apply_standardizer(Y_va,  y_mu,  y_sd).reshape(Y_va.shape)\n",
    "    Y0_va_n = apply_standardizer(Y0_va.reshape(-1,1), y_mu, y_sd).reshape(-1)\n",
    "\n",
    "# ---------- torch datasets ----------\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xe, Xd, Y, Y0):\n",
    "        self.Xe = torch.from_numpy(Xe).float()\n",
    "        self.Xd = torch.from_numpy(Xd).float()\n",
    "        self.Y  = torch.from_numpy(Y).float()\n",
    "        self.Y0 = torch.from_numpy(Y0).float()\n",
    "    def __len__(self): return self.Xe.shape[0]\n",
    "    def __getitem__(self, i): return self.Xe[i], self.Xd[i], self.Y[i], self.Y0[i]\n",
    "\n",
    "bs = 32\n",
    "train_loader = torch.utils.data.DataLoader(Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=bs, shuffle=True)\n",
    "val_loader = None if Xe_va is None else torch.utils.data.DataLoader(Seq2SeqDataset(Xe_va_n, Xd_va_n, Y_va_n, Y0_va_n), batch_size=bs, shuffle=False)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim, hidden, batch_first=True)\n",
    "    def forward(self, x):  # x: (B,W,E)\n",
    "        _, h = self.rnn(x)\n",
    "        return h  # (1,B,H)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim + 1, hidden, batch_first=True)  # + prev y\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "    def forward(self, future_feats, h0, y0, teacher=None, tf_prob=0.5):\n",
    "        B, T, D = future_feats.shape\n",
    "        y_prev = y0.view(B,1)\n",
    "        h = h0\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            x_t = torch.cat([future_feats[:,t,:], y_prev], dim=1).unsqueeze(1)  # (B,1,D+1)\n",
    "            o, h = self.rnn(x_t, h)\n",
    "            y_t = self.out(o[:, -1, :]).squeeze(1)\n",
    "            outs.append(y_t)\n",
    "            if (teacher is not None) and (rng.random() < tf_prob):\n",
    "                y_prev = teacher[:,t].view(B,1)\n",
    "            else:\n",
    "                y_prev = y_t.view(B,1)\n",
    "        return torch.stack(outs, dim=1)  # (B,24)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(enc_in, hidden)\n",
    "        self.dec = Decoder(dec_in, hidden)\n",
    "    def forward(self, x_enc, x_dec, y0, y_teacher=None, tf_prob=0.5):\n",
    "        h = self.enc(x_enc)\n",
    "        return self.dec(x_dec, h, y0, y_teacher, tf_prob)\n",
    "\n",
    "enc_in = Xe_tr_n.shape[2]\n",
    "dec_in = Xd_tr_n.shape[2]\n",
    "model = Seq2Seq(enc_in, dec_in, HIDDEN).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in train_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=y, tf_prob=0.5)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    if val_loader is None: return None\n",
    "    model.eval()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(val_loader.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    print(f\"Epoch {ep:02d}  train_loss={tr:.4f}\" + (\"\" if va is None else f\"  val_loss={va:.4f}\"))\n",
    "\n",
    "# ---- evaluate holdout MAE in real units ----\n",
    "if val_loader is not None:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xenc, xdec, y, y0 in val_loader:\n",
    "            xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "            yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "            y_n    = y.detach().cpu().numpy()\n",
    "            # invert scaling\n",
    "            yhat = (yhat_n * y_sd + y_mu).reshape(-1)\n",
    "            yt   = (y_n    * y_sd + y_mu).reshape(-1)\n",
    "            preds.append(yhat); trues.append(yt)\n",
    "    preds = np.concatenate(preds); trues = np.concatenate(trues)\n",
    "    print({\"holdout_MAE\": float(mean_absolute_error(trues, preds))})\n",
    "\n",
    "# ---- predict requested DELIVERY_DATE ----\n",
    "# find base row (previous day EOD with price)\n",
    "prev_day = DELIVERY_DATE - dt.timedelta(days=1)\n",
    "base_row = df_lags[(pd.to_datetime(df_lags[\"OperatingDTM\"]).dt.date == prev_day) & (df_lags[\"Interval\"]==24) & df_lags[\"hb_houston\"].notna()]\n",
    "if base_row.empty:\n",
    "    raise RuntimeError(f\"No base EOD price for {prev_day}\")\n",
    "ts0 = pd.to_datetime(base_row.iloc[0][\"ts\"])\n",
    "y0  = np.float32(base_row.iloc[0][\"hb_houston\"])\n",
    "\n",
    "# encoder window\n",
    "win = df_lags.set_index(\"ts\").loc[ts0 - pd.Timedelta(hours=W_PAST-1): ts0][enc_cols]\n",
    "if win.shape[0] != W_PAST or win.isna().any().any():\n",
    "    raise RuntimeError(\"Insufficient/NaN history for encoder window.\")\n",
    "Xe_pred = win.values.astype(np.float32)[None, ...]\n",
    "\n",
    "# decoder future features (24 rows)\n",
    "frows = df_future[(df_future[\"delivery_date\"]==DELIVERY_DATE)].sort_values(\"delivery_interval\")\n",
    "if frows.shape[0] != H_FUT:\n",
    "    raise RuntimeError(\"Spine missing 24 rows for delivery date.\")\n",
    "Xd_pred = frows[dec_future_cols].values.astype(np.float32)[None, ...]\n",
    "\n",
    "# impute + standardize pred\n",
    "# impute decoder NaNs\n",
    "m = np.isnan(Xd_pred)\n",
    "if m.any():\n",
    "    Xd_pred[m] = np.take(dec_med, np.where(m)[2])\n",
    "Xe_pred_n = apply_standardizer(Xe_pred, enc_mu, enc_sd)\n",
    "Xd_pred_n = apply_standardizer(Xd_pred, dec_mu, dec_sd)\n",
    "y0_n = ((y0 - y_mu) / y_sd).reshape(()).astype(np.float32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_n = model(torch.from_numpy(Xe_pred_n).to(DEVICE),\n",
    "                   torch.from_numpy(Xd_pred_n).to(DEVICE),\n",
    "                   torch.tensor([y0_n], dtype=torch.float32, device=DEVICE),\n",
    "                   y_teacher=None, tf_prob=0.0).detach().cpu().numpy()[0]\n",
    "yhat = (yhat_n * y_sd + y_mu).reshape(-1)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*H_FUT,\n",
    "    \"Interval\": list(range(1, H_FUT+1)),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "})\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d491c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1216: RuntimeWarning: All-NaN slice encountered\n",
      "  return fnb._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "C:\\Users\\db\\AppData\\Local\\Temp\\ipykernel_6672\\3780957588.py:21: RuntimeWarning: Mean of empty slice\n",
      "  mu = np.nanmean(X2d, axis=0)\n",
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:2015: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Xd_tr_n has 122112 non-finite values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 183\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# hard sanity checks\u001b[39;00m\n\u001b[32m    182\u001b[39m assert_finite(\u001b[33m\"\u001b[39m\u001b[33mXe_tr_n\u001b[39m\u001b[33m\"\u001b[39m, Xe_tr_n)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43massert_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mXd_tr_n\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXd_tr_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m assert_finite(\u001b[33m\"\u001b[39m\u001b[33mY_tr_n\u001b[39m\u001b[33m\"\u001b[39m,  Y_tr_n)\n\u001b[32m    185\u001b[39m assert_finite(\u001b[33m\"\u001b[39m\u001b[33mY0_tr_n\u001b[39m\u001b[33m\"\u001b[39m, Y0_tr_n)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36massert_finite\u001b[39m\u001b[34m(name, arr)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(arr).all():\n\u001b[32m     40\u001b[39m     bad = \u001b[38;5;28mint\u001b[39m(np.isnan(arr).sum() + np.isinf(arr).sum())\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbad\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m non-finite values\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Xd_tr_n has 122112 non-finite values"
     ]
    }
   ],
   "source": [
    "# ==== PyTorch seq2seq with ROBUST preprocessing (median impute + safe standardize + safe MAE) ====\n",
    "import duckdb, pandas as pd, numpy as np, datetime as dt, math\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()   # change target date if needed\n",
    "W_PAST = 168\n",
    "H_FUT = 24\n",
    "HOLDOUT_DAYS = 60\n",
    "HIDDEN = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def fit_standardizer(X2d):\n",
    "    \"\"\"Return (mean,std) with std guarded (==0 -> 1). X2d: (N, F)\"\"\"\n",
    "    mu = np.nanmean(X2d, axis=0)\n",
    "    sd = np.nanstd(X2d, axis=0)\n",
    "    sd = np.where(sd <= 1e-8, 1.0, sd)\n",
    "    return mu, sd\n",
    "\n",
    "def apply_standardizer(X, mu, sd):\n",
    "    return (X - mu) / sd\n",
    "\n",
    "# robust 3D imputer\n",
    "def impute_inplace_3d(X, med_vec):\n",
    "    \"\"\"X: (N, T, F) or (N, W, F); med_vec: (F,). Fills NaNs in-place with per-feature medians.\"\"\"\n",
    "    mask = np.isnan(X)\n",
    "    if mask.any():\n",
    "        med_broadcast = np.broadcast_to(med_vec, X.shape)\n",
    "        X[mask] = med_broadcast[mask]\n",
    "\n",
    "# sanity checker\n",
    "def assert_finite(name, arr):\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = int(np.isnan(arr).sum() + np.isinf(arr).sum())\n",
    "        raise RuntimeError(f\"{name} has {bad} non-finite values\")\n",
    "\n",
    "# ---------- pull data ----------\n",
    "con = duckdb.connect(DB)\n",
    "df_lags = con.execute(\"\"\"\n",
    "SELECT\n",
    "  ts, OperatingDTM, Interval, hb_houston,\n",
    "  p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "  dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "FROM vw_master_spine_lags\n",
    "ORDER BY ts\n",
    "\"\"\").df()\n",
    "df_lags[\"ts\"] = pd.to_datetime(df_lags[\"ts\"])\n",
    "\n",
    "df_future = con.execute(\"\"\"\n",
    "SELECT\n",
    "  f.OperatingDTM AS delivery_date,\n",
    "  f.Interval     AS delivery_interval,\n",
    "  f.hb_houston   AS target,\n",
    "  -- base future features\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.temp_the_woodlands_tx, f.hum_the_woodlands_tx, f.wind_the_woodlands_tx, f.precip_the_woodlands_tx,\n",
    "  f.temp_katy_tx, f.hum_katy_tx, f.wind_katy_tx, f.precip_katy_tx,\n",
    "  f.temp_friendswood_tx, f.hum_friendswood_tx, f.wind_friendswood_tx, f.precip_friendswood_tx,\n",
    "  f.temp_baytown_tx, f.hum_baytown_tx, f.wind_baytown_tx, f.precip_baytown_tx,\n",
    "  f.temp_houston_tx, f.hum_houston_tx, f.wind_houston_tx, f.precip_houston_tx,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "  f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f,\n",
    "  f.cal_sin_dow  AS cal_sin_dow_f,  f.cal_cos_dow  AS cal_cos_dow_f,\n",
    "  -- enhanced joins\n",
    "  l.net_wz, l.net_lz, l.net_wz_ramp1, l.net_lz_ramp1, l.lz_houston_ramp1, l.wz_spread, l.lz_spread,\n",
    "  w.temp_avg, w.temp_spread, w.temp_avg_ramp1, w.hum_avg, w.hum_spread, w.hum_avg_ramp1\n",
    "FROM vw_master_spine_ts f\n",
    "LEFT JOIN load_fcst_enh l\n",
    "  ON l.OperatingDTM = f.OperatingDTM AND l.Interval = f.Interval\n",
    "LEFT JOIN wx_fcst_enh w\n",
    "  ON w.OperatingDTM = f.OperatingDTM AND w.Interval = f.Interval\n",
    "ORDER BY f.OperatingDTM, f.Interval\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "df_future[\"delivery_date\"] = pd.to_datetime(df_future[\"delivery_date\"]).dt.date\n",
    "df_future[\"delivery_interval\"] = df_future[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# ---------- assemble samples (day-ahead) ----------\n",
    "enc_cols = [\n",
    "    \"hb_houston\",\"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"\n",
    "]\n",
    "dec_future_cols = [\n",
    "    \"cal_hour_f\",\"cal_dow_f\",\"cal_is_weekend_f\",\"cal_sin_hour_f\",\"cal_cos_hour_f\",\"cal_sin_dow_f\",\"cal_cos_dow_f\",\n",
    "    \"net_wz\",\"net_lz\",\"net_wz_ramp1\",\"net_lz_ramp1\",\"lz_houston_ramp1\",\"wz_spread\",\"lz_spread\",\n",
    "    \"temp_avg\",\"temp_spread\",\"temp_avg_ramp1\",\"hum_avg\",\"hum_spread\",\"hum_avg_ramp1\"\n",
    "]\n",
    "\n",
    "# base rows = end-of-day with price\n",
    "base_rows = df_lags[(df_lags[\"hb_houston\"].notna()) & (df_lags[\"Interval\"]==24)][[\"ts\",\"OperatingDTM\",\"hb_houston\"]].copy()\n",
    "base_rows[\"base_date\"] = pd.to_datetime(base_rows[\"OperatingDTM\"]).dt.date\n",
    "base_rows[\"delivery_date\"] = base_rows[\"base_date\"] + dt.timedelta(days=1)\n",
    "\n",
    "# only use delivery days with 24 actuals for training/val\n",
    "has24 = df_future.groupby(\"delivery_date\")[\"target\"].apply(lambda s: s.notna().sum()==24)\n",
    "valid_delivery_dates = set(has24[has24].index)\n",
    "\n",
    "df_lags_idx = df_lags.set_index(\"ts\").sortindex() if hasattr(pd.DataFrame, 'sortindex') else df_lags.set_index(\"ts\").sort_index()\n",
    "\n",
    "samples = []\n",
    "for _, r in base_rows.iterrows():\n",
    "    ddate = r[\"delivery_date\"]\n",
    "    ts0 = r[\"ts\"]\n",
    "    # encoder window\n",
    "    start = ts0 - pd.Timedelta(hours=W_PAST-1)\n",
    "    enc_df = df_lags_idx.loc[start:ts0][enc_cols]\n",
    "    if enc_df.shape[0] != W_PAST: \n",
    "        continue\n",
    "    if enc_df.isna().any().any():\n",
    "        continue  # strict: skip any encoder NaNs\n",
    "    # decoder future rows\n",
    "    fdf = df_future[(df_future[\"delivery_date\"]==ddate)].sort_values(\"delivery_interval\")\n",
    "    if fdf.shape[0] != H_FUT:\n",
    "        continue\n",
    "    X_enc = enc_df.values.astype(np.float32)\n",
    "    X_dec = fdf[dec_future_cols].values.astype(np.float32)\n",
    "    y     = fdf[\"target\"].values.astype(np.float32)\n",
    "    y0    = np.float32(r[\"hb_houston\"])\n",
    "    samples.append((X_enc, X_dec, y, y0, ddate))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples assembled; check data coverage or reduce W_PAST.\")\n",
    "\n",
    "# split by date\n",
    "dates = sorted({s[4] for s in samples})\n",
    "cut_date = dates[-1] - dt.timedelta(days=HOLDOUT_DAYS) if len(dates)>HOLDOUT_DAYS else dates[0]\n",
    "train_idx = [i for i,s in enumerate(samples) if s[4] <= cut_date and s[4] in valid_delivery_dates]\n",
    "val_idx   = [i for i,s in enumerate(samples) if s[4] >  cut_date and s[4] in valid_delivery_dates]\n",
    "\n",
    "def stack(idxs):\n",
    "    Xe = np.stack([samples[i][0] for i in idxs], axis=0)\n",
    "    Xd = np.stack([samples[i][1] for i in idxs], axis=0)\n",
    "    Y  = np.stack([samples[i][2] for i in idxs], axis=0)\n",
    "    Y0 = np.stack([samples[i][3] for i in idxs], axis=0)\n",
    "    return Xe, Xd, Y, Y0\n",
    "\n",
    "Xe_tr, Xd_tr, Y_tr, Y0_tr = stack(train_idx)\n",
    "Xe_va, Xd_va, Y_va, Y0_va = (None, None, None, None)\n",
    "if len(val_idx) > 0:\n",
    "    Xe_va, Xd_va, Y_va, Y0_va = stack(val_idx)\n",
    "\n",
    "# ---------- impute NaNs (robust) ----------\n",
    "enc_med = np.nanmedian(Xe_tr.reshape(-1, Xe_tr.shape[2]), axis=0)\n",
    "dec_med = np.nanmedian(Xd_tr.reshape(-1, Xd_tr.shape[2]), axis=0)\n",
    "\n",
    "impute_inplace_3d(Xe_tr, enc_med)\n",
    "impute_inplace_3d(Xd_tr, dec_med)\n",
    "if Xe_va is not None:\n",
    "    impute_inplace_3d(Xe_va, enc_med)\n",
    "    impute_inplace_3d(Xd_va, dec_med)\n",
    "\n",
    "# Targets should be complete for training/val; assert\n",
    "assert np.isfinite(Y_tr).all(), \"Y_tr has non-finite values\"\n",
    "if Y_va is not None:\n",
    "    assert np.isfinite(Y_va).all(), \"Y_va has non-finite values\"\n",
    "\n",
    "# ---------- safe standardization ----------\n",
    "enc_mu, enc_sd = fit_standardizer(Xe_tr.reshape(-1, Xe_tr.shape[2]))\n",
    "dec_mu, dec_sd = fit_standardizer(Xd_tr.reshape(-1, Xd_tr.shape[2]))\n",
    "y_mu,  y_sd    = fit_standardizer(Y_tr.reshape(-1,1))\n",
    "\n",
    "Xe_tr_n = apply_standardizer(Xe_tr, enc_mu, enc_sd)\n",
    "Xd_tr_n = apply_standardizer(Xd_tr, dec_mu, dec_sd)\n",
    "Y_tr_n  = apply_standardizer(Y_tr,  y_mu,  y_sd).reshape(Y_tr.shape)\n",
    "Y0_tr_n = apply_standardizer(Y0_tr.reshape(-1,1), y_mu, y_sd).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = apply_standardizer(Xe_va, enc_mu, enc_sd)\n",
    "    Xd_va_n = apply_standardizer(Xd_va, dec_mu, dec_sd)\n",
    "    Y_va_n  = apply_standardizer(Y_va,  y_mu,  y_sd).reshape(Y_va.shape)\n",
    "    Y0_va_n = apply_standardizer(Y0_va.reshape(-1,1), y_mu, y_sd).reshape(-1)\n",
    "\n",
    "# hard sanity checks\n",
    "assert_finite(\"Xe_tr_n\", Xe_tr_n)\n",
    "assert_finite(\"Xd_tr_n\", Xd_tr_n)\n",
    "assert_finite(\"Y_tr_n\",  Y_tr_n)\n",
    "assert_finite(\"Y0_tr_n\", Y0_tr_n)\n",
    "if Xe_va is not None:\n",
    "    assert_finite(\"Xe_va_n\", Xe_va_n)\n",
    "    assert_finite(\"Xd_va_n\", Xd_va_n)\n",
    "    assert_finite(\"Y_va_n\",  Y_va_n)\n",
    "    assert_finite(\"Y0_va_n\", Y0_va_n)\n",
    "\n",
    "# ---------- torch datasets ----------\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xe, Xd, Y, Y0):\n",
    "        self.Xe = torch.from_numpy(Xe).float()\n",
    "        self.Xd = torch.from_numpy(Xd).float()\n",
    "        self.Y  = torch.from_numpy(Y).float()\n",
    "        self.Y0 = torch.from_numpy(Y0).float()\n",
    "    def __len__(self): return self.Xe.shape[0]\n",
    "    def __getitem__(self, i): return self.Xe[i], self.Xd[i], self.Y[i], self.Y0[i]\n",
    "\n",
    "bs = 32\n",
    "train_loader = torch.utils.data.DataLoader(Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=bs, shuffle=True)\n",
    "val_loader = None if Xe_va is None else torch.utils.data.DataLoader(Seq2SeqDataset(Xe_va_n, Xd_va_n, Y_va_n, Y0_va_n), batch_size=bs, shuffle=False)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim, hidden, batch_first=True)\n",
    "    def forward(self, x):  # x: (B,W,E)\n",
    "        _, h = self.rnn(x)\n",
    "        return h  # (1,B,H)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim + 1, hidden, batch_first=True)  # + prev y\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "    def forward(self, future_feats, h0, y0, teacher=None, tf_prob=0.5):\n",
    "        B, T, D = future_feats.shape\n",
    "        y_prev = y0.view(B,1)\n",
    "        h = h0\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            x_t = torch.cat([future_feats[:,t,:], y_prev], dim=1).unsqueeze(1)  # (B,1,D+1)\n",
    "            o, h = self.rnn(x_t, h)\n",
    "            y_t = self.out(o[:, -1, :]).squeeze(1)\n",
    "            outs.append(y_t)\n",
    "            if (teacher is not None) and (rng.random() < tf_prob):\n",
    "                y_prev = teacher[:,t].view(B,1)\n",
    "            else:\n",
    "                y_prev = y_t.view(B,1)\n",
    "        return torch.stack(outs, dim=1)  # (B,24)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(enc_in, hidden)\n",
    "        self.dec = Decoder(dec_in, hidden)\n",
    "    def forward(self, x_enc, x_dec, y0, y_teacher=None, tf_prob=0.5):\n",
    "        h = self.enc(x_enc)\n",
    "        return self.dec(x_dec, h, y0, y_teacher, tf_prob)\n",
    "\n",
    "enc_in = Xe_tr_n.shape[2]\n",
    "dec_in = Xd_tr_n.shape[2]\n",
    "model = Seq2Seq(enc_in, dec_in, HIDDEN).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in train_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=y, tf_prob=0.5)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    if val_loader is None: return None\n",
    "    model.eval()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(val_loader.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    print(f\"Epoch {ep:02d}  train_loss={tr:.4f}\" + (\"\" if va is None else f\"  val_loss={va:.4f}\"))\n",
    "\n",
    "# ---- evaluate holdout MAE in real units (SAFE) ----\n",
    "if val_loader is not None:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xenc, xdec, y, y0 in val_loader:\n",
    "            xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "            yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "            y_n    = y.detach().cpu().numpy()\n",
    "            # invert scaling\n",
    "            yhat = (yhat_n * y_sd + y_mu)\n",
    "            yt   = (y_n    * y_sd + y_mu)\n",
    "            preds.append(yhat.reshape(-1))\n",
    "            trues.append(yt.reshape(-1))\n",
    "    preds = np.concatenate(preds)\n",
    "    trues = np.concatenate(trues)\n",
    "    mask = np.isfinite(preds) & np.isfinite(trues)\n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped:\n",
    "        print(f\"Dropped {dropped} non-finite pairs from holdout scoring.\")\n",
    "    print({\"holdout_MAE\": float(mean_absolute_error(trues[mask], preds[mask]))})\n",
    "\n",
    "# ---- predict requested DELIVERY_DATE ----\n",
    "# find base row (previous day EOD with price)\n",
    "prev_day = DELIVERY_DATE - dt.timedelta(days=1)\n",
    "base_row = df_lags[(pd.to_datetime(df_lags[\"OperatingDTM\"]).dt.date == prev_day) & (df_lags[\"Interval\"]==24) & df_lags[\"hb_houston\"].notna()]\n",
    "if base_row.empty:\n",
    "    raise RuntimeError(f\"No base EOD price for {prev_day}\")\n",
    "ts0 = pd.to_datetime(base_row.iloc[0][\"ts\"])\n",
    "y0  = np.float32(base_row.iloc[0][\"hb_houston\"])\n",
    "\n",
    "# encoder window\n",
    "win = df_lags.set_index(\"ts\").loc[ts0 - pd.Timedelta(hours=W_PAST-1): ts0][enc_cols]\n",
    "if win.shape[0] != W_PAST or win.isna().any().any():\n",
    "    raise RuntimeError(\"Insufficient/NaN history for encoder window.\")\n",
    "Xe_pred = win.values.astype(np.float32)[None, ...]\n",
    "\n",
    "# decoder future features (24 rows)\n",
    "frows = df_future[(df_future[\"delivery_date\"]==DELIVERY_DATE)].sort_values(\"delivery_interval\")\n",
    "if frows.shape[0] != H_FUT:\n",
    "    raise RuntimeError(\"Spine missing 24 rows for delivery date.\")\n",
    "Xd_pred = frows[dec_future_cols].values.astype(np.float32)[None, ...]\n",
    "\n",
    "# impute + standardize pred\n",
    "m = np.isnan(Xd_pred)\n",
    "if m.any():\n",
    "    # dec_med computed on train; reuse here\n",
    "    Xd_pred[m] = np.broadcast_to(dec_med, Xd_pred.shape)[m]\n",
    "Xe_pred_n = apply_standardizer(Xe_pred, enc_mu, enc_sd)\n",
    "Xd_pred_n = apply_standardizer(Xd_pred, dec_mu, dec_sd)\n",
    "y0_n = ((y0 - y_mu) / y_sd).reshape(()).astype(np.float32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_n = model(torch.from_numpy(Xe_pred_n).to(DEVICE),\n",
    "                   torch.from_numpy(Xd_pred_n).to(DEVICE),\n",
    "                   torch.tensor([y0_n], dtype=torch.float32, device=DEVICE),\n",
    "                   y_teacher=None, tf_prob=0.0).detach().cpu().numpy()[0]\n",
    "yhat = (yhat_n * y_sd + y_mu).reshape(-1)\n",
    "# replace any residual NaN with the day's median\n",
    "if np.isnan(yhat).any():\n",
    "    day_med = float(np.nanmedian(yhat))\n",
    "    yhat = np.nan_to_num(yhat, nan=day_med)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*H_FUT,\n",
    "    \"Interval\": list(range(1, H_FUT+1)),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "})\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d94fa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 6 decoder feature(s) with all-NaN in TRAIN: ['temp_avg', 'temp_spread', 'temp_avg_ramp1', 'hum_avg', 'hum_spread', 'hum_avg_ramp1']\n",
      "Epoch 01  train_loss=0.0845  val_loss=0.0419\n",
      "Epoch 02  train_loss=0.0758  val_loss=0.0196\n",
      "Epoch 03  train_loss=0.0704  val_loss=0.0355\n",
      "Epoch 04  train_loss=0.0656  val_loss=0.0257\n",
      "Epoch 05  train_loss=0.0615  val_loss=0.0164\n",
      "Epoch 06  train_loss=0.0610  val_loss=0.2036\n",
      "Epoch 07  train_loss=0.0598  val_loss=0.0212\n",
      "Epoch 08  train_loss=0.0539  val_loss=0.1410\n",
      "Epoch 09  train_loss=0.0515  val_loss=0.0525\n",
      "Epoch 10  train_loss=0.0513  val_loss=0.0224\n",
      "Epoch 11  train_loss=0.0505  val_loss=0.0124\n",
      "Epoch 12  train_loss=0.0496  val_loss=0.0114\n",
      "Epoch 13  train_loss=0.0513  val_loss=0.0164\n",
      "Epoch 14  train_loss=0.0484  val_loss=0.0120\n",
      "Epoch 15  train_loss=0.0462  val_loss=0.0585\n",
      "Epoch 16  train_loss=0.0439  val_loss=0.0155\n",
      "Epoch 17  train_loss=0.0452  val_loss=0.0752\n",
      "Epoch 18  train_loss=0.0440  val_loss=0.0272\n",
      "Epoch 19  train_loss=0.0417  val_loss=0.0271\n",
      "Epoch 20  train_loss=0.0401  val_loss=0.0236\n",
      "{'holdout_MAE': 18.966270446777344}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 370\u001b[39m\n\u001b[32m    366\u001b[39m model.eval()\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    368\u001b[39m     yhat_n = model(torch.from_numpy(Xe_pred_n).to(DEVICE),\n\u001b[32m    369\u001b[39m                    torch.from_numpy(Xd_pred_n).to(DEVICE),\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m                    \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43my0_n\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    371\u001b[39m                    y_teacher=\u001b[38;5;28;01mNone\u001b[39;00m, tf_prob=\u001b[32m0.0\u001b[39m).detach().cpu().numpy()[\u001b[32m0\u001b[39m]\n\u001b[32m    372\u001b[39m yhat = (yhat_n * y_sd + y_mu).reshape(-\u001b[32m1\u001b[39m)\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# replace any residual NaN with the day's median\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "# ==== PyTorch seq2seq with ROBUST preprocessing (median impute + safe standardize + safe MAE) ====\n",
    "import duckdb, pandas as pd, numpy as np, datetime as dt, math\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()   # change target date if needed\n",
    "W_PAST = 168\n",
    "H_FUT = 24\n",
    "HOLDOUT_DAYS = 60\n",
    "HIDDEN = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def fit_standardizer(X2d):\n",
    "    \"\"\"Return (mean,std) with std guarded (==0 -> 1). X2d: (N, F)\"\"\"\n",
    "    mu = np.nanmean(X2d, axis=0)\n",
    "    sd = np.nanstd(X2d, axis=0)\n",
    "    sd = np.where(sd <= 1e-8, 1.0, sd)\n",
    "    return mu, sd\n",
    "\n",
    "def apply_standardizer(X, mu, sd):\n",
    "    return (X - mu) / sd\n",
    "\n",
    "# robust 3D imputer\n",
    "def impute_inplace_3d(X, med_vec):\n",
    "    \"\"\"X: (N, T, F) or (N, W, F); med_vec: (F,). Fills NaNs in-place with per-feature medians.\"\"\"\n",
    "    mask = np.isnan(X)\n",
    "    if mask.any():\n",
    "        med_broadcast = np.broadcast_to(med_vec, X.shape)\n",
    "        X[mask] = med_broadcast[mask]\n",
    "\n",
    "# sanity checker\n",
    "def assert_finite(name, arr):\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = int(np.isnan(arr).sum() + np.isinf(arr).sum())\n",
    "        raise RuntimeError(f\"{name} has {bad} non-finite values\")\n",
    "\n",
    "# ---------- pull data ----------\n",
    "con = duckdb.connect(DB)\n",
    "df_lags = con.execute(\"\"\"\n",
    "SELECT\n",
    "  ts, OperatingDTM, Interval, hb_houston,\n",
    "  p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "  dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "FROM vw_master_spine_lags\n",
    "ORDER BY ts\n",
    "\"\"\").df()\n",
    "df_lags[\"ts\"] = pd.to_datetime(df_lags[\"ts\"])\n",
    "\n",
    "df_future = con.execute(\"\"\"\n",
    "SELECT\n",
    "  f.OperatingDTM AS delivery_date,\n",
    "  f.Interval     AS delivery_interval,\n",
    "  f.hb_houston   AS target,\n",
    "  -- base future features\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.temp_the_woodlands_tx, f.hum_the_woodlands_tx, f.wind_the_woodlands_tx, f.precip_the_woodlands_tx,\n",
    "  f.temp_katy_tx, f.hum_katy_tx, f.wind_katy_tx, f.precip_katy_tx,\n",
    "  f.temp_friendswood_tx, f.hum_friendswood_tx, f.wind_friendswood_tx, f.precip_friendswood_tx,\n",
    "  f.temp_baytown_tx, f.hum_baytown_tx, f.wind_baytown_tx, f.precip_baytown_tx,\n",
    "  f.temp_houston_tx, f.hum_houston_tx, f.wind_houston_tx, f.precip_houston_tx,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "  f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f,\n",
    "  f.cal_sin_dow  AS cal_sin_dow_f,  f.cal_cos_dow  AS cal_cos_dow_f,\n",
    "  -- enhanced joins\n",
    "  l.net_wz, l.net_lz, l.net_wz_ramp1, l.net_lz_ramp1, l.lz_houston_ramp1, l.wz_spread, l.lz_spread,\n",
    "  w.temp_avg, w.temp_spread, w.temp_avg_ramp1, w.hum_avg, w.hum_spread, w.hum_avg_ramp1\n",
    "FROM vw_master_spine_ts f\n",
    "LEFT JOIN load_fcst_enh l\n",
    "  ON l.OperatingDTM = f.OperatingDTM AND l.Interval = f.Interval\n",
    "LEFT JOIN wx_fcst_enh w\n",
    "  ON w.OperatingDTM = f.OperatingDTM AND w.Interval = f.Interval\n",
    "ORDER BY f.OperatingDTM, f.Interval\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "df_future[\"delivery_date\"] = pd.to_datetime(df_future[\"delivery_date\"]).dt.date\n",
    "df_future[\"delivery_interval\"] = df_future[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# ---------- assemble samples (day-ahead) ----------\n",
    "enc_cols = [\n",
    "    \"hb_houston\",\"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"\n",
    "]\n",
    "dec_future_cols = [\n",
    "    \"cal_hour_f\",\"cal_dow_f\",\"cal_is_weekend_f\",\"cal_sin_hour_f\",\"cal_cos_hour_f\",\"cal_sin_dow_f\",\"cal_cos_dow_f\",\n",
    "    \"net_wz\",\"net_lz\",\"net_wz_ramp1\",\"net_lz_ramp1\",\"lz_houston_ramp1\",\"wz_spread\",\"lz_spread\",\n",
    "    \"temp_avg\",\"temp_spread\",\"temp_avg_ramp1\",\"hum_avg\",\"hum_spread\",\"hum_avg_ramp1\"\n",
    "]\n",
    "\n",
    "# base rows = end-of-day with price\n",
    "base_rows = df_lags[(df_lags[\"hb_houston\"].notna()) & (df_lags[\"Interval\"]==24)][[\"ts\",\"OperatingDTM\",\"hb_houston\"]].copy()\n",
    "base_rows[\"base_date\"] = pd.to_datetime(base_rows[\"OperatingDTM\"]).dt.date\n",
    "base_rows[\"delivery_date\"] = base_rows[\"base_date\"] + dt.timedelta(days=1)\n",
    "\n",
    "# only use delivery days with 24 actuals for training/val\n",
    "has24 = df_future.groupby(\"delivery_date\")[\"target\"].apply(lambda s: s.notna().sum()==24)\n",
    "valid_delivery_dates = set(has24[has24].index)\n",
    "\n",
    "df_lags_idx = df_lags.set_index(\"ts\").sortindex() if hasattr(pd.DataFrame, 'sortindex') else df_lags.set_index(\"ts\").sort_index()\n",
    "\n",
    "samples = []\n",
    "for _, r in base_rows.iterrows():\n",
    "    ddate = r[\"delivery_date\"]\n",
    "    ts0 = r[\"ts\"]\n",
    "    # encoder window\n",
    "    start = ts0 - pd.Timedelta(hours=W_PAST-1)\n",
    "    enc_df = df_lags_idx.loc[start:ts0][enc_cols]\n",
    "    if enc_df.shape[0] != W_PAST: \n",
    "        continue\n",
    "    if enc_df.isna().any().any():\n",
    "        continue  # strict: skip any encoder NaNs\n",
    "    # decoder future rows\n",
    "    fdf = df_future[(df_future[\"delivery_date\"]==ddate)].sort_values(\"delivery_interval\")\n",
    "    if fdf.shape[0] != H_FUT:\n",
    "        continue\n",
    "    X_enc = enc_df.values.astype(np.float32)\n",
    "    X_dec = fdf[dec_future_cols].values.astype(np.float32)\n",
    "    y     = fdf[\"target\"].values.astype(np.float32)\n",
    "    y0    = np.float32(r[\"hb_houston\"])\n",
    "    samples.append((X_enc, X_dec, y, y0, ddate))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples assembled; check data coverage or reduce W_PAST.\")\n",
    "\n",
    "# split by date\n",
    "dates = sorted({s[4] for s in samples})\n",
    "cut_date = dates[-1] - dt.timedelta(days=HOLDOUT_DAYS) if len(dates)>HOLDOUT_DAYS else dates[0]\n",
    "train_idx = [i for i,s in enumerate(samples) if s[4] <= cut_date and s[4] in valid_delivery_dates]\n",
    "val_idx   = [i for i,s in enumerate(samples) if s[4] >  cut_date and s[4] in valid_delivery_dates]\n",
    "\n",
    "def stack(idxs):\n",
    "    Xe = np.stack([samples[i][0] for i in idxs], axis=0)\n",
    "    Xd = np.stack([samples[i][1] for i in idxs], axis=0)\n",
    "    Y  = np.stack([samples[i][2] for i in idxs], axis=0)\n",
    "    Y0 = np.stack([samples[i][3] for i in idxs], axis=0)\n",
    "    return Xe, Xd, Y, Y0\n",
    "\n",
    "Xe_tr, Xd_tr, Y_tr, Y0_tr = stack(train_idx)\n",
    "Xe_va, Xd_va, Y_va, Y0_va = (None, None, None, None)\n",
    "if len(val_idx) > 0:\n",
    "    Xe_va, Xd_va, Y_va, Y0_va = stack(val_idx)\n",
    "\n",
    "# ---------- diagnose + drop all-NaN decoder features, then impute (robust) ----------\n",
    "# Flatten decoder train to 2D to inspect per-feature missingness\n",
    "F_orig = Xd_tr.shape[2]\n",
    "Xd_tr_2d_raw = Xd_tr.reshape(-1, F_orig)\n",
    "nan_counts = np.isnan(Xd_tr_2d_raw).sum(axis=0)\n",
    "all_nan_cols = nan_counts == Xd_tr_2d_raw.shape[0]\n",
    "\n",
    "if all_nan_cols.any():\n",
    "    dropped_cols = [dec_future_cols[i] for i,flag in enumerate(all_nan_cols) if flag]\n",
    "    print(f\"Dropping {int(all_nan_cols.sum())} decoder feature(s) with all-NaN in TRAIN: {dropped_cols}\")\n",
    "else:\n",
    "    dropped_cols = []\n",
    "\n",
    "# Keep mask to be reused at prediction time\n",
    "dec_keep_mask = ~all_nan_cols\n",
    "\n",
    "# Apply mask to decoder tensors\n",
    "Xd_tr = Xd_tr[:, :, dec_keep_mask]\n",
    "if Xe_va is not None:\n",
    "    Xd_va = Xd_va[:, :, dec_keep_mask]\n",
    "\n",
    "# Recompute medians on KEPT features\n",
    "dec_med = np.nanmedian(Xd_tr.reshape(-1, Xd_tr.shape[2]), axis=0)\n",
    "# Guard: if a median is still NaN for some reason, set to 0\n",
    "dec_med = np.where(np.isnan(dec_med), 0.0, dec_med)\n",
    "\n",
    "# Encoder median (rare but safe)\n",
    "enc_med = np.nanmedian(Xe_tr.reshape(-1, Xe_tr.shape[2]), axis=0)\n",
    "enc_med = np.where(np.isnan(enc_med), 0.0, enc_med)\n",
    "\n",
    "# Impute in-place (3D-safe)\n",
    "impute_inplace_3d(Xe_tr, enc_med)\n",
    "impute_inplace_3d(Xd_tr, dec_med)\n",
    "if Xe_va is not None:\n",
    "    impute_inplace_3d(Xe_va, enc_med)\n",
    "    impute_inplace_3d(Xd_va, dec_med)\n",
    "\n",
    "# Targets should be complete for training/val; assert\n",
    "assert np.isfinite(Y_tr).all(), \"Y_tr has non-finite values\"\n",
    "if Y_va is not None:\n",
    "    assert np.isfinite(Y_va).all(), \"Y_va has non-finite values\"\n",
    "\n",
    "# ---------- safe standardization ----------\n",
    "enc_mu, enc_sd = fit_standardizer(Xe_tr.reshape(-1, Xe_tr.shape[2]))\n",
    "dec_mu, dec_sd = fit_standardizer(Xd_tr.reshape(-1, Xd_tr.shape[2]))\n",
    "y_mu,  y_sd    = fit_standardizer(Y_tr.reshape(-1,1))\n",
    "\n",
    "Xe_tr_n = apply_standardizer(Xe_tr, enc_mu, enc_sd)\n",
    "Xd_tr_n = apply_standardizer(Xd_tr, dec_mu, dec_sd)\n",
    "Y_tr_n  = apply_standardizer(Y_tr,  y_mu,  y_sd).reshape(Y_tr.shape)\n",
    "Y0_tr_n = apply_standardizer(Y0_tr.reshape(-1,1), y_mu, y_sd).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = apply_standardizer(Xe_va, enc_mu, enc_sd)\n",
    "    Xd_va_n = apply_standardizer(Xd_va, dec_mu, dec_sd)\n",
    "    Y_va_n  = apply_standardizer(Y_va,  y_mu,  y_sd).reshape(Y_va.shape)\n",
    "    Y0_va_n = apply_standardizer(Y0_va.reshape(-1,1), y_mu, y_sd).reshape(-1)\n",
    "\n",
    "# Final safety: ensure everything is finite\n",
    "assert_finite(\"Xe_tr_n\", Xe_tr_n)\n",
    "assert_finite(\"Xd_tr_n\", Xd_tr_n)\n",
    "assert_finite(\"Y_tr_n\",  Y_tr_n)\n",
    "assert_finite(\"Y0_tr_n\", Y0_tr_n)\n",
    "if Xe_va is not None:\n",
    "    assert_finite(\"Xe_va_n\", Xe_va_n)\n",
    "    assert_finite(\"Xd_va_n\", Xd_va_n)\n",
    "    assert_finite(\"Y_va_n\",  Y_va_n)\n",
    "    assert_finite(\"Y0_va_n\", Y0_va_n)\n",
    "\n",
    "# ---------- torch datasets ----------\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xe, Xd, Y, Y0):\n",
    "        self.Xe = torch.from_numpy(Xe).float()\n",
    "        self.Xd = torch.from_numpy(Xd).float()\n",
    "        self.Y  = torch.from_numpy(Y).float()\n",
    "        self.Y0 = torch.from_numpy(Y0).float()\n",
    "    def __len__(self): return self.Xe.shape[0]\n",
    "    def __getitem__(self, i): return self.Xe[i], self.Xd[i], self.Y[i], self.Y0[i]\n",
    "\n",
    "bs = 32\n",
    "train_loader = torch.utils.data.DataLoader(Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=bs, shuffle=True)\n",
    "val_loader = None if Xe_va is None else torch.utils.data.DataLoader(Seq2SeqDataset(Xe_va_n, Xd_va_n, Y_va_n, Y0_va_n), batch_size=bs, shuffle=False)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim, hidden, batch_first=True)\n",
    "    def forward(self, x):  # x: (B,W,E)\n",
    "        _, h = self.rnn(x)\n",
    "        return h  # (1,B,H)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim + 1, hidden, batch_first=True)  # + prev y\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "    def forward(self, future_feats, h0, y0, teacher=None, tf_prob=0.5):\n",
    "        B, T, D = future_feats.shape\n",
    "        y_prev = y0.view(B,1)\n",
    "        h = h0\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            x_t = torch.cat([future_feats[:,t,:], y_prev], dim=1).unsqueeze(1)  # (B,1,D+1)\n",
    "            o, h = self.rnn(x_t, h)\n",
    "            y_t = self.out(o[:, -1, :]).squeeze(1)\n",
    "            outs.append(y_t)\n",
    "            if (teacher is not None) and (rng.random() < tf_prob):\n",
    "                y_prev = teacher[:,t].view(B,1)\n",
    "            else:\n",
    "                y_prev = y_t.view(B,1)\n",
    "        return torch.stack(outs, dim=1)  # (B,24)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(enc_in, hidden)\n",
    "        self.dec = Decoder(dec_in, hidden)\n",
    "    def forward(self, x_enc, x_dec, y0, y_teacher=None, tf_prob=0.5):\n",
    "        h = self.enc(x_enc)\n",
    "        return self.dec(x_dec, h, y0, y_teacher, tf_prob)\n",
    "\n",
    "enc_in = Xe_tr_n.shape[2]\n",
    "dec_in = Xd_tr_n.shape[2]\n",
    "model = Seq2Seq(enc_in, dec_in, HIDDEN).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in train_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=y, tf_prob=0.5)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    if val_loader is None: return None\n",
    "    model.eval()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(val_loader.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    print(f\"Epoch {ep:02d}  train_loss={tr:.4f}\" + (\"\" if va is None else f\"  val_loss={va:.4f}\"))\n",
    "\n",
    "# ---- evaluate holdout MAE in real units (SAFE) ----\n",
    "if val_loader is not None:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xenc, xdec, y, y0 in val_loader:\n",
    "            xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "            yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "            y_n    = y.detach().cpu().numpy()\n",
    "            # invert scaling\n",
    "            yhat = (yhat_n * y_sd + y_mu)\n",
    "            yt   = (y_n    * y_sd + y_mu)\n",
    "            preds.append(yhat.reshape(-1))\n",
    "            trues.append(yt.reshape(-1))\n",
    "    preds = np.concatenate(preds)\n",
    "    trues = np.concatenate(trues)\n",
    "    mask = np.isfinite(preds) & np.isfinite(trues)\n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped:\n",
    "        print(f\"Dropped {dropped} non-finite pairs from holdout scoring.\")\n",
    "    print({\"holdout_MAE\": float(mean_absolute_error(trues[mask], preds[mask]))})\n",
    "\n",
    "# ---- predict requested DELIVERY_DATE ----\n",
    "# find base row (previous day EOD with price)\n",
    "prev_day = DELIVERY_DATE - dt.timedelta(days=1)\n",
    "base_row = df_lags[(pd.to_datetime(df_lags[\"OperatingDTM\"]).dt.date == prev_day) & (df_lags[\"Interval\"]==24) & df_lags[\"hb_houston\"].notna()]\n",
    "if base_row.empty:\n",
    "    raise RuntimeError(f\"No base EOD price for {prev_day}\")\n",
    "ts0 = pd.to_datetime(base_row.iloc[0][\"ts\"])\n",
    "y0  = np.float32(base_row.iloc[0][\"hb_houston\"])\n",
    "\n",
    "# encoder window\n",
    "win = df_lags.set_index(\"ts\").loc[ts0 - pd.Timedelta(hours=W_PAST-1): ts0][enc_cols]\n",
    "if win.shape[0] != W_PAST or win.isna().any().any():\n",
    "    raise RuntimeError(\"Insufficient/NaN history for encoder window.\")\n",
    "Xe_pred = win.values.astype(np.float32)[None, ...]\n",
    "\n",
    "# decoder future features (24 rows)\n",
    "frows = df_future[(df_future[\"delivery_date\"]==DELIVERY_DATE)].sort_values(\"delivery_interval\")\n",
    "if frows.shape[0] != H_FUT:\n",
    "    raise RuntimeError(\"Spine missing 24 rows for delivery date.\")\n",
    "Xd_pred_full = frows[dec_future_cols].values.astype(np.float32)[None, ...]\n",
    "\n",
    "# Apply the same keep mask used in training (drop columns that were all-NaN in train)\n",
    "if 'dec_keep_mask' in globals():\n",
    "    Xd_pred = Xd_pred_full[:, :, dec_keep_mask]\n",
    "else:\n",
    "    Xd_pred = Xd_pred_full\n",
    "\n",
    "# impute + standardize pred using TRAIN stats\n",
    "m = np.isnan(Xd_pred)\n",
    "if m.any():\n",
    "    Xd_pred[m] = np.broadcast_to(dec_med, Xd_pred.shape)[m]\n",
    "Xe_pred_n = apply_standardizer(Xe_pred, enc_mu, enc_sd)\n",
    "Xd_pred_n = apply_standardizer(Xd_pred, dec_mu, dec_sd)\n",
    "y0_n = ((y0 - y_mu) / y_sd).reshape(()).astype(np.float32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_n = model(torch.from_numpy(Xe_pred_n).to(DEVICE),\n",
    "                   torch.from_numpy(Xd_pred_n).to(DEVICE),\n",
    "                   torch.tensor([y0_n], dtype=torch.float32, device=DEVICE),\n",
    "                   y_teacher=None, tf_prob=0.0).detach().cpu().numpy()[0]\n",
    "yhat = (yhat_n * y_sd + y_mu).reshape(-1)\n",
    "# replace any residual NaN with the day's median\n",
    "if np.isnan(yhat).any():\n",
    "    day_med = float(np.nanmedian(yhat))\n",
    "    yhat = np.nan_to_num(yhat, nan=day_med)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*H_FUT,\n",
    "    \"Interval\": list(range(1, H_FUT+1)),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "})\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "005fc7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 6 decoder feature(s) with all-NaN in TRAIN: ['temp_avg', 'temp_spread', 'temp_avg_ramp1', 'hum_avg', 'hum_spread', 'hum_avg_ramp1']\n",
      "Epoch 01  train_loss=0.0845  val_loss=0.0419\n",
      "Epoch 02  train_loss=0.0758  val_loss=0.0196\n",
      "Epoch 03  train_loss=0.0704  val_loss=0.0355\n",
      "Epoch 04  train_loss=0.0656  val_loss=0.0257\n",
      "Epoch 05  train_loss=0.0615  val_loss=0.0164\n",
      "Epoch 06  train_loss=0.0610  val_loss=0.2036\n",
      "Epoch 07  train_loss=0.0598  val_loss=0.0212\n",
      "Epoch 08  train_loss=0.0539  val_loss=0.1410\n",
      "Epoch 09  train_loss=0.0515  val_loss=0.0525\n",
      "Epoch 10  train_loss=0.0513  val_loss=0.0224\n",
      "Epoch 11  train_loss=0.0505  val_loss=0.0124\n",
      "Epoch 12  train_loss=0.0496  val_loss=0.0114\n",
      "Epoch 13  train_loss=0.0513  val_loss=0.0164\n",
      "Epoch 14  train_loss=0.0484  val_loss=0.0120\n",
      "Epoch 15  train_loss=0.0462  val_loss=0.0585\n",
      "Epoch 16  train_loss=0.0439  val_loss=0.0155\n",
      "Epoch 17  train_loss=0.0452  val_loss=0.0752\n",
      "Epoch 18  train_loss=0.0440  val_loss=0.0272\n",
      "Epoch 19  train_loss=0.0417  val_loss=0.0271\n",
      "Epoch 20  train_loss=0.0401  val_loss=0.0236\n",
      "{'holdout_MAE': 18.966270446777344}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OperatingDTM</th>\n",
       "      <th>Interval</th>\n",
       "      <th>hb_houston_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>1</td>\n",
       "      <td>35.257851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>2</td>\n",
       "      <td>36.735786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>3</td>\n",
       "      <td>34.342613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>4</td>\n",
       "      <td>31.196121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>5</td>\n",
       "      <td>29.188484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>6</td>\n",
       "      <td>28.787140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>7</td>\n",
       "      <td>28.717094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>8</td>\n",
       "      <td>26.184206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>9</td>\n",
       "      <td>28.311657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>10</td>\n",
       "      <td>27.253433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>11</td>\n",
       "      <td>23.242804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>12</td>\n",
       "      <td>20.145208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>13</td>\n",
       "      <td>19.890390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>14</td>\n",
       "      <td>22.825565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>15</td>\n",
       "      <td>25.668217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>16</td>\n",
       "      <td>24.794592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>17</td>\n",
       "      <td>22.974262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>18</td>\n",
       "      <td>34.377617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>19</td>\n",
       "      <td>57.447636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>20</td>\n",
       "      <td>87.135391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>21</td>\n",
       "      <td>81.351685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>22</td>\n",
       "      <td>53.209480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>23</td>\n",
       "      <td>36.522980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>24</td>\n",
       "      <td>28.648493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OperatingDTM  Interval  hb_houston_pred\n",
       "0    2025-08-18         1        35.257851\n",
       "1    2025-08-18         2        36.735786\n",
       "2    2025-08-18         3        34.342613\n",
       "3    2025-08-18         4        31.196121\n",
       "4    2025-08-18         5        29.188484\n",
       "5    2025-08-18         6        28.787140\n",
       "6    2025-08-18         7        28.717094\n",
       "7    2025-08-18         8        26.184206\n",
       "8    2025-08-18         9        28.311657\n",
       "9    2025-08-18        10        27.253433\n",
       "10   2025-08-18        11        23.242804\n",
       "11   2025-08-18        12        20.145208\n",
       "12   2025-08-18        13        19.890390\n",
       "13   2025-08-18        14        22.825565\n",
       "14   2025-08-18        15        25.668217\n",
       "15   2025-08-18        16        24.794592\n",
       "16   2025-08-18        17        22.974262\n",
       "17   2025-08-18        18        34.377617\n",
       "18   2025-08-18        19        57.447636\n",
       "19   2025-08-18        20        87.135391\n",
       "20   2025-08-18        21        81.351685\n",
       "21   2025-08-18        22        53.209480\n",
       "22   2025-08-18        23        36.522980\n",
       "23   2025-08-18        24        28.648493"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== PyTorch seq2seq with ROBUST preprocessing (median impute + safe standardize + safe MAE) ====\n",
    "import duckdb, pandas as pd, numpy as np, datetime as dt, math\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()   # change target date if needed\n",
    "W_PAST = 168\n",
    "H_FUT = 24\n",
    "HOLDOUT_DAYS = 60\n",
    "HIDDEN = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def fit_standardizer(X2d):\n",
    "    \"\"\"Return (mean,std) with std guarded (==0 -> 1). X2d: (N, F)\"\"\"\n",
    "    mu = np.nanmean(X2d, axis=0)\n",
    "    sd = np.nanstd(X2d, axis=0)\n",
    "    sd = np.where(sd <= 1e-8, 1.0, sd)\n",
    "    return mu, sd\n",
    "\n",
    "def apply_standardizer(X, mu, sd):\n",
    "    return (X - mu) / sd\n",
    "\n",
    "# robust 3D imputer\n",
    "def impute_inplace_3d(X, med_vec):\n",
    "    \"\"\"X: (N, T, F) or (N, W, F); med_vec: (F,). Fills NaNs in-place with per-feature medians.\"\"\"\n",
    "    mask = np.isnan(X)\n",
    "    if mask.any():\n",
    "        med_broadcast = np.broadcast_to(med_vec, X.shape)\n",
    "        X[mask] = med_broadcast[mask]\n",
    "\n",
    "# sanity checker\n",
    "def assert_finite(name, arr):\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = int(np.isnan(arr).sum() + np.isinf(arr).sum())\n",
    "        raise RuntimeError(f\"{name} has {bad} non-finite values\")\n",
    "\n",
    "# ---------- pull data ----------\n",
    "con = duckdb.connect(DB)\n",
    "df_lags = con.execute(\"\"\"\n",
    "SELECT\n",
    "  ts, OperatingDTM, Interval, hb_houston,\n",
    "  p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "  dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "FROM vw_master_spine_lags\n",
    "ORDER BY ts\n",
    "\"\"\").df()\n",
    "df_lags[\"ts\"] = pd.to_datetime(df_lags[\"ts\"])\n",
    "\n",
    "df_future = con.execute(\"\"\"\n",
    "SELECT\n",
    "  f.OperatingDTM AS delivery_date,\n",
    "  f.Interval     AS delivery_interval,\n",
    "  f.hb_houston   AS target,\n",
    "  -- base future features\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.temp_the_woodlands_tx, f.hum_the_woodlands_tx, f.wind_the_woodlands_tx, f.precip_the_woodlands_tx,\n",
    "  f.temp_katy_tx, f.hum_katy_tx, f.wind_katy_tx, f.precip_katy_tx,\n",
    "  f.temp_friendswood_tx, f.hum_friendswood_tx, f.wind_friendswood_tx, f.precip_friendswood_tx,\n",
    "  f.temp_baytown_tx, f.hum_baytown_tx, f.wind_baytown_tx, f.precip_baytown_tx,\n",
    "  f.temp_houston_tx, f.hum_houston_tx, f.wind_houston_tx, f.precip_houston_tx,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "  f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f,\n",
    "  f.cal_sin_dow  AS cal_sin_dow_f,  f.cal_cos_dow  AS cal_cos_dow_f,\n",
    "  -- enhanced joins\n",
    "  l.net_wz, l.net_lz, l.net_wz_ramp1, l.net_lz_ramp1, l.lz_houston_ramp1, l.wz_spread, l.lz_spread,\n",
    "  w.temp_avg, w.temp_spread, w.temp_avg_ramp1, w.hum_avg, w.hum_spread, w.hum_avg_ramp1\n",
    "FROM vw_master_spine_ts f\n",
    "LEFT JOIN load_fcst_enh l\n",
    "  ON l.OperatingDTM = f.OperatingDTM AND l.Interval = f.Interval\n",
    "LEFT JOIN wx_fcst_enh w\n",
    "  ON w.OperatingDTM = f.OperatingDTM AND w.Interval = f.Interval\n",
    "ORDER BY f.OperatingDTM, f.Interval\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "df_future[\"delivery_date\"] = pd.to_datetime(df_future[\"delivery_date\"]).dt.date\n",
    "df_future[\"delivery_interval\"] = df_future[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# ---------- assemble samples (day-ahead) ----------\n",
    "enc_cols = [\n",
    "    \"hb_houston\",\"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"\n",
    "]\n",
    "dec_future_cols = [\n",
    "    \"cal_hour_f\",\"cal_dow_f\",\"cal_is_weekend_f\",\"cal_sin_hour_f\",\"cal_cos_hour_f\",\"cal_sin_dow_f\",\"cal_cos_dow_f\",\n",
    "    \"net_wz\",\"net_lz\",\"net_wz_ramp1\",\"net_lz_ramp1\",\"lz_houston_ramp1\",\"wz_spread\",\"lz_spread\",\n",
    "    \"temp_avg\",\"temp_spread\",\"temp_avg_ramp1\",\"hum_avg\",\"hum_spread\",\"hum_avg_ramp1\"\n",
    "]\n",
    "\n",
    "# base rows = end-of-day with price\n",
    "base_rows = df_lags[(df_lags[\"hb_houston\"].notna()) & (df_lags[\"Interval\"]==24)][[\"ts\",\"OperatingDTM\",\"hb_houston\"]].copy()\n",
    "base_rows[\"base_date\"] = pd.to_datetime(base_rows[\"OperatingDTM\"]).dt.date\n",
    "base_rows[\"delivery_date\"] = base_rows[\"base_date\"] + dt.timedelta(days=1)\n",
    "\n",
    "# only use delivery days with 24 actuals for training/val\n",
    "has24 = df_future.groupby(\"delivery_date\")[\"target\"].apply(lambda s: s.notna().sum()==24)\n",
    "valid_delivery_dates = set(has24[has24].index)\n",
    "\n",
    "df_lags_idx = df_lags.set_index(\"ts\").sortindex() if hasattr(pd.DataFrame, 'sortindex') else df_lags.set_index(\"ts\").sort_index()\n",
    "\n",
    "samples = []\n",
    "for _, r in base_rows.iterrows():\n",
    "    ddate = r[\"delivery_date\"]\n",
    "    ts0 = r[\"ts\"]\n",
    "    # encoder window\n",
    "    start = ts0 - pd.Timedelta(hours=W_PAST-1)\n",
    "    enc_df = df_lags_idx.loc[start:ts0][enc_cols]\n",
    "    if enc_df.shape[0] != W_PAST: \n",
    "        continue\n",
    "    if enc_df.isna().any().any():\n",
    "        continue  # strict: skip any encoder NaNs\n",
    "    # decoder future rows\n",
    "    fdf = df_future[(df_future[\"delivery_date\"]==ddate)].sort_values(\"delivery_interval\")\n",
    "    if fdf.shape[0] != H_FUT:\n",
    "        continue\n",
    "    X_enc = enc_df.values.astype(np.float32)\n",
    "    X_dec = fdf[dec_future_cols].values.astype(np.float32)\n",
    "    y     = fdf[\"target\"].values.astype(np.float32)\n",
    "    y0    = np.float32(r[\"hb_houston\"])\n",
    "    samples.append((X_enc, X_dec, y, y0, ddate))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples assembled; check data coverage or reduce W_PAST.\")\n",
    "\n",
    "# split by date\n",
    "dates = sorted({s[4] for s in samples})\n",
    "cut_date = dates[-1] - dt.timedelta(days=HOLDOUT_DAYS) if len(dates)>HOLDOUT_DAYS else dates[0]\n",
    "train_idx = [i for i,s in enumerate(samples) if s[4] <= cut_date and s[4] in valid_delivery_dates]\n",
    "val_idx   = [i for i,s in enumerate(samples) if s[4] >  cut_date and s[4] in valid_delivery_dates]\n",
    "\n",
    "def stack(idxs):\n",
    "    Xe = np.stack([samples[i][0] for i in idxs], axis=0)\n",
    "    Xd = np.stack([samples[i][1] for i in idxs], axis=0)\n",
    "    Y  = np.stack([samples[i][2] for i in idxs], axis=0)\n",
    "    Y0 = np.stack([samples[i][3] for i in idxs], axis=0)\n",
    "    return Xe, Xd, Y, Y0\n",
    "\n",
    "Xe_tr, Xd_tr, Y_tr, Y0_tr = stack(train_idx)\n",
    "Xe_va, Xd_va, Y_va, Y0_va = (None, None, None, None)\n",
    "if len(val_idx) > 0:\n",
    "    Xe_va, Xd_va, Y_va, Y0_va = stack(val_idx)\n",
    "\n",
    "# ---------- diagnose + drop all-NaN decoder features, then impute (robust) ----------\n",
    "# Flatten decoder train to 2D to inspect per-feature missingness\n",
    "F_orig = Xd_tr.shape[2]\n",
    "Xd_tr_2d_raw = Xd_tr.reshape(-1, F_orig)\n",
    "nan_counts = np.isnan(Xd_tr_2d_raw).sum(axis=0)\n",
    "all_nan_cols = nan_counts == Xd_tr_2d_raw.shape[0]\n",
    "\n",
    "if all_nan_cols.any():\n",
    "    dropped_cols = [dec_future_cols[i] for i,flag in enumerate(all_nan_cols) if flag]\n",
    "    print(f\"Dropping {int(all_nan_cols.sum())} decoder feature(s) with all-NaN in TRAIN: {dropped_cols}\")\n",
    "else:\n",
    "    dropped_cols = []\n",
    "\n",
    "# Keep mask to be reused at prediction time\n",
    "dec_keep_mask = ~all_nan_cols\n",
    "\n",
    "# Apply mask to decoder tensors\n",
    "Xd_tr = Xd_tr[:, :, dec_keep_mask]\n",
    "if Xe_va is not None:\n",
    "    Xd_va = Xd_va[:, :, dec_keep_mask]\n",
    "\n",
    "# Recompute medians on KEPT features\n",
    "dec_med = np.nanmedian(Xd_tr.reshape(-1, Xd_tr.shape[2]), axis=0)\n",
    "# Guard: if a median is still NaN for some reason, set to 0\n",
    "dec_med = np.where(np.isnan(dec_med), 0.0, dec_med)\n",
    "\n",
    "# Encoder median (rare but safe)\n",
    "enc_med = np.nanmedian(Xe_tr.reshape(-1, Xe_tr.shape[2]), axis=0)\n",
    "enc_med = np.where(np.isnan(enc_med), 0.0, enc_med)\n",
    "\n",
    "# Impute in-place (3D-safe)\n",
    "impute_inplace_3d(Xe_tr, enc_med)\n",
    "impute_inplace_3d(Xd_tr, dec_med)\n",
    "if Xe_va is not None:\n",
    "    impute_inplace_3d(Xe_va, enc_med)\n",
    "    impute_inplace_3d(Xd_va, dec_med)\n",
    "\n",
    "# Targets should be complete for training/val; assert\n",
    "assert np.isfinite(Y_tr).all(), \"Y_tr has non-finite values\"\n",
    "if Y_va is not None:\n",
    "    assert np.isfinite(Y_va).all(), \"Y_va has non-finite values\"\n",
    "\n",
    "# ---------- safe standardization ----------\n",
    "enc_mu, enc_sd = fit_standardizer(Xe_tr.reshape(-1, Xe_tr.shape[2]))\n",
    "dec_mu, dec_sd = fit_standardizer(Xd_tr.reshape(-1, Xd_tr.shape[2]))\n",
    "y_mu,  y_sd    = fit_standardizer(Y_tr.reshape(-1,1))\n",
    "# Cast to Python floats to avoid 0-d array issues in scalar ops\n",
    "y_mu = float(np.atleast_1d(y_mu)[0]); y_sd = float(np.atleast_1d(y_sd)[0])\n",
    "\n",
    "Xe_tr_n = apply_standardizer(Xe_tr, enc_mu, enc_sd)\n",
    "Xd_tr_n = apply_standardizer(Xd_tr, dec_mu, dec_sd)\n",
    "Y_tr_n  = apply_standardizer(Y_tr,  y_mu,  y_sd).reshape(Y_tr.shape)\n",
    "Y0_tr_n = apply_standardizer(Y0_tr.reshape(-1,1), y_mu, y_sd).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = apply_standardizer(Xe_va, enc_mu, enc_sd)\n",
    "    Xd_va_n = apply_standardizer(Xd_va, dec_mu, dec_sd)\n",
    "    Y_va_n  = apply_standardizer(Y_va,  y_mu,  y_sd).reshape(Y_va.shape)\n",
    "    Y0_va_n = apply_standardizer(Y0_va.reshape(-1,1), y_mu, y_sd).reshape(-1)\n",
    "\n",
    "# Final safety: ensure everything is finite\n",
    "assert_finite(\"Xe_tr_n\", Xe_tr_n)\n",
    "assert_finite(\"Xd_tr_n\", Xd_tr_n)\n",
    "assert_finite(\"Y_tr_n\",  Y_tr_n)\n",
    "assert_finite(\"Y0_tr_n\", Y0_tr_n)\n",
    "if Xe_va is not None:\n",
    "    assert_finite(\"Xe_va_n\", Xe_va_n)\n",
    "    assert_finite(\"Xd_va_n\", Xd_va_n)\n",
    "    assert_finite(\"Y_va_n\",  Y_va_n)\n",
    "    assert_finite(\"Y0_va_n\", Y0_va_n)\n",
    "\n",
    "# ---------- torch datasets ----------\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xe, Xd, Y, Y0):\n",
    "        self.Xe = torch.from_numpy(Xe).float()\n",
    "        self.Xd = torch.from_numpy(Xd).float()\n",
    "        self.Y  = torch.from_numpy(Y).float()\n",
    "        self.Y0 = torch.from_numpy(Y0).float()\n",
    "    def __len__(self): return self.Xe.shape[0]\n",
    "    def __getitem__(self, i): return self.Xe[i], self.Xd[i], self.Y[i], self.Y0[i]\n",
    "\n",
    "bs = 32\n",
    "train_loader = torch.utils.data.DataLoader(Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=bs, shuffle=True)\n",
    "val_loader = None if Xe_va is None else torch.utils.data.DataLoader(Seq2SeqDataset(Xe_va_n, Xd_va_n, Y_va_n, Y0_va_n), batch_size=bs, shuffle=False)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim, hidden, batch_first=True)\n",
    "    def forward(self, x):  # x: (B,W,E)\n",
    "        _, h = self.rnn(x)\n",
    "        return h  # (1,B,H)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim + 1, hidden, batch_first=True)  # + prev y\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "    def forward(self, future_feats, h0, y0, teacher=None, tf_prob=0.5):\n",
    "        B, T, D = future_feats.shape\n",
    "        y_prev = y0.view(B,1)\n",
    "        h = h0\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            x_t = torch.cat([future_feats[:,t,:], y_prev], dim=1).unsqueeze(1)  # (B,1,D+1)\n",
    "            o, h = self.rnn(x_t, h)\n",
    "            y_t = self.out(o[:, -1, :]).squeeze(1)\n",
    "            outs.append(y_t)\n",
    "            if (teacher is not None) and (rng.random() < tf_prob):\n",
    "                y_prev = teacher[:,t].view(B,1)\n",
    "            else:\n",
    "                y_prev = y_t.view(B,1)\n",
    "        return torch.stack(outs, dim=1)  # (B,24)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(enc_in, hidden)\n",
    "        self.dec = Decoder(dec_in, hidden)\n",
    "    def forward(self, x_enc, x_dec, y0, y_teacher=None, tf_prob=0.5):\n",
    "        h = self.enc(x_enc)\n",
    "        return self.dec(x_dec, h, y0, y_teacher, tf_prob)\n",
    "\n",
    "enc_in = Xe_tr_n.shape[2]\n",
    "dec_in = Xd_tr_n.shape[2]\n",
    "model = Seq2Seq(enc_in, dec_in, HIDDEN).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in train_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=y, tf_prob=0.5)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    if val_loader is None: return None\n",
    "    model.eval()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(val_loader.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    print(f\"Epoch {ep:02d}  train_loss={tr:.4f}\" + (\"\" if va is None else f\"  val_loss={va:.4f}\"))\n",
    "\n",
    "# ---- evaluate holdout MAE in real units (SAFE) ----\n",
    "if val_loader is not None:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xenc, xdec, y, y0 in val_loader:\n",
    "            xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "            yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "            y_n    = y.detach().cpu().numpy()\n",
    "            # invert scaling\n",
    "            yhat = (yhat_n * y_sd + y_mu)\n",
    "            yt   = (y_n    * y_sd + y_mu)\n",
    "            preds.append(yhat.reshape(-1))\n",
    "            trues.append(yt.reshape(-1))\n",
    "    preds = np.concatenate(preds)\n",
    "    trues = np.concatenate(trues)\n",
    "    mask = np.isfinite(preds) & np.isfinite(trues)\n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped:\n",
    "        print(f\"Dropped {dropped} non-finite pairs from holdout scoring.\")\n",
    "    print({\"holdout_MAE\": float(mean_absolute_error(trues[mask], preds[mask]))})\n",
    "\n",
    "# ---- predict requested DELIVERY_DATE ----\n",
    "# find base row (previous day EOD with price)\n",
    "prev_day = DELIVERY_DATE - dt.timedelta(days=1)\n",
    "base_row = df_lags[(pd.to_datetime(df_lags[\"OperatingDTM\"]).dt.date == prev_day) & (df_lags[\"Interval\"]==24) & df_lags[\"hb_houston\"].notna()]\n",
    "if base_row.empty:\n",
    "    raise RuntimeError(f\"No base EOD price for {prev_day}\")\n",
    "ts0 = pd.to_datetime(base_row.iloc[0][\"ts\"])\n",
    "y0  = np.float32(base_row.iloc[0][\"hb_houston\"])\n",
    "\n",
    "# encoder window\n",
    "win = df_lags.set_index(\"ts\").loc[ts0 - pd.Timedelta(hours=W_PAST-1): ts0][enc_cols]\n",
    "if win.shape[0] != W_PAST or win.isna().any().any():\n",
    "    raise RuntimeError(\"Insufficient/NaN history for encoder window.\")\n",
    "Xe_pred = win.values.astype(np.float32)[None, ...]\n",
    "\n",
    "# decoder future features (24 rows)\n",
    "frows = df_future[(df_future[\"delivery_date\"]==DELIVERY_DATE)].sort_values(\"delivery_interval\")\n",
    "if frows.shape[0] != H_FUT:\n",
    "    raise RuntimeError(\"Spine missing 24 rows for delivery date.\")\n",
    "Xd_pred_full = frows[dec_future_cols].values.astype(np.float32)[None, ...]\n",
    "\n",
    "# Apply the same keep mask used in training (drop columns that were all-NaN in train)\n",
    "if 'dec_keep_mask' in globals():\n",
    "    Xd_pred = Xd_pred_full[:, :, dec_keep_mask]\n",
    "else:\n",
    "    Xd_pred = Xd_pred_full\n",
    "\n",
    "# impute + standardize pred using TRAIN stats\n",
    "m = np.isnan(Xd_pred)\n",
    "if m.any():\n",
    "    Xd_pred[m] = np.broadcast_to(dec_med, Xd_pred.shape)[m]\n",
    "Xe_pred_n = apply_standardizer(Xe_pred, enc_mu, enc_sd)\n",
    "Xd_pred_n = apply_standardizer(Xd_pred, dec_mu, dec_sd)\n",
    "# compute normalized scalar y0_n robustly\n",
    "y0_n = np.float32(((y0 - y_mu) / y_sd)).ravel()[0]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_n = model(torch.from_numpy(Xe_pred_n).to(DEVICE),\n",
    "                   torch.from_numpy(Xd_pred_n).to(DEVICE),\n",
    "                   torch.tensor([y0_n], dtype=torch.float32, device=DEVICE),\n",
    "                   y_teacher=None, tf_prob=0.0).detach().cpu().numpy()[0]\n",
    "yhat = (yhat_n * y_sd + y_mu).reshape(-1)\n",
    "# replace any residual NaN with the day's median\n",
    "if np.isnan(yhat).any():\n",
    "    day_med = float(np.nanmedian(yhat))\n",
    "    yhat = np.nan_to_num(yhat, nan=day_med)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*H_FUT,\n",
    "    \"Interval\": list(range(1, H_FUT+1)),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "})\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea9944",
   "metadata": {},
   "source": [
    "Weather backfill (no more all-NaN drops):\n",
    "create wx_hist_enh (historical weather enhanced) and modified the data pull to COALESCE forecast → historical (wx_fcst_enh → wx_hist_enh). We also add is_weather_proxy to see if using a proxy.\n",
    "\n",
    "Spike-aware target transform:\n",
    "Trained on a signed log of price (sgn(y)*log1p(|y|)) and invert with sgn(z)*expm1(|z|). This will hopefully compresses peaks during learning, helping the model fit surge hours without being dominated by MSE-like behavior. The scaler (μ,σ) is now fit on the transformed target.\n",
    "\n",
    "Peak-hour weighting:\n",
    "Added hour-of-day weights (2× for intervals 17–21) inside a weighted Huber loss. This gently prioritizes evening ramp/peak errors.\n",
    "\n",
    "Keep all earlier robustness:\n",
    "3-D median impute, zero-variance guards, strict finiteness checks, safe holdout MAE (now computed after inverse-transform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac881276",
   "metadata": {},
   "outputs": [
    {
     "ename": "BinderException",
     "evalue": "Binder Error: Referenced column \"temp_the_woodlands_tx\" not found in FROM clause!\nCandidate bindings: \"hist_temp_the_woodlands_tx\", \"hist_temp_friendswood_tx\", \"hist_precip_the_woodlands_tx\", \"hist_hum_the_woodlands_tx\", \"hist_wind_the_woodlands_tx\"\n\nLINE 6:     temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx...\n            ^",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBinderException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m con_ensure = duckdb.connect(DB)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Historical weather enhanced (to backfill when forecast is missing during training)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mcon_ensure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     47\u001b[39m \u001b[33;43mCREATE OR REPLACE VIEW wx_hist_enh AS\u001b[39;49m\n\u001b[32m     48\u001b[39m \u001b[33;43mWITH w AS (\u001b[39;49m\n\u001b[32m     49\u001b[39m \u001b[33;43m  SELECT\u001b[39;49m\n\u001b[32m     50\u001b[39m \u001b[33;43m    OperatingDTM, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m AS Interval,\u001b[39;49m\n\u001b[32m     51\u001b[39m \u001b[33;43m    temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx,\u001b[39;49m\n\u001b[32m     52\u001b[39m \u001b[33;43m    hum_the_woodlands_tx,  hum_katy_tx,  hum_friendswood_tx,  hum_baytown_tx,  hum_houston_tx\u001b[39;49m\n\u001b[32m     53\u001b[39m \u001b[33;43m  FROM vw_historical_weather_by_city\u001b[39;49m\n\u001b[32m     54\u001b[39m \u001b[33;43m),\u001b[39;49m\n\u001b[32m     55\u001b[39m \u001b[33;43magg AS (\u001b[39;49m\n\u001b[32m     56\u001b[39m \u001b[33;43m  SELECT\u001b[39;49m\n\u001b[32m     57\u001b[39m \u001b[33;43m    OperatingDTM, Interval,\u001b[39;49m\n\u001b[32m     58\u001b[39m \u001b[33;43m    (temp_the_woodlands_tx + temp_katy_tx + temp_friendswood_tx + temp_baytown_tx + temp_houston_tx)/5.0 AS temp_avg,\u001b[39;49m\n\u001b[32m     59\u001b[39m \u001b[33;43m    (hum_the_woodlands_tx  + hum_katy_tx  + hum_friendswood_tx  + hum_baytown_tx  + hum_houston_tx)/5.0  AS hum_avg,\u001b[39;49m\n\u001b[32m     60\u001b[39m \u001b[33;43m    GREATEST(temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx)\u001b[39;49m\n\u001b[32m     61\u001b[39m \u001b[33;43m      - LEAST(temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx) AS temp_spread,\u001b[39;49m\n\u001b[32m     62\u001b[39m \u001b[33;43m    GREATEST(hum_the_woodlands_tx, hum_katy_tx, hum_friendswood_tx, hum_baytown_tx, hum_houston_tx)\u001b[39;49m\n\u001b[32m     63\u001b[39m \u001b[33;43m      - LEAST(hum_the_woodlands_tx, hum_katy_tx, hum_friendswood_tx, hum_baytown_tx, hum_houston_tx) AS hum_spread\u001b[39;49m\n\u001b[32m     64\u001b[39m \u001b[33;43m  FROM w\u001b[39;49m\n\u001b[32m     65\u001b[39m \u001b[33;43m)\u001b[39;49m\n\u001b[32m     66\u001b[39m \u001b[33;43mSELECT\u001b[39;49m\n\u001b[32m     67\u001b[39m \u001b[33;43m  a.*,\u001b[39;49m\n\u001b[32m     68\u001b[39m \u001b[33;43m  a.temp_avg - LAG(a.temp_avg) OVER (ORDER BY OperatingDTM, Interval) AS temp_avg_ramp1,\u001b[39;49m\n\u001b[32m     69\u001b[39m \u001b[33;43m  a.hum_avg  - LAG(a.hum_avg)  OVER (ORDER BY OperatingDTM, Interval) AS hum_avg_ramp1\u001b[39;49m\n\u001b[32m     70\u001b[39m \u001b[33;43mFROM agg a;\u001b[39;49m\n\u001b[32m     71\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m con_ensure.close()\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Signed log transforms for spikes\u001b[39;00m\n",
      "\u001b[31mBinderException\u001b[39m: Binder Error: Referenced column \"temp_the_woodlands_tx\" not found in FROM clause!\nCandidate bindings: \"hist_temp_the_woodlands_tx\", \"hist_temp_friendswood_tx\", \"hist_precip_the_woodlands_tx\", \"hist_hum_the_woodlands_tx\", \"hist_wind_the_woodlands_tx\"\n\nLINE 6:     temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx...\n            ^"
     ]
    }
   ],
   "source": [
    "# ==== PyTorch seq2seq with ROBUST preprocessing (median impute + safe standardize + safe MAE) ====\n",
    "import duckdb, pandas as pd, numpy as np, datetime as dt, math\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()   # change target date if needed\n",
    "W_PAST = 168\n",
    "H_FUT = 24\n",
    "HOLDOUT_DAYS = 60\n",
    "HIDDEN = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def fit_standardizer(X2d):\n",
    "    \"\"\"Return (mean,std) with std guarded (==0 -> 1). X2d: (N, F)\"\"\"\n",
    "    mu = np.nanmean(X2d, axis=0)\n",
    "    sd = np.nanstd(X2d, axis=0)\n",
    "    sd = np.where(sd <= 1e-8, 1.0, sd)\n",
    "    return mu, sd\n",
    "\n",
    "def apply_standardizer(X, mu, sd):\n",
    "    return (X - mu) / sd\n",
    "\n",
    "# robust 3D imputer\n",
    "def impute_inplace_3d(X, med_vec):\n",
    "    \"\"\"X: (N, T, F) or (N, W, F); med_vec: (F,). Fills NaNs in-place with per-feature medians.\"\"\"\n",
    "    mask = np.isnan(X)\n",
    "    if mask.any():\n",
    "        med_broadcast = np.broadcast_to(med_vec, X.shape)\n",
    "        X[mask] = med_broadcast[mask]\n",
    "\n",
    "# sanity checker\n",
    "def assert_finite(name, arr):\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = int(np.isnan(arr).sum() + np.isinf(arr).sum())\n",
    "        raise RuntimeError(f\"{name} has {bad} non-finite values\")\n",
    "\n",
    "# ---------- ensure weather-enhanced views (hist + fcst) exist & helper transforms ----------\n",
    "con_ensure = duckdb.connect(DB)\n",
    "# Historical weather enhanced (to backfill when forecast is missing during training)\n",
    "con_ensure.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW wx_hist_enh AS\n",
    "WITH w AS (\n",
    "  SELECT\n",
    "    OperatingDTM, \"interval\" AS Interval,\n",
    "    temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx,\n",
    "    hum_the_woodlands_tx,  hum_katy_tx,  hum_friendswood_tx,  hum_baytown_tx,  hum_houston_tx\n",
    "  FROM vw_historical_weather_by_city\n",
    "),\n",
    "agg AS (\n",
    "  SELECT\n",
    "    OperatingDTM, Interval,\n",
    "    (temp_the_woodlands_tx + temp_katy_tx + temp_friendswood_tx + temp_baytown_tx + temp_houston_tx)/5.0 AS temp_avg,\n",
    "    (hum_the_woodlands_tx  + hum_katy_tx  + hum_friendswood_tx  + hum_baytown_tx  + hum_houston_tx)/5.0  AS hum_avg,\n",
    "    GREATEST(temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx)\n",
    "      - LEAST(temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx) AS temp_spread,\n",
    "    GREATEST(hum_the_woodlands_tx, hum_katy_tx, hum_friendswood_tx, hum_baytown_tx, hum_houston_tx)\n",
    "      - LEAST(hum_the_woodlands_tx, hum_katy_tx, hum_friendswood_tx, hum_baytown_tx, hum_houston_tx) AS hum_spread\n",
    "  FROM w\n",
    ")\n",
    "SELECT\n",
    "  a.*,\n",
    "  a.temp_avg - LAG(a.temp_avg) OVER (ORDER BY OperatingDTM, Interval) AS temp_avg_ramp1,\n",
    "  a.hum_avg  - LAG(a.hum_avg)  OVER (ORDER BY OperatingDTM, Interval) AS hum_avg_ramp1\n",
    "FROM agg a;\n",
    "\"\"\")\n",
    "con_ensure.close()\n",
    "\n",
    "# Signed log transforms for spikes\n",
    "sgn = np.sign\n",
    "abs_ = np.abs\n",
    "\n",
    "def sgn_log1p(y):\n",
    "    return sgn(y) * np.log1p(abs_(y))\n",
    "\n",
    "def sgn_expm1(z):\n",
    "    return sgn(z) * np.expm1(abs_(z))\n",
    "\n",
    "# Hour-of-day weights (emphasize 17-21 local hours)\n",
    "HOUR_WEIGHTS = np.ones(24, dtype=np.float32)\n",
    "HOUR_WEIGHTS[16:21] = 2.0  # intervals 17..21\n",
    "\n",
    "# ---------- pull data ----------\n",
    "con = duckdb.connect(DB)\n",
    "df_lags = con.execute(\"\"\"\n",
    "SELECT\n",
    "  ts, OperatingDTM, Interval, hb_houston,\n",
    "  p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "  dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "FROM vw_master_spine_lags\n",
    "ORDER BY ts\n",
    "\"\"\").df()\n",
    "df_lags[\"ts\"] = pd.to_datetime(df_lags[\"ts\"])\n",
    "\n",
    "df_future = con.execute(\"\"\"\n",
    "SELECT\n",
    "  f.OperatingDTM AS delivery_date,\n",
    "  f.Interval     AS delivery_interval,\n",
    "  f.hb_houston   AS target,\n",
    "  -- base future features\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "  f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f,\n",
    "  f.cal_sin_dow  AS cal_sin_dow_f,  f.cal_cos_dow  AS cal_cos_dow_f,\n",
    "  -- load-enhanced joins\n",
    "  l.net_wz, l.net_lz, l.net_wz_ramp1, l.net_lz_ramp1, l.lz_houston_ramp1, l.wz_spread, l.lz_spread,\n",
    "  -- weather: COALESCE forecast -> historical\n",
    "  COALESCE(wf.temp_avg,      wh.temp_avg)      AS temp_avg,\n",
    "  COALESCE(wf.temp_spread,   wh.temp_spread)   AS temp_spread,\n",
    "  COALESCE(wf.temp_avg_ramp1,wh.temp_avg_ramp1)AS temp_avg_ramp1,\n",
    "  COALESCE(wf.hum_avg,       wh.hum_avg)       AS hum_avg,\n",
    "  COALESCE(wf.hum_spread,    wh.hum_spread)    AS hum_spread,\n",
    "  COALESCE(wf.hum_avg_ramp1, wh.hum_avg_ramp1) AS hum_avg_ramp1,\n",
    "  CASE WHEN wf.temp_avg IS NULL THEN 1 ELSE 0 END AS is_weather_proxy\n",
    "FROM vw_master_spine_ts f\n",
    "LEFT JOIN load_fcst_enh l\n",
    "  ON l.OperatingDTM = f.OperatingDTM AND l.Interval = f.Interval\n",
    "LEFT JOIN wx_fcst_enh  wf\n",
    "  ON wf.OperatingDTM = f.OperatingDTM AND wf.Interval = f.Interval\n",
    "LEFT JOIN wx_hist_enh  wh\n",
    "  ON wh.OperatingDTM = f.OperatingDTM AND wh.Interval = f.Interval\n",
    "ORDER BY f.OperatingDTM, f.Interval\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "df_future[\"delivery_date\"] = pd.to_datetime(df_future[\"delivery_date\"]).dt.date\n",
    "df_future[\"delivery_interval\"] = df_future[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# ---------- assemble samples (day-ahead) ----------\n",
    "enc_cols = [\n",
    "    \"hb_houston\",\"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"\n",
    "]\n",
    "dec_future_cols = [\n",
    "    \"cal_hour_f\",\"cal_dow_f\",\"cal_is_weekend_f\",\"cal_sin_hour_f\",\"cal_cos_hour_f\",\"cal_sin_dow_f\",\"cal_cos_dow_f\",\n",
    "    \"net_wz\",\"net_lz\",\"net_wz_ramp1\",\"net_lz_ramp1\",\"lz_houston_ramp1\",\"wz_spread\",\"lz_spread\",\n",
    "    \"temp_avg\",\"temp_spread\",\"temp_avg_ramp1\",\"hum_avg\",\"hum_spread\",\"hum_avg_ramp1\"\n",
    "]\n",
    "\n",
    "# base rows = end-of-day with price\n",
    "base_rows = df_lags[(df_lags[\"hb_houston\"].notna()) & (df_lags[\"Interval\"]==24)][[\"ts\",\"OperatingDTM\",\"hb_houston\"]].copy()\n",
    "base_rows[\"base_date\"] = pd.to_datetime(base_rows[\"OperatingDTM\"]).dt.date\n",
    "base_rows[\"delivery_date\"] = base_rows[\"base_date\"] + dt.timedelta(days=1)\n",
    "\n",
    "# only use delivery days with 24 actuals for training/val\n",
    "has24 = df_future.groupby(\"delivery_date\")[\"target\"].apply(lambda s: s.notna().sum()==24)\n",
    "valid_delivery_dates = set(has24[has24].index)\n",
    "\n",
    "df_lags_idx = df_lags.set_index(\"ts\").sortindex() if hasattr(pd.DataFrame, 'sortindex') else df_lags.set_index(\"ts\").sort_index()\n",
    "\n",
    "samples = []\n",
    "for _, r in base_rows.iterrows():\n",
    "    ddate = r[\"delivery_date\"]\n",
    "    ts0 = r[\"ts\"]\n",
    "    # encoder window\n",
    "    start = ts0 - pd.Timedelta(hours=W_PAST-1)\n",
    "    enc_df = df_lags_idx.loc[start:ts0][enc_cols]\n",
    "    if enc_df.shape[0] != W_PAST: \n",
    "        continue\n",
    "    if enc_df.isna().any().any():\n",
    "        continue  # strict: skip any encoder NaNs\n",
    "    # decoder future rows\n",
    "    fdf = df_future[(df_future[\"delivery_date\"]==ddate)].sort_values(\"delivery_interval\")\n",
    "    if fdf.shape[0] != H_FUT:\n",
    "        continue\n",
    "    X_enc = enc_df.values.astype(np.float32)\n",
    "    X_dec = fdf[dec_future_cols].values.astype(np.float32)\n",
    "    y     = fdf[\"target\"].values.astype(np.float32)\n",
    "    y0    = np.float32(r[\"hb_houston\"])\n",
    "    samples.append((X_enc, X_dec, y, y0, ddate))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples assembled; check data coverage or reduce W_PAST.\")\n",
    "\n",
    "# split by date\n",
    "dates = sorted({s[4] for s in samples})\n",
    "cut_date = dates[-1] - dt.timedelta(days=HOLDOUT_DAYS) if len(dates)>HOLDOUT_DAYS else dates[0]\n",
    "train_idx = [i for i,s in enumerate(samples) if s[4] <= cut_date and s[4] in valid_delivery_dates]\n",
    "val_idx   = [i for i,s in enumerate(samples) if s[4] >  cut_date and s[4] in valid_delivery_dates]\n",
    "\n",
    "def stack(idxs):\n",
    "    Xe = np.stack([samples[i][0] for i in idxs], axis=0)\n",
    "    Xd = np.stack([samples[i][1] for i in idxs], axis=0)\n",
    "    Y  = np.stack([samples[i][2] for i in idxs], axis=0)\n",
    "    Y0 = np.stack([samples[i][3] for i in idxs], axis=0)\n",
    "    return Xe, Xd, Y, Y0\n",
    "\n",
    "Xe_tr, Xd_tr, Y_tr, Y0_tr = stack(train_idx)\n",
    "Xe_va, Xd_va, Y_va, Y0_va = (None, None, None, None)\n",
    "if len(val_idx) > 0:\n",
    "    Xe_va, Xd_va, Y_va, Y0_va = stack(val_idx)\n",
    "\n",
    "# ---------- diagnose + drop all-NaN decoder features, then impute (robust) ----------\n",
    "# Flatten decoder train to 2D to inspect per-feature missingness\n",
    "F_orig = Xd_tr.shape[2]\n",
    "Xd_tr_2d_raw = Xd_tr.reshape(-1, F_orig)\n",
    "nan_counts = np.isnan(Xd_tr_2d_raw).sum(axis=0)\n",
    "all_nan_cols = nan_counts == Xd_tr_2d_raw.shape[0]\n",
    "\n",
    "if all_nan_cols.any():\n",
    "    dropped_cols = [dec_future_cols[i] for i,flag in enumerate(all_nan_cols) if flag]\n",
    "    print(f\"Dropping {int(all_nan_cols.sum())} decoder feature(s) with all-NaN in TRAIN: {dropped_cols}\")\n",
    "else:\n",
    "    dropped_cols = []\n",
    "\n",
    "# Keep mask to be reused at prediction time\n",
    "dec_keep_mask = ~all_nan_cols\n",
    "\n",
    "# Apply mask to decoder tensors\n",
    "Xd_tr = Xd_tr[:, :, dec_keep_mask]\n",
    "if Xe_va is not None:\n",
    "    Xd_va = Xd_va[:, :, dec_keep_mask]\n",
    "\n",
    "# Recompute medians on KEPT features\n",
    "dec_med = np.nanmedian(Xd_tr.reshape(-1, Xd_tr.shape[2]), axis=0)\n",
    "# Guard: if a median is still NaN for some reason, set to 0\n",
    "dec_med = np.where(np.isnan(dec_med), 0.0, dec_med)\n",
    "\n",
    "# Encoder median (rare but safe)\n",
    "enc_med = np.nanmedian(Xe_tr.reshape(-1, Xe_tr.shape[2]), axis=0)\n",
    "enc_med = np.where(np.isnan(enc_med), 0.0, enc_med)\n",
    "\n",
    "# Impute in-place (3D-safe)\n",
    "impute_inplace_3d(Xe_tr, enc_med)\n",
    "impute_inplace_3d(Xd_tr, dec_med)\n",
    "if Xe_va is not None:\n",
    "    impute_inplace_3d(Xe_va, enc_med)\n",
    "    impute_inplace_3d(Xd_va, dec_med)\n",
    "\n",
    "# Targets should be complete for training/val; assert\n",
    "assert np.isfinite(Y_tr).all(), \"Y_tr has non-finite values\"\n",
    "if Y_va is not None:\n",
    "    assert np.isfinite(Y_va).all(), \"Y_va has non-finite values\"\n",
    "\n",
    "# ---------- safe standardization ----------\n",
    "enc_mu, enc_sd = fit_standardizer(Xe_tr.reshape(-1, Xe_tr.shape[2]))\n",
    "dec_mu, dec_sd = fit_standardizer(Xd_tr.reshape(-1, Xd_tr.shape[2]))\n",
    "# ---- target transform (spike-aware) then standardize ----\n",
    "Y_tr_t  = sgn_log1p(Y_tr)\n",
    "y_mu, y_sd = fit_standardizer(Y_tr_t.reshape(-1,1))\n",
    "y_mu = float(np.atleast_1d(y_mu)[0]); y_sd = float(np.atleast_1d(y_sd)[0])\n",
    "\n",
    "Xe_tr_n = apply_standardizer(Xe_tr, enc_mu, enc_sd)\n",
    "Xd_tr_n = apply_standardizer(Xd_tr, dec_mu, dec_sd)\n",
    "Y_tr_n  = apply_standardizer(Y_tr_t, y_mu, y_sd).reshape(Y_tr.shape)\n",
    "Y0_tr_t = sgn_log1p(Y0_tr.reshape(-1,1)).reshape(-1,1)\n",
    "Y0_tr_n = apply_standardizer(Y0_tr_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = apply_standardizer(Xe_va, enc_mu, enc_sd)\n",
    "    Xd_va_n = apply_standardizer(Xd_va, dec_mu, dec_sd)\n",
    "    Y_va_t  = sgn_log1p(Y_va)\n",
    "    Y_va_n  = apply_standardizer(Y_va_t,  y_mu,  y_sd).reshape(Y_va.shape)\n",
    "    Y0_va_t = sgn_log1p(Y0_va.reshape(-1,1))\n",
    "    Y0_va_n = apply_standardizer(Y0_va_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = apply_standardizer(Xe_va, enc_mu, enc_sd)\n",
    "    Xd_va_n = apply_standardizer(Xd_va, dec_mu, dec_sd)\n",
    "    Y_va_n  = apply_standardizer(Y_va,  y_mu,  y_sd).reshape(Y_va.shape)\n",
    "    Y0_va_n = apply_standardizer(Y0_va.reshape(-1,1), y_mu, y_sd).reshape(-1)\n",
    "\n",
    "# Final safety: ensure everything is finite\n",
    "assert_finite(\"Xe_tr_n\", Xe_tr_n)\n",
    "assert_finite(\"Xd_tr_n\", Xd_tr_n)\n",
    "assert_finite(\"Y_tr_n\",  Y_tr_n)\n",
    "assert_finite(\"Y0_tr_n\", Y0_tr_n)\n",
    "if Xe_va is not None:\n",
    "    assert_finite(\"Xe_va_n\", Xe_va_n)\n",
    "    assert_finite(\"Xd_va_n\", Xd_va_n)\n",
    "    assert_finite(\"Y_va_n\",  Y_va_n)\n",
    "    assert_finite(\"Y0_va_n\", Y0_va_n)\n",
    "\n",
    "# ---------- torch datasets ----------\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xe, Xd, Y, Y0):\n",
    "        self.Xe = torch.from_numpy(Xe).float()\n",
    "        self.Xd = torch.from_numpy(Xd).float()\n",
    "        self.Y  = torch.from_numpy(Y).float()\n",
    "        self.Y0 = torch.from_numpy(Y0).float()\n",
    "    def __len__(self): return self.Xe.shape[0]\n",
    "    def __getitem__(self, i): return self.Xe[i], self.Xd[i], self.Y[i], self.Y0[i]\n",
    "\n",
    "bs = 32\n",
    "train_loader = torch.utils.data.DataLoader(Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=bs, shuffle=True)\n",
    "val_loader = None if Xe_va is None else torch.utils.data.DataLoader(Seq2SeqDataset(Xe_va_n, Xd_va_n, Y_va_n, Y0_va_n), batch_size=bs, shuffle=False)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim, hidden, batch_first=True)\n",
    "    def forward(self, x):  # x: (B,W,E)\n",
    "        _, h = self.rnn(x)\n",
    "        return h  # (1,B,H)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim + 1, hidden, batch_first=True)  # + prev y\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "    def forward(self, future_feats, h0, y0, teacher=None, tf_prob=0.5):\n",
    "        B, T, D = future_feats.shape\n",
    "        y_prev = y0.view(B,1)\n",
    "        h = h0\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            x_t = torch.cat([future_feats[:,t,:], y_prev], dim=1).unsqueeze(1)  # (B,1,D+1)\n",
    "            o, h = self.rnn(x_t, h)\n",
    "            y_t = self.out(o[:, -1, :]).squeeze(1)\n",
    "            outs.append(y_t)\n",
    "            if (teacher is not None) and (rng.random() < tf_prob):\n",
    "                y_prev = teacher[:,t].view(B,1)\n",
    "            else:\n",
    "                y_prev = y_t.view(B,1)\n",
    "        return torch.stack(outs, dim=1)  # (B,24)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(enc_in, hidden)\n",
    "        self.dec = Decoder(dec_in, hidden)\n",
    "    def forward(self, x_enc, x_dec, y0, y_teacher=None, tf_prob=0.5):\n",
    "        h = self.enc(x_enc)\n",
    "        return self.dec(x_dec, h, y0, y_teacher, tf_prob)\n",
    "\n",
    "enc_in = Xe_tr_n.shape[2]\n",
    "dec_in = Xd_tr_n.shape[2]\n",
    "model = Seq2Seq(enc_in, dec_in, HIDDEN).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "def huber_weighted(yhat, y, w, delta=1.0):\n",
    "    d = torch.abs(yhat - y)\n",
    "    q = torch.clamp(d, max=delta)\n",
    "    l = 0.5 * q**2 + delta * (d - q)\n",
    "    return (l * w).mean()\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in train_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=y, tf_prob=0.5)\n",
    "        # time-step weights (B,24)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    if val_loader is None: return None\n",
    "    model.eval()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(val_loader.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    print(f\"Epoch {ep:02d}  train_loss={tr:.4f}\" + (\"\" if va is None else f\"  val_loss={va:.4f}\"))\n",
    "\n",
    "# ---- evaluate holdout MAE in real units (SAFE) ----\n",
    "if val_loader is not None:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xenc, xdec, y, y0 in val_loader:\n",
    "            xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "            yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "            y_n    = y.detach().cpu().numpy()\n",
    "            # invert scaling\n",
    "            yhat_t = (yhat_n * y_sd + y_mu)\n",
    "yt_t   = (y_n    * y_sd + y_mu)\n",
    "yhat = sgn_expm1(yhat_t).reshape(-1)\n",
    "yt   = sgn_expm1(yt_t).reshape(-1)\n",
    "preds.append(yhat); trues.append(yt)\n",
    "preds = np.concatenate(preds)\n",
    "trues = np.concatenate(trues)\n",
    "mask = np.isfinite(preds) & np.isfinite(trues)\n",
    "dropped = int((~mask).sum())\n",
    "if dropped:\n",
    "        print(f\"Dropped {dropped} non-finite pairs from holdout scoring.\")\n",
    "print({\"holdout_MAE\": float(mean_absolute_error(trues[mask], preds[mask]))})\n",
    "\n",
    "# ---- predict requested DELIVERY_DATE ----\n",
    "# find base row (previous day EOD with price)\n",
    "prev_day = DELIVERY_DATE - dt.timedelta(days=1)\n",
    "base_row = df_lags[(pd.to_datetime(df_lags[\"OperatingDTM\"]).dt.date == prev_day) & (df_lags[\"Interval\"]==24) & df_lags[\"hb_houston\"].notna()]\n",
    "if base_row.empty:\n",
    "    raise RuntimeError(f\"No base EOD price for {prev_day}\")\n",
    "ts0 = pd.to_datetime(base_row.iloc[0][\"ts\"])\n",
    "y0  = np.float32(base_row.iloc[0][\"hb_houston\"])\n",
    "\n",
    "# encoder window\n",
    "win = df_lags.set_index(\"ts\").loc[ts0 - pd.Timedelta(hours=W_PAST-1): ts0][enc_cols]\n",
    "if win.shape[0] != W_PAST or win.isna().any().any():\n",
    "    raise RuntimeError(\"Insufficient/NaN history for encoder window.\")\n",
    "Xe_pred = win.values.astype(np.float32)[None, ...]\n",
    "\n",
    "# decoder future features (24 rows)\n",
    "frows = df_future[(df_future[\"delivery_date\"]==DELIVERY_DATE)].sort_values(\"delivery_interval\")\n",
    "if frows.shape[0] != H_FUT:\n",
    "    raise RuntimeError(\"Spine missing 24 rows for delivery date.\")\n",
    "Xd_pred_full = frows[dec_future_cols].values.astype(np.float32)[None, ...]\n",
    "\n",
    "# Apply the same keep mask used in training (drop columns that were all-NaN in train)\n",
    "if 'dec_keep_mask' in globals():\n",
    "    Xd_pred = Xd_pred_full[:, :, dec_keep_mask]\n",
    "else:\n",
    "    Xd_pred = Xd_pred_full\n",
    "\n",
    "# impute + standardize pred using TRAIN stats\n",
    "m = np.isnan(Xd_pred)\n",
    "if m.any():\n",
    "    Xd_pred[m] = np.broadcast_to(dec_med, Xd_pred.shape)[m]\n",
    "Xe_pred_n = apply_standardizer(Xe_pred, enc_mu, enc_sd)\n",
    "Xd_pred_n = apply_standardizer(Xd_pred, dec_mu, dec_sd)\n",
    "# compute normalized scalar y0_n using spike-aware transform\n",
    "y0_t = sgn_log1p(y0)\n",
    "y0_n = np.float32(((y0_t - y_mu) / y_sd)).ravel()[0]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_n = model(torch.from_numpy(Xe_pred_n).to(DEVICE),\n",
    "                   torch.from_numpy(Xd_pred_n).to(DEVICE),\n",
    "                   torch.tensor([y0_n], dtype=torch.float32, device=DEVICE),\n",
    "                   y_teacher=None, tf_prob=0.0).detach().cpu().numpy()[0]\n",
    "yhat_t = (yhat_n * y_sd + y_mu).reshape(-1)\n",
    "yhat = sgn_expm1(yhat_t)\n",
    "# replace any residual NaN with the day's median\n",
    "if np.isnan(yhat).any():\n",
    "    day_med = float(np.nanmedian(yhat))\n",
    "    yhat = np.nan_to_num(yhat, nan=day_med)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*H_FUT,\n",
    "    \"Interval\": list(range(1, H_FUT+1)),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "})\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26768a43",
   "metadata": {},
   "outputs": [
    {
     "ename": "BinderException",
     "evalue": "Binder Error: Referenced column \"temp_the_woodlands_tx\" not found in FROM clause!\nCandidate bindings: \"hist_temp_the_woodlands_tx\", \"hist_temp_friendswood_tx\", \"hist_precip_the_woodlands_tx\", \"hist_hum_the_woodlands_tx\", \"hist_wind_the_woodlands_tx\"\n\nLINE 6:     temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx...\n            ^",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBinderException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m con_ensure = duckdb.connect(DB)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Historical weather enhanced (to backfill when forecast is missing during training)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mcon_ensure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     47\u001b[39m \u001b[33;43mCREATE OR REPLACE VIEW wx_hist_enh AS\u001b[39;49m\n\u001b[32m     48\u001b[39m \u001b[33;43mWITH w AS (\u001b[39;49m\n\u001b[32m     49\u001b[39m \u001b[33;43m  SELECT\u001b[39;49m\n\u001b[32m     50\u001b[39m \u001b[33;43m    OperatingDTM, \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minterval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m AS Interval,\u001b[39;49m\n\u001b[32m     51\u001b[39m \u001b[33;43m    temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx,\u001b[39;49m\n\u001b[32m     52\u001b[39m \u001b[33;43m    hum_the_woodlands_tx,  hum_katy_tx,  hum_friendswood_tx,  hum_baytown_tx,  hum_houston_tx\u001b[39;49m\n\u001b[32m     53\u001b[39m \u001b[33;43m  FROM vw_historical_weather_by_city\u001b[39;49m\n\u001b[32m     54\u001b[39m \u001b[33;43m),\u001b[39;49m\n\u001b[32m     55\u001b[39m \u001b[33;43magg AS (\u001b[39;49m\n\u001b[32m     56\u001b[39m \u001b[33;43m  SELECT\u001b[39;49m\n\u001b[32m     57\u001b[39m \u001b[33;43m    OperatingDTM, Interval,\u001b[39;49m\n\u001b[32m     58\u001b[39m \u001b[33;43m    (temp_the_woodlands_tx + temp_katy_tx + temp_friendswood_tx + temp_baytown_tx + temp_houston_tx)/5.0 AS temp_avg,\u001b[39;49m\n\u001b[32m     59\u001b[39m \u001b[33;43m    (hum_the_woodlands_tx  + hum_katy_tx  + hum_friendswood_tx  + hum_baytown_tx  + hum_houston_tx)/5.0  AS hum_avg,\u001b[39;49m\n\u001b[32m     60\u001b[39m \u001b[33;43m    GREATEST(temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx)\u001b[39;49m\n\u001b[32m     61\u001b[39m \u001b[33;43m      - LEAST(temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx) AS temp_spread,\u001b[39;49m\n\u001b[32m     62\u001b[39m \u001b[33;43m    GREATEST(hum_the_woodlands_tx, hum_katy_tx, hum_friendswood_tx, hum_baytown_tx, hum_houston_tx)\u001b[39;49m\n\u001b[32m     63\u001b[39m \u001b[33;43m      - LEAST(hum_the_woodlands_tx, hum_katy_tx, hum_friendswood_tx, hum_baytown_tx, hum_houston_tx) AS hum_spread\u001b[39;49m\n\u001b[32m     64\u001b[39m \u001b[33;43m  FROM w\u001b[39;49m\n\u001b[32m     65\u001b[39m \u001b[33;43m)\u001b[39;49m\n\u001b[32m     66\u001b[39m \u001b[33;43mSELECT\u001b[39;49m\n\u001b[32m     67\u001b[39m \u001b[33;43m  a.*,\u001b[39;49m\n\u001b[32m     68\u001b[39m \u001b[33;43m  a.temp_avg - LAG(a.temp_avg) OVER (ORDER BY OperatingDTM, Interval) AS temp_avg_ramp1,\u001b[39;49m\n\u001b[32m     69\u001b[39m \u001b[33;43m  a.hum_avg  - LAG(a.hum_avg)  OVER (ORDER BY OperatingDTM, Interval) AS hum_avg_ramp1\u001b[39;49m\n\u001b[32m     70\u001b[39m \u001b[33;43mFROM agg a;\u001b[39;49m\n\u001b[32m     71\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m con_ensure.close()\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Signed log transforms for spikes\u001b[39;00m\n",
      "\u001b[31mBinderException\u001b[39m: Binder Error: Referenced column \"temp_the_woodlands_tx\" not found in FROM clause!\nCandidate bindings: \"hist_temp_the_woodlands_tx\", \"hist_temp_friendswood_tx\", \"hist_precip_the_woodlands_tx\", \"hist_hum_the_woodlands_tx\", \"hist_wind_the_woodlands_tx\"\n\nLINE 6:     temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx...\n            ^"
     ]
    }
   ],
   "source": [
    "# ==== PyTorch seq2seq with ROBUST preprocessing (median impute + safe standardize + safe MAE) ====\n",
    "import duckdb, pandas as pd, numpy as np, datetime as dt, math\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()   # change target date if needed\n",
    "W_PAST = 168\n",
    "H_FUT = 24\n",
    "HOLDOUT_DAYS = 60\n",
    "HIDDEN = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def fit_standardizer(X2d):\n",
    "    \"\"\"Return (mean,std) with std guarded (==0 -> 1). X2d: (N, F)\"\"\"\n",
    "    mu = np.nanmean(X2d, axis=0)\n",
    "    sd = np.nanstd(X2d, axis=0)\n",
    "    sd = np.where(sd <= 1e-8, 1.0, sd)\n",
    "    return mu, sd\n",
    "\n",
    "def apply_standardizer(X, mu, sd):\n",
    "    return (X - mu) / sd\n",
    "\n",
    "# robust 3D imputer\n",
    "def impute_inplace_3d(X, med_vec):\n",
    "    \"\"\"X: (N, T, F) or (N, W, F); med_vec: (F,). Fills NaNs in-place with per-feature medians.\"\"\"\n",
    "    mask = np.isnan(X)\n",
    "    if mask.any():\n",
    "        med_broadcast = np.broadcast_to(med_vec, X.shape)\n",
    "        X[mask] = med_broadcast[mask]\n",
    "\n",
    "# sanity checker\n",
    "def assert_finite(name, arr):\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = int(np.isnan(arr).sum() + np.isinf(arr).sum())\n",
    "        raise RuntimeError(f\"{name} has {bad} non-finite values\")\n",
    "\n",
    "# ---------- ensure weather-enhanced views (hist + fcst) exist & helper transforms ----------\n",
    "con_ensure = duckdb.connect(DB)\n",
    "# Historical weather enhanced (to backfill when forecast is missing during training)\n",
    "con_ensure.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW wx_hist_enh AS\n",
    "WITH w AS (\n",
    "  SELECT\n",
    "    OperatingDTM, \"interval\" AS Interval,\n",
    "    temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx,\n",
    "    hum_the_woodlands_tx,  hum_katy_tx,  hum_friendswood_tx,  hum_baytown_tx,  hum_houston_tx\n",
    "  FROM vw_historical_weather_by_city\n",
    "),\n",
    "agg AS (\n",
    "  SELECT\n",
    "    OperatingDTM, Interval,\n",
    "    (temp_the_woodlands_tx + temp_katy_tx + temp_friendswood_tx + temp_baytown_tx + temp_houston_tx)/5.0 AS temp_avg,\n",
    "    (hum_the_woodlands_tx  + hum_katy_tx  + hum_friendswood_tx  + hum_baytown_tx  + hum_houston_tx)/5.0  AS hum_avg,\n",
    "    GREATEST(temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx)\n",
    "      - LEAST(temp_the_woodlands_tx, temp_katy_tx, temp_friendswood_tx, temp_baytown_tx, temp_houston_tx) AS temp_spread,\n",
    "    GREATEST(hum_the_woodlands_tx, hum_katy_tx, hum_friendswood_tx, hum_baytown_tx, hum_houston_tx)\n",
    "      - LEAST(hum_the_woodlands_tx, hum_katy_tx, hum_friendswood_tx, hum_baytown_tx, hum_houston_tx) AS hum_spread\n",
    "  FROM w\n",
    ")\n",
    "SELECT\n",
    "  a.*,\n",
    "  a.temp_avg - LAG(a.temp_avg) OVER (ORDER BY OperatingDTM, Interval) AS temp_avg_ramp1,\n",
    "  a.hum_avg  - LAG(a.hum_avg)  OVER (ORDER BY OperatingDTM, Interval) AS hum_avg_ramp1\n",
    "FROM agg a;\n",
    "\"\"\")\n",
    "con_ensure.close()\n",
    "\n",
    "# Signed log transforms for spikes\n",
    "sgn = np.sign\n",
    "abs_ = np.abs\n",
    "\n",
    "def sgn_log1p(y):\n",
    "    return sgn(y) * np.log1p(abs_(y))\n",
    "\n",
    "def sgn_expm1(z):\n",
    "    return sgn(z) * np.expm1(abs_(z))\n",
    "\n",
    "# Hour-of-day weights (emphasize 17-21 local hours)\n",
    "HOUR_WEIGHTS = np.ones(24, dtype=np.float32)\n",
    "HOUR_WEIGHTS[16:21] = 2.0  # intervals 17..21\n",
    "\n",
    "# ---------- pull data ----------\n",
    "con = duckdb.connect(DB)\n",
    "df_lags = con.execute(\"\"\"\n",
    "SELECT\n",
    "  ts, OperatingDTM, Interval, hb_houston,\n",
    "  p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "  dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "FROM vw_master_spine_lags\n",
    "ORDER BY ts\n",
    "\"\"\").df()\n",
    "df_lags[\"ts\"] = pd.to_datetime(df_lags[\"ts\"])\n",
    "\n",
    "df_future = con.execute(\"\"\"\n",
    "SELECT\n",
    "  f.OperatingDTM AS delivery_date,\n",
    "  f.Interval     AS delivery_interval,\n",
    "  f.hb_houston   AS target,\n",
    "  -- base future features\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "  f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f,\n",
    "  f.cal_sin_dow  AS cal_sin_dow_f,  f.cal_cos_dow  AS cal_cos_dow_f,\n",
    "  -- load-enhanced joins\n",
    "  l.net_wz, l.net_lz, l.net_wz_ramp1, l.net_lz_ramp1, l.lz_houston_ramp1, l.wz_spread, l.lz_spread,\n",
    "  -- weather: COALESCE forecast -> historical\n",
    "  COALESCE(wf.temp_avg,      wh.temp_avg)      AS temp_avg,\n",
    "  COALESCE(wf.temp_spread,   wh.temp_spread)   AS temp_spread,\n",
    "  COALESCE(wf.temp_avg_ramp1,wh.temp_avg_ramp1)AS temp_avg_ramp1,\n",
    "  COALESCE(wf.hum_avg,       wh.hum_avg)       AS hum_avg,\n",
    "  COALESCE(wf.hum_spread,    wh.hum_spread)    AS hum_spread,\n",
    "  COALESCE(wf.hum_avg_ramp1, wh.hum_avg_ramp1) AS hum_avg_ramp1,\n",
    "  CASE WHEN wf.temp_avg IS NULL THEN 1 ELSE 0 END AS is_weather_proxy\n",
    "FROM vw_master_spine_ts f\n",
    "LEFT JOIN load_fcst_enh l\n",
    "  ON l.OperatingDTM = f.OperatingDTM AND l.Interval = f.Interval\n",
    "LEFT JOIN wx_fcst_enh  wf\n",
    "  ON wf.OperatingDTM = f.OperatingDTM AND wf.Interval = f.Interval\n",
    "LEFT JOIN wx_hist_enh  wh\n",
    "  ON wh.OperatingDTM = f.OperatingDTM AND wh.Interval = f.Interval\n",
    "ORDER BY f.OperatingDTM, f.Interval\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "df_future[\"delivery_date\"] = pd.to_datetime(df_future[\"delivery_date\"]).dt.date\n",
    "df_future[\"delivery_interval\"] = df_future[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# ---------- assemble samples (day-ahead) ----------\n",
    "enc_cols = [\n",
    "    \"hb_houston\",\"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"\n",
    "]\n",
    "dec_future_cols = [\n",
    "    \"cal_hour_f\",\"cal_dow_f\",\"cal_is_weekend_f\",\"cal_sin_hour_f\",\"cal_cos_hour_f\",\"cal_sin_dow_f\",\"cal_cos_dow_f\",\n",
    "    \"net_wz\",\"net_lz\",\"net_wz_ramp1\",\"net_lz_ramp1\",\"lz_houston_ramp1\",\"wz_spread\",\"lz_spread\",\n",
    "    \"temp_avg\",\"temp_spread\",\"temp_avg_ramp1\",\"hum_avg\",\"hum_spread\",\"hum_avg_ramp1\"\n",
    "]\n",
    "\n",
    "# base rows = end-of-day with price\n",
    "base_rows = df_lags[(df_lags[\"hb_houston\"].notna()) & (df_lags[\"Interval\"]==24)][[\"ts\",\"OperatingDTM\",\"hb_houston\"]].copy()\n",
    "base_rows[\"base_date\"] = pd.to_datetime(base_rows[\"OperatingDTM\"]).dt.date\n",
    "base_rows[\"delivery_date\"] = base_rows[\"base_date\"] + dt.timedelta(days=1)\n",
    "\n",
    "# only use delivery days with 24 actuals for training/val\n",
    "has24 = df_future.groupby(\"delivery_date\")[\"target\"].apply(lambda s: s.notna().sum()==24)\n",
    "valid_delivery_dates = set(has24[has24].index)\n",
    "\n",
    "df_lags_idx = df_lags.set_index(\"ts\").sort_index()\n",
    "\n",
    "samples = []\n",
    "for _, r in base_rows.iterrows():\n",
    "    ddate = r[\"delivery_date\"]\n",
    "    ts0 = r[\"ts\"]\n",
    "    # encoder window\n",
    "    start = ts0 - pd.Timedelta(hours=W_PAST-1)\n",
    "    enc_df = df_lags_idx.loc[start:ts0][enc_cols]\n",
    "    if enc_df.shape[0] != W_PAST: \n",
    "        continue\n",
    "    if enc_df.isna().any().any():\n",
    "        continue  # strict: skip any encoder NaNs\n",
    "    # decoder future rows\n",
    "    fdf = df_future[(df_future[\"delivery_date\"]==ddate)].sort_values(\"delivery_interval\")\n",
    "    if fdf.shape[0] != H_FUT:\n",
    "        continue\n",
    "    X_enc = enc_df.values.astype(np.float32)\n",
    "    X_dec = fdf[dec_future_cols].values.astype(np.float32)\n",
    "    y     = fdf[\"target\"].values.astype(np.float32)\n",
    "    y0    = np.float32(r[\"hb_houston\"])\n",
    "    samples.append((X_enc, X_dec, y, y0, ddate))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples assembled; check data coverage or reduce W_PAST.\")\n",
    "\n",
    "# split by date\n",
    "dates = sorted({s[4] for s in samples})\n",
    "cut_date = dates[-1] - dt.timedelta(days=HOLDOUT_DAYS) if len(dates)>HOLDOUT_DAYS else dates[0]\n",
    "train_idx = [i for i,s in enumerate(samples) if s[4] <= cut_date and s[4] in valid_delivery_dates]\n",
    "val_idx   = [i for i,s in enumerate(samples) if s[4] >  cut_date and s[4] in valid_delivery_dates]\n",
    "\n",
    "def stack(idxs):\n",
    "    Xe = np.stack([samples[i][0] for i in idxs], axis=0)\n",
    "    Xd = np.stack([samples[i][1] for i in idxs], axis=0)\n",
    "    Y  = np.stack([samples[i][2] for i in idxs], axis=0)\n",
    "    Y0 = np.stack([samples[i][3] for i in idxs], axis=0)\n",
    "    return Xe, Xd, Y, Y0\n",
    "\n",
    "Xe_tr, Xd_tr, Y_tr, Y0_tr = stack(train_idx)\n",
    "Xe_va, Xd_va, Y_va, Y0_va = (None, None, None, None)\n",
    "if len(val_idx) > 0:\n",
    "    Xe_va, Xd_va, Y_va, Y0_va = stack(val_idx)\n",
    "\n",
    "# ---------- diagnose + drop all-NaN decoder features, then impute (robust) ----------\n",
    "# Flatten decoder train to 2D to inspect per-feature missingness\n",
    "F_orig = Xd_tr.shape[2]\n",
    "Xd_tr_2d_raw = Xd_tr.reshape(-1, F_orig)\n",
    "nan_counts = np.isnan(Xd_tr_2d_raw).sum(axis=0)\n",
    "all_nan_cols = nan_counts == Xd_tr_2d_raw.shape[0]\n",
    "\n",
    "if all_nan_cols.any():\n",
    "    dropped_cols = [dec_future_cols[i] for i,flag in enumerate(all_nan_cols) if flag]\n",
    "    print(f\"Dropping {int(all_nan_cols.sum())} decoder feature(s) with all-NaN in TRAIN: {dropped_cols}\")\n",
    "else:\n",
    "    dropped_cols = []\n",
    "\n",
    "# Keep mask to be reused at prediction time\n",
    "dec_keep_mask = ~all_nan_cols\n",
    "\n",
    "# Apply mask to decoder tensors\n",
    "Xd_tr = Xd_tr[:, :, dec_keep_mask]\n",
    "if Xe_va is not None:\n",
    "    Xd_va = Xd_va[:, :, dec_keep_mask]\n",
    "\n",
    "# Recompute medians on KEPT features\n",
    "dec_med = np.nanmedian(Xd_tr.reshape(-1, Xd_tr.shape[2]), axis=0)\n",
    "# Guard: if a median is still NaN for some reason, set to 0\n",
    "dec_med = np.where(np.isnan(dec_med), 0.0, dec_med)\n",
    "\n",
    "# Encoder median (rare but safe)\n",
    "enc_med = np.nanmedian(Xe_tr.reshape(-1, Xe_tr.shape[2]), axis=0)\n",
    "enc_med = np.where(np.isnan(enc_med), 0.0, enc_med)\n",
    "\n",
    "# Impute in-place (3D-safe)\n",
    "impute_inplace_3d(Xe_tr, enc_med)\n",
    "impute_inplace_3d(Xd_tr, dec_med)\n",
    "if Xe_va is not None:\n",
    "    impute_inplace_3d(Xe_va, enc_med)\n",
    "    impute_inplace_3d(Xd_va, dec_med)\n",
    "\n",
    "# Targets should be complete for training/val; assert\n",
    "assert np.isfinite(Y_tr).all(), \"Y_tr has non-finite values\"\n",
    "if Y_va is not None:\n",
    "    assert np.isfinite(Y_va).all(), \"Y_va has non-finite values\"\n",
    "\n",
    "# ---------- safe standardization ----------\n",
    "enc_mu, enc_sd = fit_standardizer(Xe_tr.reshape(-1, Xe_tr.shape[2]))\n",
    "dec_mu, dec_sd = fit_standardizer(Xd_tr.reshape(-1, Xd_tr.shape[2]))\n",
    "# ---- target transform (spike-aware) then standardize ----\n",
    "Y_tr_t  = sgn_log1p(Y_tr)\n",
    "y_mu, y_sd = fit_standardizer(Y_tr_t.reshape(-1,1))\n",
    "y_mu = float(np.atleast_1d(y_mu)[0]); y_sd = float(np.atleast_1d(y_sd)[0])\n",
    "\n",
    "Xe_tr_n = apply_standardizer(Xe_tr, enc_mu, enc_sd)\n",
    "Xd_tr_n = apply_standardizer(Xd_tr, dec_mu, dec_sd)\n",
    "Y_tr_n  = apply_standardizer(Y_tr_t, y_mu, y_sd).reshape(Y_tr.shape)\n",
    "Y0_tr_t = sgn_log1p(Y0_tr.reshape(-1,1)).reshape(-1,1)\n",
    "Y0_tr_n = apply_standardizer(Y0_tr_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = apply_standardizer(Xe_va, enc_mu, enc_sd)\n",
    "    Xd_va_n = apply_standardizer(Xd_va, dec_mu, dec_sd)\n",
    "    Y_va_t  = sgn_log1p(Y_va)\n",
    "    Y_va_n  = apply_standardizer(Y_va_t,  y_mu,  y_sd).reshape(Y_va.shape)\n",
    "    Y0_va_t = sgn_log1p(Y0_va.reshape(-1,1))\n",
    "    Y0_va_n = apply_standardizer(Y0_va_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "\n",
    "# Final safety: ensure everything is finite\n",
    "assert_finite(\"Xe_tr_n\", Xe_tr_n)\n",
    "assert_finite(\"Xd_tr_n\", Xd_tr_n)\n",
    "assert_finite(\"Y_tr_n\",  Y_tr_n)\n",
    "assert_finite(\"Y0_tr_n\", Y0_tr_n)\n",
    "if Xe_va is not None:\n",
    "    assert_finite(\"Xe_va_n\", Xe_va_n)\n",
    "    assert_finite(\"Xd_va_n\", Xd_va_n)\n",
    "    assert_finite(\"Y_va_n\",  Y_va_n)\n",
    "    assert_finite(\"Y0_va_n\", Y0_va_n)\n",
    "\n",
    "# ---------- torch datasets ----------\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xe, Xd, Y, Y0):\n",
    "        self.Xe = torch.from_numpy(Xe).float()\n",
    "        self.Xd = torch.from_numpy(Xd).float()\n",
    "        self.Y  = torch.from_numpy(Y).float()\n",
    "        self.Y0 = torch.from_numpy(Y0).float()\n",
    "    def __len__(self): return self.Xe.shape[0]\n",
    "    def __getitem__(self, i): return self.Xe[i], self.Xd[i], self.Y[i], self.Y0[i]\n",
    "\n",
    "bs = 32\n",
    "train_loader = torch.utils.data.DataLoader(Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=bs, shuffle=True)\n",
    "val_loader = None if Xe_va is None else torch.utils.data.DataLoader(Seq2SeqDataset(Xe_va_n, Xd_va_n, Y_va_n, Y0_va_n), batch_size=bs, shuffle=False)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim, hidden, batch_first=True)\n",
    "    def forward(self, x):  # x: (B,W,E)\n",
    "        _, h = self.rnn(x)\n",
    "        return h  # (1,B,H)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim + 1, hidden, batch_first=True)  # + prev y\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "    def forward(self, future_feats, h0, y0, teacher=None, tf_prob=0.5):\n",
    "        B, T, D = future_feats.shape\n",
    "        y_prev = y0.view(B,1)\n",
    "        h = h0\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            x_t = torch.cat([future_feats[:,t,:], y_prev], dim=1).unsqueeze(1)  # (B,1,D+1)\n",
    "            o, h = self.rnn(x_t, h)\n",
    "            y_t = self.out(o[:, -1, :]).squeeze(1)\n",
    "            outs.append(y_t)\n",
    "            if (teacher is not None) and (rng.random() < tf_prob):\n",
    "                y_prev = teacher[:,t].view(B,1)\n",
    "            else:\n",
    "                y_prev = y_t.view(B,1)\n",
    "        return torch.stack(outs, dim=1)  # (B,24)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(enc_in, hidden)\n",
    "        self.dec = Decoder(dec_in, hidden)\n",
    "    def forward(self, x_enc, x_dec, y0, y_teacher=None, tf_prob=0.5):\n",
    "        h = self.enc(x_enc)\n",
    "        return self.dec(x_dec, h, y0, y_teacher, tf_prob)\n",
    "\n",
    "enc_in = Xe_tr_n.shape[2]\n",
    "dec_in = Xd_tr_n.shape[2]\n",
    "model = Seq2Seq(enc_in, dec_in, HIDDEN).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "def huber_weighted(yhat, y, w, delta=1.0):\n",
    "    d = torch.abs(yhat - y)\n",
    "    q = torch.clamp(d, max=delta)\n",
    "    l = 0.5 * q**2 + delta * (d - q)\n",
    "    return (l * w).mean()\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in train_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=y, tf_prob=0.5)\n",
    "        # time-step weights (B,24)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    if val_loader is None: return None\n",
    "    model.eval()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(val_loader.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    print(f\"Epoch {ep:02d}  train_loss={tr:.4f}\" + (\"\" if va is None else f\"  val_loss={va:.4f}\"))\n",
    "\n",
    "# ---- evaluate holdout MAE in real units (SAFE) ----\n",
    "if val_loader is not None:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xenc, xdec, y, y0 in val_loader:\n",
    "            xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "            yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "            y_n    = y.detach().cpu().numpy()\n",
    "            # invert scaling + inverse spike transform\n",
    "            yhat_t = (yhat_n * y_sd + y_mu)\n",
    "            yt_t   = (y_n    * y_sd + y_mu)\n",
    "            yhat = sgn_expm1(yhat_t).reshape(-1)\n",
    "            yt   = sgn_expm1(yt_t).reshape(-1)\n",
    "            preds.append(yhat)\n",
    "            trues.append(yt)\n",
    "    preds = np.concatenate(preds)\n",
    "    trues = np.concatenate(trues)\n",
    "    mask = np.isfinite(preds) & np.isfinite(trues)\n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped:\n",
    "        print(f\"Dropped {dropped} non-finite pairs from holdout scoring.\")\n",
    "    print({\"holdout_MAE\": float(mean_absolute_error(trues[mask], preds[mask]))})\n",
    "\n",
    "# ---- predict requested DELIVERY_DATE ----\n",
    "\n",
    "# find base row (previous day EOD with price)\n",
    "prev_day = DELIVERY_DATE - dt.timedelta(days=1)\n",
    "base_row = df_lags[(pd.to_datetime(df_lags[\"OperatingDTM\"]).dt.date == prev_day) & (df_lags[\"Interval\"]==24) & df_lags[\"hb_houston\"].notna()]\n",
    "if base_row.empty:\n",
    "    raise RuntimeError(f\"No base EOD price for {prev_day}\")\n",
    "ts0 = pd.to_datetime(base_row.iloc[0][\"ts\"])\n",
    "y0  = np.float32(base_row.iloc[0][\"hb_houston\"])\n",
    "\n",
    "# encoder window\n",
    "win = df_lags.set_index(\"ts\").loc[ts0 - pd.Timedelta(hours=W_PAST-1): ts0][enc_cols]\n",
    "if win.shape[0] != W_PAST or win.isna().any().any():\n",
    "    raise RuntimeError(\"Insufficient/NaN history for encoder window.\")\n",
    "Xe_pred = win.values.astype(np.float32)[None, ...]\n",
    "\n",
    "# decoder future features (24 rows)\n",
    "frows = df_future[(df_future[\"delivery_date\"]==DELIVERY_DATE)].sort_values(\"delivery_interval\")\n",
    "if frows.shape[0] != H_FUT:\n",
    "    raise RuntimeError(\"Spine missing 24 rows for delivery date.\")\n",
    "Xd_pred_full = frows[dec_future_cols].values.astype(np.float32)[None, ...]\n",
    "\n",
    "# Apply the same keep mask used in training (drop columns that were all-NaN in train)\n",
    "if 'dec_keep_mask' in globals():\n",
    "    Xd_pred = Xd_pred_full[:, :, dec_keep_mask]\n",
    "else:\n",
    "    Xd_pred = Xd_pred_full\n",
    "\n",
    "# impute + standardize pred using TRAIN stats\n",
    "m = np.isnan(Xd_pred)\n",
    "if m.any():\n",
    "    Xd_pred[m] = np.broadcast_to(dec_med, Xd_pred.shape)[m]\n",
    "Xe_pred_n = apply_standardizer(Xe_pred, enc_mu, enc_sd)\n",
    "Xd_pred_n = apply_standardizer(Xd_pred, dec_mu, dec_sd)\n",
    "# compute normalized scalar y0_n using spike-aware transform\n",
    "y0_t = sgn_log1p(y0)\n",
    "y0_n = np.float32(((y0_t - y_mu) / y_sd)).ravel()[0]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_n = model(torch.from_numpy(Xe_pred_n).to(DEVICE),\n",
    "                   torch.from_numpy(Xd_pred_n).to(DEVICE),\n",
    "                   torch.tensor([y0_n], dtype=torch.float32, device=DEVICE),\n",
    "                   y_teacher=None, tf_prob=0.0).detach().cpu().numpy()[0]\n",
    "yhat_t = (yhat_n * y_sd + y_mu).reshape(-1)\n",
    "yhat = sgn_expm1(yhat_t)\n",
    "# replace any residual NaN with the day's median\n",
    "if np.isnan(yhat).any():\n",
    "    day_med = float(np.nanmedian(yhat))\n",
    "    yhat = np.nan_to_num(yhat, nan=day_med)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*H_FUT,\n",
    "    \"Interval\": list(range(1, H_FUT+1)),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "})\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d21584d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  train_loss=0.3078  val_loss=0.1501\n",
      "Epoch 02  train_loss=0.2225  val_loss=0.1873\n",
      "Epoch 03  train_loss=0.1700  val_loss=0.1696\n",
      "Epoch 04  train_loss=0.1443  val_loss=0.3007\n",
      "Epoch 05  train_loss=0.1277  val_loss=0.1242\n",
      "Epoch 06  train_loss=0.1166  val_loss=0.1986\n",
      "Epoch 07  train_loss=0.1162  val_loss=0.2015\n",
      "Epoch 08  train_loss=0.1009  val_loss=0.1260\n",
      "Epoch 09  train_loss=0.0988  val_loss=0.1296\n",
      "Epoch 10  train_loss=0.0936  val_loss=0.1493\n",
      "Epoch 11  train_loss=0.0932  val_loss=0.1031\n",
      "Epoch 12  train_loss=0.0966  val_loss=0.1198\n",
      "Epoch 13  train_loss=0.0889  val_loss=0.1190\n",
      "Epoch 14  train_loss=0.0875  val_loss=0.1079\n",
      "Epoch 15  train_loss=0.0814  val_loss=0.1033\n",
      "Epoch 16  train_loss=0.0795  val_loss=0.0920\n",
      "Epoch 17  train_loss=0.0810  val_loss=0.1136\n",
      "Epoch 18  train_loss=0.0793  val_loss=0.1885\n",
      "Epoch 19  train_loss=0.0782  val_loss=0.1102\n",
      "Epoch 20  train_loss=0.0785  val_loss=0.1913\n",
      "{'holdout_MAE': 10.914621353149414}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OperatingDTM</th>\n",
       "      <th>Interval</th>\n",
       "      <th>hb_houston_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>1</td>\n",
       "      <td>32.939453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>2</td>\n",
       "      <td>31.030563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>3</td>\n",
       "      <td>28.780090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>4</td>\n",
       "      <td>26.337210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>5</td>\n",
       "      <td>24.641678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>6</td>\n",
       "      <td>23.785067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>7</td>\n",
       "      <td>23.253466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>8</td>\n",
       "      <td>21.878345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>9</td>\n",
       "      <td>18.707254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>10</td>\n",
       "      <td>16.998575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>11</td>\n",
       "      <td>19.656256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>12</td>\n",
       "      <td>23.643316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>13</td>\n",
       "      <td>28.904154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>14</td>\n",
       "      <td>33.989517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>15</td>\n",
       "      <td>38.657562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>16</td>\n",
       "      <td>42.352085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>17</td>\n",
       "      <td>41.636353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>18</td>\n",
       "      <td>37.200459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>19</td>\n",
       "      <td>39.112503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>20</td>\n",
       "      <td>49.397194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>21</td>\n",
       "      <td>39.084003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>22</td>\n",
       "      <td>29.068308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>23</td>\n",
       "      <td>24.942270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>24</td>\n",
       "      <td>21.894268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OperatingDTM  Interval  hb_houston_pred\n",
       "0    2025-08-18         1        32.939453\n",
       "1    2025-08-18         2        31.030563\n",
       "2    2025-08-18         3        28.780090\n",
       "3    2025-08-18         4        26.337210\n",
       "4    2025-08-18         5        24.641678\n",
       "5    2025-08-18         6        23.785067\n",
       "6    2025-08-18         7        23.253466\n",
       "7    2025-08-18         8        21.878345\n",
       "8    2025-08-18         9        18.707254\n",
       "9    2025-08-18        10        16.998575\n",
       "10   2025-08-18        11        19.656256\n",
       "11   2025-08-18        12        23.643316\n",
       "12   2025-08-18        13        28.904154\n",
       "13   2025-08-18        14        33.989517\n",
       "14   2025-08-18        15        38.657562\n",
       "15   2025-08-18        16        42.352085\n",
       "16   2025-08-18        17        41.636353\n",
       "17   2025-08-18        18        37.200459\n",
       "18   2025-08-18        19        39.112503\n",
       "19   2025-08-18        20        49.397194\n",
       "20   2025-08-18        21        39.084003\n",
       "21   2025-08-18        22        29.068308\n",
       "22   2025-08-18        23        24.942270\n",
       "23   2025-08-18        24        21.894268"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== PyTorch seq2seq with ROBUST preprocessing (median impute + safe standardize + safe MAE) ====\n",
    "import duckdb, pandas as pd, numpy as np, datetime as dt, math\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()   # change target date if needed\n",
    "W_PAST = 168\n",
    "H_FUT = 24\n",
    "HOLDOUT_DAYS = 60\n",
    "HIDDEN = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def fit_standardizer(X2d):\n",
    "    \"\"\"Return (mean,std) with std guarded (==0 -> 1). X2d: (N, F)\"\"\"\n",
    "    mu = np.nanmean(X2d, axis=0)\n",
    "    sd = np.nanstd(X2d, axis=0)\n",
    "    sd = np.where(sd <= 1e-8, 1.0, sd)\n",
    "    return mu, sd\n",
    "\n",
    "def apply_standardizer(X, mu, sd):\n",
    "    return (X - mu) / sd\n",
    "\n",
    "# robust 3D imputer\n",
    "def impute_inplace_3d(X, med_vec):\n",
    "    \"\"\"X: (N, T, F) or (N, W, F); med_vec: (F,). Fills NaNs in-place with per-feature medians.\"\"\"\n",
    "    mask = np.isnan(X)\n",
    "    if mask.any():\n",
    "        med_broadcast = np.broadcast_to(med_vec, X.shape)\n",
    "        X[mask] = med_broadcast[mask]\n",
    "\n",
    "# sanity checker\n",
    "def assert_finite(name, arr):\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = int(np.isnan(arr).sum() + np.isinf(arr).sum())\n",
    "        raise RuntimeError(f\"{name} has {bad} non-finite values\")\n",
    "\n",
    "# ---------- ensure weather-enhanced views (hist + fcst) exist & helper transforms ----------\n",
    "con_ensure = duckdb.connect(DB)\n",
    "# Historical weather enhanced (to backfill when forecast is missing during training)\n",
    "con_ensure.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW wx_hist_enh AS\n",
    "WITH agg AS (\n",
    "  SELECT\n",
    "    OperatingDTM, \"interval\" AS Interval,\n",
    "    (hist_temp_the_woodlands_tx + hist_temp_katy_tx + hist_temp_friendswood_tx + hist_temp_baytown_tx + hist_temp_houston_tx)/5.0 AS temp_avg,\n",
    "    (hist_hum_the_woodlands_tx  + hist_hum_katy_tx  + hist_hum_friendswood_tx  + hist_hum_baytown_tx  + hist_hum_houston_tx)/5.0  AS hum_avg,\n",
    "    GREATEST(hist_temp_the_woodlands_tx, hist_temp_katy_tx, hist_temp_friendswood_tx, hist_temp_baytown_tx, hist_temp_houston_tx)\n",
    "      - LEAST(hist_temp_the_woodlands_tx, hist_temp_katy_tx, hist_temp_friendswood_tx, hist_temp_baytown_tx, hist_temp_houston_tx) AS temp_spread,\n",
    "    GREATEST(hist_hum_the_woodlands_tx, hist_hum_katy_tx, hist_hum_friendswood_tx, hist_hum_baytown_tx, hist_hum_houston_tx)\n",
    "      - LEAST(hist_hum_the_woodlands_tx, hist_hum_katy_tx, hist_hum_friendswood_tx, hist_hum_baytown_tx, hist_hum_houston_tx) AS hum_spread\n",
    "  FROM vw_historical_weather_by_city\n",
    ")\n",
    "SELECT\n",
    "  a.*,\n",
    "  a.temp_avg - LAG(a.temp_avg) OVER (ORDER BY OperatingDTM, Interval) AS temp_avg_ramp1,\n",
    "  a.hum_avg  - LAG(a.hum_avg)  OVER (ORDER BY OperatingDTM, Interval) AS hum_avg_ramp1\n",
    "FROM agg a;\n",
    "\"\"\")\n",
    "con_ensure.close()\n",
    "\n",
    "# Signed log transforms for spikes\n",
    "sgn = np.sign\n",
    "abs_ = np.abs\n",
    "\n",
    "def sgn_log1p(y):\n",
    "    return sgn(y) * np.log1p(abs_(y))\n",
    "\n",
    "def sgn_expm1(z):\n",
    "    return sgn(z) * np.expm1(abs_(z))\n",
    "\n",
    "# Hour-of-day weights (emphasize 17-21 local hours)\n",
    "HOUR_WEIGHTS = np.ones(24, dtype=np.float32)\n",
    "HOUR_WEIGHTS[16:21] = 2.0  # intervals 17..21\n",
    "\n",
    "# ---------- pull data ----------\n",
    "con = duckdb.connect(DB)\n",
    "df_lags = con.execute(\"\"\"\n",
    "SELECT\n",
    "  ts, OperatingDTM, Interval, hb_houston,\n",
    "  p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "  dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "FROM vw_master_spine_lags\n",
    "ORDER BY ts\n",
    "\"\"\").df()\n",
    "df_lags[\"ts\"] = pd.to_datetime(df_lags[\"ts\"])\n",
    "\n",
    "df_future = con.execute(\"\"\"\n",
    "SELECT\n",
    "  f.OperatingDTM AS delivery_date,\n",
    "  f.Interval     AS delivery_interval,\n",
    "  f.hb_houston   AS target,\n",
    "  -- base future features\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "  f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f,\n",
    "  f.cal_sin_dow  AS cal_sin_dow_f,  f.cal_cos_dow  AS cal_cos_dow_f,\n",
    "  -- load-enhanced joins\n",
    "  l.net_wz, l.net_lz, l.net_wz_ramp1, l.net_lz_ramp1, l.lz_houston_ramp1, l.wz_spread, l.lz_spread,\n",
    "  -- weather: COALESCE forecast -> historical\n",
    "  COALESCE(wf.temp_avg,      wh.temp_avg)      AS temp_avg,\n",
    "  COALESCE(wf.temp_spread,   wh.temp_spread)   AS temp_spread,\n",
    "  COALESCE(wf.temp_avg_ramp1,wh.temp_avg_ramp1)AS temp_avg_ramp1,\n",
    "  COALESCE(wf.hum_avg,       wh.hum_avg)       AS hum_avg,\n",
    "  COALESCE(wf.hum_spread,    wh.hum_spread)    AS hum_spread,\n",
    "  COALESCE(wf.hum_avg_ramp1, wh.hum_avg_ramp1) AS hum_avg_ramp1,\n",
    "  CASE WHEN wf.temp_avg IS NULL THEN 1 ELSE 0 END AS is_weather_proxy\n",
    "FROM vw_master_spine_ts f\n",
    "LEFT JOIN load_fcst_enh l\n",
    "  ON l.OperatingDTM = f.OperatingDTM AND l.Interval = f.Interval\n",
    "LEFT JOIN wx_fcst_enh  wf\n",
    "  ON wf.OperatingDTM = f.OperatingDTM AND wf.Interval = f.Interval\n",
    "LEFT JOIN wx_hist_enh  wh\n",
    "  ON wh.OperatingDTM = f.OperatingDTM AND wh.Interval = f.Interval\n",
    "ORDER BY f.OperatingDTM, f.Interval\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "df_future[\"delivery_date\"] = pd.to_datetime(df_future[\"delivery_date\"]).dt.date\n",
    "df_future[\"delivery_interval\"] = df_future[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# ---------- assemble samples (day-ahead) ----------\n",
    "enc_cols = [\n",
    "    \"hb_houston\",\"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"\n",
    "]\n",
    "dec_future_cols = [\n",
    "    \"cal_hour_f\",\"cal_dow_f\",\"cal_is_weekend_f\",\"cal_sin_hour_f\",\"cal_cos_hour_f\",\"cal_sin_dow_f\",\"cal_cos_dow_f\",\n",
    "    \"net_wz\",\"net_lz\",\"net_wz_ramp1\",\"net_lz_ramp1\",\"lz_houston_ramp1\",\"wz_spread\",\"lz_spread\",\n",
    "    \"temp_avg\",\"temp_spread\",\"temp_avg_ramp1\",\"hum_avg\",\"hum_spread\",\"hum_avg_ramp1\"\n",
    "]\n",
    "\n",
    "# base rows = end-of-day with price\n",
    "base_rows = df_lags[(df_lags[\"hb_houston\"].notna()) & (df_lags[\"Interval\"]==24)][[\"ts\",\"OperatingDTM\",\"hb_houston\"]].copy()\n",
    "base_rows[\"base_date\"] = pd.to_datetime(base_rows[\"OperatingDTM\"]).dt.date\n",
    "base_rows[\"delivery_date\"] = base_rows[\"base_date\"] + dt.timedelta(days=1)\n",
    "\n",
    "# only use delivery days with 24 actuals for training/val\n",
    "has24 = df_future.groupby(\"delivery_date\")[\"target\"].apply(lambda s: s.notna().sum()==24)\n",
    "valid_delivery_dates = set(has24[has24].index)\n",
    "\n",
    "df_lags_idx = df_lags.set_index(\"ts\").sort_index()\n",
    "\n",
    "samples = []\n",
    "for _, r in base_rows.iterrows():\n",
    "    ddate = r[\"delivery_date\"]\n",
    "    ts0 = r[\"ts\"]\n",
    "    # encoder window\n",
    "    start = ts0 - pd.Timedelta(hours=W_PAST-1)\n",
    "    enc_df = df_lags_idx.loc[start:ts0][enc_cols]\n",
    "    if enc_df.shape[0] != W_PAST: \n",
    "        continue\n",
    "    if enc_df.isna().any().any():\n",
    "        continue  # strict: skip any encoder NaNs\n",
    "    # decoder future rows\n",
    "    fdf = df_future[(df_future[\"delivery_date\"]==ddate)].sort_values(\"delivery_interval\")\n",
    "    if fdf.shape[0] != H_FUT:\n",
    "        continue\n",
    "    X_enc = enc_df.values.astype(np.float32)\n",
    "    X_dec = fdf[dec_future_cols].values.astype(np.float32)\n",
    "    y     = fdf[\"target\"].values.astype(np.float32)\n",
    "    y0    = np.float32(r[\"hb_houston\"])\n",
    "    samples.append((X_enc, X_dec, y, y0, ddate))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples assembled; check data coverage or reduce W_PAST.\")\n",
    "\n",
    "# split by date\n",
    "dates = sorted({s[4] for s in samples})\n",
    "cut_date = dates[-1] - dt.timedelta(days=HOLDOUT_DAYS) if len(dates)>HOLDOUT_DAYS else dates[0]\n",
    "train_idx = [i for i,s in enumerate(samples) if s[4] <= cut_date and s[4] in valid_delivery_dates]\n",
    "val_idx   = [i for i,s in enumerate(samples) if s[4] >  cut_date and s[4] in valid_delivery_dates]\n",
    "\n",
    "def stack(idxs):\n",
    "    Xe = np.stack([samples[i][0] for i in idxs], axis=0)\n",
    "    Xd = np.stack([samples[i][1] for i in idxs], axis=0)\n",
    "    Y  = np.stack([samples[i][2] for i in idxs], axis=0)\n",
    "    Y0 = np.stack([samples[i][3] for i in idxs], axis=0)\n",
    "    return Xe, Xd, Y, Y0\n",
    "\n",
    "Xe_tr, Xd_tr, Y_tr, Y0_tr = stack(train_idx)\n",
    "Xe_va, Xd_va, Y_va, Y0_va = (None, None, None, None)\n",
    "if len(val_idx) > 0:\n",
    "    Xe_va, Xd_va, Y_va, Y0_va = stack(val_idx)\n",
    "\n",
    "# ---------- diagnose + drop all-NaN decoder features, then impute (robust) ----------\n",
    "# Flatten decoder train to 2D to inspect per-feature missingness\n",
    "F_orig = Xd_tr.shape[2]\n",
    "Xd_tr_2d_raw = Xd_tr.reshape(-1, F_orig)\n",
    "nan_counts = np.isnan(Xd_tr_2d_raw).sum(axis=0)\n",
    "all_nan_cols = nan_counts == Xd_tr_2d_raw.shape[0]\n",
    "\n",
    "if all_nan_cols.any():\n",
    "    dropped_cols = [dec_future_cols[i] for i,flag in enumerate(all_nan_cols) if flag]\n",
    "    print(f\"Dropping {int(all_nan_cols.sum())} decoder feature(s) with all-NaN in TRAIN: {dropped_cols}\")\n",
    "else:\n",
    "    dropped_cols = []\n",
    "\n",
    "# Keep mask to be reused at prediction time\n",
    "dec_keep_mask = ~all_nan_cols\n",
    "\n",
    "# Apply mask to decoder tensors\n",
    "Xd_tr = Xd_tr[:, :, dec_keep_mask]\n",
    "if Xe_va is not None:\n",
    "    Xd_va = Xd_va[:, :, dec_keep_mask]\n",
    "\n",
    "# Recompute medians on KEPT features\n",
    "dec_med = np.nanmedian(Xd_tr.reshape(-1, Xd_tr.shape[2]), axis=0)\n",
    "# Guard: if a median is still NaN for some reason, set to 0\n",
    "dec_med = np.where(np.isnan(dec_med), 0.0, dec_med)\n",
    "\n",
    "# Encoder median (rare but safe)\n",
    "enc_med = np.nanmedian(Xe_tr.reshape(-1, Xe_tr.shape[2]), axis=0)\n",
    "enc_med = np.where(np.isnan(enc_med), 0.0, enc_med)\n",
    "\n",
    "# Impute in-place (3D-safe)\n",
    "impute_inplace_3d(Xe_tr, enc_med)\n",
    "impute_inplace_3d(Xd_tr, dec_med)\n",
    "if Xe_va is not None:\n",
    "    impute_inplace_3d(Xe_va, enc_med)\n",
    "    impute_inplace_3d(Xd_va, dec_med)\n",
    "\n",
    "# Targets should be complete for training/val; assert\n",
    "assert np.isfinite(Y_tr).all(), \"Y_tr has non-finite values\"\n",
    "if Y_va is not None:\n",
    "    assert np.isfinite(Y_va).all(), \"Y_va has non-finite values\"\n",
    "\n",
    "# ---------- safe standardization ----------\n",
    "enc_mu, enc_sd = fit_standardizer(Xe_tr.reshape(-1, Xe_tr.shape[2]))\n",
    "dec_mu, dec_sd = fit_standardizer(Xd_tr.reshape(-1, Xd_tr.shape[2]))\n",
    "# ---- target transform (spike-aware) then standardize ----\n",
    "Y_tr_t  = sgn_log1p(Y_tr)\n",
    "y_mu, y_sd = fit_standardizer(Y_tr_t.reshape(-1,1))\n",
    "y_mu = float(np.atleast_1d(y_mu)[0]); y_sd = float(np.atleast_1d(y_sd)[0])\n",
    "\n",
    "Xe_tr_n = apply_standardizer(Xe_tr, enc_mu, enc_sd)\n",
    "Xd_tr_n = apply_standardizer(Xd_tr, dec_mu, dec_sd)\n",
    "Y_tr_n  = apply_standardizer(Y_tr_t, y_mu, y_sd).reshape(Y_tr.shape)\n",
    "Y0_tr_t = sgn_log1p(Y0_tr.reshape(-1,1)).reshape(-1,1)\n",
    "Y0_tr_n = apply_standardizer(Y0_tr_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = apply_standardizer(Xe_va, enc_mu, enc_sd)\n",
    "    Xd_va_n = apply_standardizer(Xd_va, dec_mu, dec_sd)\n",
    "    Y_va_t  = sgn_log1p(Y_va)\n",
    "    Y_va_n  = apply_standardizer(Y_va_t,  y_mu,  y_sd).reshape(Y_va.shape)\n",
    "    Y0_va_t = sgn_log1p(Y0_va.reshape(-1,1))\n",
    "    Y0_va_n = apply_standardizer(Y0_va_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "\n",
    "# Final safety: ensure everything is finite\n",
    "assert_finite(\"Xe_tr_n\", Xe_tr_n)\n",
    "assert_finite(\"Xd_tr_n\", Xd_tr_n)\n",
    "assert_finite(\"Y_tr_n\",  Y_tr_n)\n",
    "assert_finite(\"Y0_tr_n\", Y0_tr_n)\n",
    "if Xe_va is not None:\n",
    "    assert_finite(\"Xe_va_n\", Xe_va_n)\n",
    "    assert_finite(\"Xd_va_n\", Xd_va_n)\n",
    "    assert_finite(\"Y_va_n\",  Y_va_n)\n",
    "    assert_finite(\"Y0_va_n\", Y0_va_n)\n",
    "\n",
    "# ---------- torch datasets ----------\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xe, Xd, Y, Y0):\n",
    "        self.Xe = torch.from_numpy(Xe).float()\n",
    "        self.Xd = torch.from_numpy(Xd).float()\n",
    "        self.Y  = torch.from_numpy(Y).float()\n",
    "        self.Y0 = torch.from_numpy(Y0).float()\n",
    "    def __len__(self): return self.Xe.shape[0]\n",
    "    def __getitem__(self, i): return self.Xe[i], self.Xd[i], self.Y[i], self.Y0[i]\n",
    "\n",
    "bs = 32\n",
    "train_loader = torch.utils.data.DataLoader(Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=bs, shuffle=True)\n",
    "val_loader = None if Xe_va is None else torch.utils.data.DataLoader(Seq2SeqDataset(Xe_va_n, Xd_va_n, Y_va_n, Y0_va_n), batch_size=bs, shuffle=False)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim, hidden, batch_first=True)\n",
    "    def forward(self, x):  # x: (B,W,E)\n",
    "        _, h = self.rnn(x)\n",
    "        return h  # (1,B,H)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim + 1, hidden, batch_first=True)  # + prev y\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "    def forward(self, future_feats, h0, y0, teacher=None, tf_prob=0.5):\n",
    "        B, T, D = future_feats.shape\n",
    "        y_prev = y0.view(B,1)\n",
    "        h = h0\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            x_t = torch.cat([future_feats[:,t,:], y_prev], dim=1).unsqueeze(1)  # (B,1,D+1)\n",
    "            o, h = self.rnn(x_t, h)\n",
    "            y_t = self.out(o[:, -1, :]).squeeze(1)\n",
    "            outs.append(y_t)\n",
    "            if (teacher is not None) and (rng.random() < tf_prob):\n",
    "                y_prev = teacher[:,t].view(B,1)\n",
    "            else:\n",
    "                y_prev = y_t.view(B,1)\n",
    "        return torch.stack(outs, dim=1)  # (B,24)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(enc_in, hidden)\n",
    "        self.dec = Decoder(dec_in, hidden)\n",
    "    def forward(self, x_enc, x_dec, y0, y_teacher=None, tf_prob=0.5):\n",
    "        h = self.enc(x_enc)\n",
    "        return self.dec(x_dec, h, y0, y_teacher, tf_prob)\n",
    "\n",
    "enc_in = Xe_tr_n.shape[2]\n",
    "dec_in = Xd_tr_n.shape[2]\n",
    "model = Seq2Seq(enc_in, dec_in, HIDDEN).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "def huber_weighted(yhat, y, w, delta=1.0):\n",
    "    d = torch.abs(yhat - y)\n",
    "    q = torch.clamp(d, max=delta)\n",
    "    l = 0.5 * q**2 + delta * (d - q)\n",
    "    return (l * w).mean()\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in train_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=y, tf_prob=0.5)\n",
    "        # time-step weights (B,24)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    if val_loader is None: return None\n",
    "    model.eval()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(val_loader.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    print(f\"Epoch {ep:02d}  train_loss={tr:.4f}\" + (\"\" if va is None else f\"  val_loss={va:.4f}\"))\n",
    "\n",
    "# ---- evaluate holdout MAE in real units (SAFE) ----\n",
    "if val_loader is not None:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xenc, xdec, y, y0 in val_loader:\n",
    "            xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "            yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "            y_n    = y.detach().cpu().numpy()\n",
    "            # invert scaling + inverse spike transform\n",
    "            yhat_t = (yhat_n * y_sd + y_mu)\n",
    "            yt_t   = (y_n    * y_sd + y_mu)\n",
    "            yhat = sgn_expm1(yhat_t).reshape(-1)\n",
    "            yt   = sgn_expm1(yt_t).reshape(-1)\n",
    "            preds.append(yhat)\n",
    "            trues.append(yt)\n",
    "    preds = np.concatenate(preds)\n",
    "    trues = np.concatenate(trues)\n",
    "    mask = np.isfinite(preds) & np.isfinite(trues)\n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped:\n",
    "        print(f\"Dropped {dropped} non-finite pairs from holdout scoring.\")\n",
    "    print({\"holdout_MAE\": float(mean_absolute_error(trues[mask], preds[mask]))})\n",
    "\n",
    "# ---- predict requested DELIVERY_DATE ----\n",
    "\n",
    "# find base row (previous day EOD with price)\n",
    "prev_day = DELIVERY_DATE - dt.timedelta(days=1)\n",
    "base_row = df_lags[(pd.to_datetime(df_lags[\"OperatingDTM\"]).dt.date == prev_day) & (df_lags[\"Interval\"]==24) & df_lags[\"hb_houston\"].notna()]\n",
    "if base_row.empty:\n",
    "    raise RuntimeError(f\"No base EOD price for {prev_day}\")\n",
    "ts0 = pd.to_datetime(base_row.iloc[0][\"ts\"])\n",
    "y0  = np.float32(base_row.iloc[0][\"hb_houston\"])\n",
    "\n",
    "# encoder window\n",
    "win = df_lags.set_index(\"ts\").loc[ts0 - pd.Timedelta(hours=W_PAST-1): ts0][enc_cols]\n",
    "if win.shape[0] != W_PAST or win.isna().any().any():\n",
    "    raise RuntimeError(\"Insufficient/NaN history for encoder window.\")\n",
    "Xe_pred = win.values.astype(np.float32)[None, ...]\n",
    "\n",
    "# decoder future features (24 rows)\n",
    "frows = df_future[(df_future[\"delivery_date\"]==DELIVERY_DATE)].sort_values(\"delivery_interval\")\n",
    "if frows.shape[0] != H_FUT:\n",
    "    raise RuntimeError(\"Spine missing 24 rows for delivery date.\")\n",
    "Xd_pred_full = frows[dec_future_cols].values.astype(np.float32)[None, ...]\n",
    "\n",
    "# Apply the same keep mask used in training (drop columns that were all-NaN in train)\n",
    "if 'dec_keep_mask' in globals():\n",
    "    Xd_pred = Xd_pred_full[:, :, dec_keep_mask]\n",
    "else:\n",
    "    Xd_pred = Xd_pred_full\n",
    "\n",
    "# impute + standardize pred using TRAIN stats\n",
    "m = np.isnan(Xd_pred)\n",
    "if m.any():\n",
    "    Xd_pred[m] = np.broadcast_to(dec_med, Xd_pred.shape)[m]\n",
    "Xe_pred_n = apply_standardizer(Xe_pred, enc_mu, enc_sd)\n",
    "Xd_pred_n = apply_standardizer(Xd_pred, dec_mu, dec_sd)\n",
    "# compute normalized scalar y0_n using spike-aware transform\n",
    "y0_t = sgn_log1p(y0)\n",
    "y0_n = np.float32(((y0_t - y_mu) / y_sd)).ravel()[0]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_n = model(torch.from_numpy(Xe_pred_n).to(DEVICE),\n",
    "                   torch.from_numpy(Xd_pred_n).to(DEVICE),\n",
    "                   torch.tensor([y0_n], dtype=torch.float32, device=DEVICE),\n",
    "                   y_teacher=None, tf_prob=0.0).detach().cpu().numpy()[0]\n",
    "yhat_t = (yhat_n * y_sd + y_mu).reshape(-1)\n",
    "yhat = sgn_expm1(yhat_t)\n",
    "# replace any residual NaN with the day's median\n",
    "if np.isnan(yhat).any():\n",
    "    day_med = float(np.nanmedian(yhat))\n",
    "    yhat = np.nan_to_num(yhat, nan=day_med)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*H_FUT,\n",
    "    \"Interval\": list(range(1, H_FUT+1)),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "})\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd4fd6",
   "metadata": {},
   "source": [
    "Peak-hour weighting bump\n",
    "Weighted-Huber now uses 3× weight for intervals 17–21 (late-day surge). This pulls the model to fit the ramp more aggressively.\n",
    "\n",
    "New ramp & day-over-day features (no schema changes needed):\n",
    "In the df_future query, added window-derived drivers that are super predictive for peaks:\n",
    "\n",
    "net_wz_ramp3, net_wz_ramp6, net_lz_ramp3, net_lz_ramp6\n",
    "\n",
    "lz_houston_ramp3, lz_houston_ramp6\n",
    "\n",
    "net_wz_dod, net_lz_dod, lz_houston_dod (value − value 24 hours ago)\n",
    "\n",
    "These are computed inline with LAG(…,3/6/24) over (ORDER BY OperatingDTM, Interval) and wired into dec_future_cols. No extra views needed.\n",
    "\n",
    "Weather backfill stays on\n",
    "We keep COALESCE(forecast→historical) and the spike-aware target transform. (Key in getting to to 10.9 MAE.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83a808d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  train_loss=0.3803  val_loss=0.2468\n",
      "Epoch 02  train_loss=0.2899  val_loss=0.3045\n",
      "Epoch 03  train_loss=0.2260  val_loss=0.2425\n",
      "Epoch 04  train_loss=0.1887  val_loss=0.1840\n",
      "Epoch 05  train_loss=0.1600  val_loss=0.2060\n",
      "Epoch 06  train_loss=0.1455  val_loss=0.1757\n",
      "Epoch 07  train_loss=0.1401  val_loss=0.3374\n",
      "Epoch 08  train_loss=0.1185  val_loss=0.3643\n",
      "Epoch 09  train_loss=0.1192  val_loss=0.2190\n",
      "Epoch 10  train_loss=0.1121  val_loss=0.1660\n",
      "Epoch 11  train_loss=0.1226  val_loss=0.3145\n",
      "Epoch 12  train_loss=0.1163  val_loss=0.2120\n",
      "Epoch 13  train_loss=0.1181  val_loss=0.3052\n",
      "Epoch 14  train_loss=0.1066  val_loss=0.1803\n",
      "Epoch 15  train_loss=0.1021  val_loss=0.1534\n",
      "Epoch 16  train_loss=0.0991  val_loss=0.2331\n",
      "Epoch 17  train_loss=0.1006  val_loss=0.1326\n",
      "Epoch 18  train_loss=0.0953  val_loss=0.1384\n",
      "Epoch 19  train_loss=0.0970  val_loss=0.1241\n",
      "Epoch 20  train_loss=0.0966  val_loss=0.1566\n",
      "{'holdout_MAE': 10.522457122802734}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OperatingDTM</th>\n",
       "      <th>Interval</th>\n",
       "      <th>hb_houston_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>1</td>\n",
       "      <td>36.066525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>2</td>\n",
       "      <td>36.127316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>3</td>\n",
       "      <td>33.805424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>4</td>\n",
       "      <td>31.458508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>5</td>\n",
       "      <td>30.529922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>6</td>\n",
       "      <td>30.530283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>7</td>\n",
       "      <td>29.699850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>8</td>\n",
       "      <td>26.760725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>9</td>\n",
       "      <td>23.487507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>10</td>\n",
       "      <td>21.850462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>11</td>\n",
       "      <td>27.643070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>12</td>\n",
       "      <td>37.211399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>13</td>\n",
       "      <td>50.817513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>14</td>\n",
       "      <td>69.760719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>15</td>\n",
       "      <td>95.352303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>16</td>\n",
       "      <td>122.388351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>17</td>\n",
       "      <td>118.924271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>18</td>\n",
       "      <td>103.545654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>19</td>\n",
       "      <td>111.733551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>20</td>\n",
       "      <td>135.197937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>21</td>\n",
       "      <td>73.977982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>22</td>\n",
       "      <td>42.013660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>23</td>\n",
       "      <td>31.271740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>24</td>\n",
       "      <td>25.191139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OperatingDTM  Interval  hb_houston_pred\n",
       "0    2025-08-18         1        36.066525\n",
       "1    2025-08-18         2        36.127316\n",
       "2    2025-08-18         3        33.805424\n",
       "3    2025-08-18         4        31.458508\n",
       "4    2025-08-18         5        30.529922\n",
       "5    2025-08-18         6        30.530283\n",
       "6    2025-08-18         7        29.699850\n",
       "7    2025-08-18         8        26.760725\n",
       "8    2025-08-18         9        23.487507\n",
       "9    2025-08-18        10        21.850462\n",
       "10   2025-08-18        11        27.643070\n",
       "11   2025-08-18        12        37.211399\n",
       "12   2025-08-18        13        50.817513\n",
       "13   2025-08-18        14        69.760719\n",
       "14   2025-08-18        15        95.352303\n",
       "15   2025-08-18        16       122.388351\n",
       "16   2025-08-18        17       118.924271\n",
       "17   2025-08-18        18       103.545654\n",
       "18   2025-08-18        19       111.733551\n",
       "19   2025-08-18        20       135.197937\n",
       "20   2025-08-18        21        73.977982\n",
       "21   2025-08-18        22        42.013660\n",
       "22   2025-08-18        23        31.271740\n",
       "23   2025-08-18        24        25.191139"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== PyTorch seq2seq with ROBUST preprocessing (median impute + safe standardize + safe MAE) ====\n",
    "import duckdb, pandas as pd, numpy as np, datetime as dt, math\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()   # change target date if needed\n",
    "W_PAST = 168\n",
    "H_FUT = 24\n",
    "HOLDOUT_DAYS = 60\n",
    "HIDDEN = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def fit_standardizer(X2d):\n",
    "    \"\"\"Return (mean,std) with std guarded (==0 -> 1). X2d: (N, F)\"\"\"\n",
    "    mu = np.nanmean(X2d, axis=0)\n",
    "    sd = np.nanstd(X2d, axis=0)\n",
    "    sd = np.where(sd <= 1e-8, 1.0, sd)\n",
    "    return mu, sd\n",
    "\n",
    "def apply_standardizer(X, mu, sd):\n",
    "    return (X - mu) / sd\n",
    "\n",
    "# robust 3D imputer\n",
    "def impute_inplace_3d(X, med_vec):\n",
    "    \"\"\"X: (N, T, F) or (N, W, F); med_vec: (F,). Fills NaNs in-place with per-feature medians.\"\"\"\n",
    "    mask = np.isnan(X)\n",
    "    if mask.any():\n",
    "        med_broadcast = np.broadcast_to(med_vec, X.shape)\n",
    "        X[mask] = med_broadcast[mask]\n",
    "\n",
    "# sanity checker\n",
    "def assert_finite(name, arr):\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = int(np.isnan(arr).sum() + np.isinf(arr).sum())\n",
    "        raise RuntimeError(f\"{name} has {bad} non-finite values\")\n",
    "\n",
    "# ---------- ensure weather-enhanced views (hist + fcst) exist & helper transforms ----------\n",
    "con_ensure = duckdb.connect(DB)\n",
    "# Historical weather enhanced (to backfill when forecast is missing during training)\n",
    "con_ensure.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW wx_hist_enh AS\n",
    "WITH agg AS (\n",
    "  SELECT\n",
    "    OperatingDTM, \"interval\" AS Interval,\n",
    "    (hist_temp_the_woodlands_tx + hist_temp_katy_tx + hist_temp_friendswood_tx + hist_temp_baytown_tx + hist_temp_houston_tx)/5.0 AS temp_avg,\n",
    "    (hist_hum_the_woodlands_tx  + hist_hum_katy_tx  + hist_hum_friendswood_tx  + hist_hum_baytown_tx  + hist_hum_houston_tx)/5.0  AS hum_avg,\n",
    "    GREATEST(hist_temp_the_woodlands_tx, hist_temp_katy_tx, hist_temp_friendswood_tx, hist_temp_baytown_tx, hist_temp_houston_tx)\n",
    "      - LEAST(hist_temp_the_woodlands_tx, hist_temp_katy_tx, hist_temp_friendswood_tx, hist_temp_baytown_tx, hist_temp_houston_tx) AS temp_spread,\n",
    "    GREATEST(hist_hum_the_woodlands_tx, hist_hum_katy_tx, hist_hum_friendswood_tx, hist_hum_baytown_tx, hist_hum_houston_tx)\n",
    "      - LEAST(hist_hum_the_woodlands_tx, hist_hum_katy_tx, hist_hum_friendswood_tx, hist_hum_baytown_tx, hist_hum_houston_tx) AS hum_spread\n",
    "  FROM vw_historical_weather_by_city\n",
    ")\n",
    "SELECT\n",
    "  a.*,\n",
    "  a.temp_avg - LAG(a.temp_avg) OVER (ORDER BY OperatingDTM, Interval) AS temp_avg_ramp1,\n",
    "  a.hum_avg  - LAG(a.hum_avg)  OVER (ORDER BY OperatingDTM, Interval) AS hum_avg_ramp1\n",
    "FROM agg a;\n",
    "\"\"\")\n",
    "con_ensure.close()\n",
    "\n",
    "# Signed log transforms for spikes\n",
    "sgn = np.sign\n",
    "abs_ = np.abs\n",
    "\n",
    "def sgn_log1p(y):\n",
    "    return sgn(y) * np.log1p(abs_(y))\n",
    "\n",
    "def sgn_expm1(z):\n",
    "    return sgn(z) * np.expm1(abs_(z))\n",
    "\n",
    "# Hour-of-day weights (emphasize 17-21 local hours)\n",
    "HOUR_WEIGHTS = np.ones(24, dtype=np.float32)\n",
    "# Emphasize 17..21 (late-day surge hours) a bit more\n",
    "HOUR_WEIGHTS[16:21] = 3.0  # intervals 17..21\n",
    "\n",
    "# ---------- pull data ----------\n",
    "con = duckdb.connect(DB)\n",
    "df_lags = con.execute(\"\"\"\n",
    "SELECT\n",
    "  ts, OperatingDTM, Interval, hb_houston,\n",
    "  p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "  dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "FROM vw_master_spine_lags\n",
    "ORDER BY ts\n",
    "\"\"\").df()\n",
    "df_lags[\"ts\"] = pd.to_datetime(df_lags[\"ts\"])\n",
    "\n",
    "df_future = con.execute(\"\"\"\n",
    "SELECT\n",
    "  f.OperatingDTM AS delivery_date,\n",
    "  f.Interval     AS delivery_interval,\n",
    "  f.hb_houston   AS target,\n",
    "  -- base future features\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "  f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f,\n",
    "  f.cal_sin_dow  AS cal_sin_dow_f,  f.cal_cos_dow  AS cal_cos_dow_f,\n",
    "  -- load-enhanced joins\n",
    "  l.net_wz, l.net_lz, l.net_wz_ramp1, l.net_lz_ramp1, l.lz_houston_ramp1, l.wz_spread, l.lz_spread,\n",
    "  -- weather: COALESCE forecast -> historical\n",
    "  COALESCE(wf.temp_avg,      wh.temp_avg)      AS temp_avg,\n",
    "  COALESCE(wf.temp_spread,   wh.temp_spread)   AS temp_spread,\n",
    "  COALESCE(wf.temp_avg_ramp1,wh.temp_avg_ramp1)AS temp_avg_ramp1,\n",
    "  COALESCE(wf.hum_avg,       wh.hum_avg)       AS hum_avg,\n",
    "  COALESCE(wf.hum_spread,    wh.hum_spread)    AS hum_spread,\n",
    "  COALESCE(wf.hum_avg_ramp1, wh.hum_avg_ramp1) AS hum_avg_ramp1,\n",
    "  CASE WHEN wf.temp_avg IS NULL THEN 1 ELSE 0 END AS is_weather_proxy,\n",
    "  -- additional ramps & day-over-day deltas to help with peaks (computed over time order)\n",
    "  (l.net_wz      - LAG(l.net_wz,      3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_ramp3,\n",
    "  (l.net_wz      - LAG(l.net_wz,      6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_ramp6,\n",
    "  (l.net_lz      - LAG(l.net_lz,      3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_ramp3,\n",
    "  (l.net_lz      - LAG(l.net_lz,      6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_ramp6,\n",
    "  (l.lz_houston  - LAG(l.lz_houston,  3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_ramp3,\n",
    "  (l.lz_houston  - LAG(l.lz_houston,  6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_ramp6,\n",
    "  (l.net_wz      - LAG(l.net_wz,     24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_dod,\n",
    "  (l.net_lz      - LAG(l.net_lz,     24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_dod,\n",
    "  (l.lz_houston  - LAG(l.lz_houston, 24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_dod\n",
    "FROM vw_master_spine_ts f\n",
    "LEFT JOIN load_fcst_enh l\n",
    "  ON l.OperatingDTM = f.OperatingDTM AND l.Interval = f.Interval\n",
    "LEFT JOIN wx_fcst_enh  wf\n",
    "  ON wf.OperatingDTM = f.OperatingDTM AND wf.Interval = f.Interval\n",
    "LEFT JOIN wx_hist_enh  wh\n",
    "  ON wh.OperatingDTM = f.OperatingDTM AND wh.Interval = f.Interval\n",
    "ORDER BY f.OperatingDTM, f.Interval\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "df_future[\"delivery_date\"] = pd.to_datetime(df_future[\"delivery_date\"]).dt.date\n",
    "df_future[\"delivery_interval\"] = df_future[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# ---------- assemble samples (day-ahead) ----------\n",
    "enc_cols = [\n",
    "    \"hb_houston\",\"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"\n",
    "]\n",
    "dec_future_cols = [\n",
    "    \"cal_hour_f\",\"cal_dow_f\",\"cal_is_weekend_f\",\"cal_sin_hour_f\",\"cal_cos_hour_f\",\"cal_sin_dow_f\",\"cal_cos_dow_f\",\n",
    "    \"net_wz\",\"net_lz\",\"net_wz_ramp1\",\"net_lz_ramp1\",\"lz_houston_ramp1\",\"wz_spread\",\"lz_spread\",\n",
    "    \"temp_avg\",\"temp_spread\",\"temp_avg_ramp1\",\"hum_avg\",\"hum_spread\",\"hum_avg_ramp1\",\n",
    "    # NEW: multi-horizon ramps + day-over-day deltas\n",
    "    \"net_wz_ramp3\",\"net_wz_ramp6\",\"net_lz_ramp3\",\"net_lz_ramp6\",\n",
    "    \"lz_houston_ramp3\",\"lz_houston_ramp6\",\"net_wz_dod\",\"net_lz_dod\",\"lz_houston_dod\"\n",
    "]\n",
    "\n",
    "# base rows = end-of-day with price\n",
    "base_rows = df_lags[(df_lags[\"hb_houston\"].notna()) & (df_lags[\"Interval\"]==24)][[\"ts\",\"OperatingDTM\",\"hb_houston\"]].copy()\n",
    "base_rows[\"base_date\"] = pd.to_datetime(base_rows[\"OperatingDTM\"]).dt.date\n",
    "base_rows[\"delivery_date\"] = base_rows[\"base_date\"] + dt.timedelta(days=1)\n",
    "\n",
    "# only use delivery days with 24 actuals for training/val\n",
    "has24 = df_future.groupby(\"delivery_date\")[\"target\"].apply(lambda s: s.notna().sum()==24)\n",
    "valid_delivery_dates = set(has24[has24].index)\n",
    "\n",
    "df_lags_idx = df_lags.set_index(\"ts\").sort_index()\n",
    "\n",
    "samples = []\n",
    "for _, r in base_rows.iterrows():\n",
    "    ddate = r[\"delivery_date\"]\n",
    "    ts0 = r[\"ts\"]\n",
    "    # encoder window\n",
    "    start = ts0 - pd.Timedelta(hours=W_PAST-1)\n",
    "    enc_df = df_lags_idx.loc[start:ts0][enc_cols]\n",
    "    if enc_df.shape[0] != W_PAST: \n",
    "        continue\n",
    "    if enc_df.isna().any().any():\n",
    "        continue  # strict: skip any encoder NaNs\n",
    "    # decoder future rows\n",
    "    fdf = df_future[(df_future[\"delivery_date\"]==ddate)].sort_values(\"delivery_interval\")\n",
    "    if fdf.shape[0] != H_FUT:\n",
    "        continue\n",
    "    X_enc = enc_df.values.astype(np.float32)\n",
    "    X_dec = fdf[dec_future_cols].values.astype(np.float32)\n",
    "    y     = fdf[\"target\"].values.astype(np.float32)\n",
    "    y0    = np.float32(r[\"hb_houston\"])\n",
    "    samples.append((X_enc, X_dec, y, y0, ddate))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples assembled; check data coverage or reduce W_PAST.\")\n",
    "\n",
    "# split by date\n",
    "dates = sorted({s[4] for s in samples})\n",
    "cut_date = dates[-1] - dt.timedelta(days=HOLDOUT_DAYS) if len(dates)>HOLDOUT_DAYS else dates[0]\n",
    "train_idx = [i for i,s in enumerate(samples) if s[4] <= cut_date and s[4] in valid_delivery_dates]\n",
    "val_idx   = [i for i,s in enumerate(samples) if s[4] >  cut_date and s[4] in valid_delivery_dates]\n",
    "\n",
    "def stack(idxs):\n",
    "    Xe = np.stack([samples[i][0] for i in idxs], axis=0)\n",
    "    Xd = np.stack([samples[i][1] for i in idxs], axis=0)\n",
    "    Y  = np.stack([samples[i][2] for i in idxs], axis=0)\n",
    "    Y0 = np.stack([samples[i][3] for i in idxs], axis=0)\n",
    "    return Xe, Xd, Y, Y0\n",
    "\n",
    "Xe_tr, Xd_tr, Y_tr, Y0_tr = stack(train_idx)\n",
    "Xe_va, Xd_va, Y_va, Y0_va = (None, None, None, None)\n",
    "if len(val_idx) > 0:\n",
    "    Xe_va, Xd_va, Y_va, Y0_va = stack(val_idx)\n",
    "\n",
    "# ---------- diagnose + drop all-NaN decoder features, then impute (robust) ----------\n",
    "# Flatten decoder train to 2D to inspect per-feature missingness\n",
    "F_orig = Xd_tr.shape[2]\n",
    "Xd_tr_2d_raw = Xd_tr.reshape(-1, F_orig)\n",
    "nan_counts = np.isnan(Xd_tr_2d_raw).sum(axis=0)\n",
    "all_nan_cols = nan_counts == Xd_tr_2d_raw.shape[0]\n",
    "\n",
    "if all_nan_cols.any():\n",
    "    dropped_cols = [dec_future_cols[i] for i,flag in enumerate(all_nan_cols) if flag]\n",
    "    print(f\"Dropping {int(all_nan_cols.sum())} decoder feature(s) with all-NaN in TRAIN: {dropped_cols}\")\n",
    "else:\n",
    "    dropped_cols = []\n",
    "\n",
    "# Keep mask to be reused at prediction time\n",
    "dec_keep_mask = ~all_nan_cols\n",
    "\n",
    "# Apply mask to decoder tensors\n",
    "Xd_tr = Xd_tr[:, :, dec_keep_mask]\n",
    "if Xe_va is not None:\n",
    "    Xd_va = Xd_va[:, :, dec_keep_mask]\n",
    "\n",
    "# Recompute medians on KEPT features\n",
    "dec_med = np.nanmedian(Xd_tr.reshape(-1, Xd_tr.shape[2]), axis=0)\n",
    "# Guard: if a median is still NaN for some reason, set to 0\n",
    "dec_med = np.where(np.isnan(dec_med), 0.0, dec_med)\n",
    "\n",
    "# Encoder median (rare but safe)\n",
    "enc_med = np.nanmedian(Xe_tr.reshape(-1, Xe_tr.shape[2]), axis=0)\n",
    "enc_med = np.where(np.isnan(enc_med), 0.0, enc_med)\n",
    "\n",
    "# Impute in-place (3D-safe)\n",
    "impute_inplace_3d(Xe_tr, enc_med)\n",
    "impute_inplace_3d(Xd_tr, dec_med)\n",
    "if Xe_va is not None:\n",
    "    impute_inplace_3d(Xe_va, enc_med)\n",
    "    impute_inplace_3d(Xd_va, dec_med)\n",
    "\n",
    "# Targets should be complete for training/val; assert\n",
    "assert np.isfinite(Y_tr).all(), \"Y_tr has non-finite values\"\n",
    "if Y_va is not None:\n",
    "    assert np.isfinite(Y_va).all(), \"Y_va has non-finite values\"\n",
    "\n",
    "# ---------- safe standardization ----------\n",
    "enc_mu, enc_sd = fit_standardizer(Xe_tr.reshape(-1, Xe_tr.shape[2]))\n",
    "dec_mu, dec_sd = fit_standardizer(Xd_tr.reshape(-1, Xd_tr.shape[2]))\n",
    "# ---- target transform (spike-aware) then standardize ----\n",
    "Y_tr_t  = sgn_log1p(Y_tr)\n",
    "y_mu, y_sd = fit_standardizer(Y_tr_t.reshape(-1,1))\n",
    "y_mu = float(np.atleast_1d(y_mu)[0]); y_sd = float(np.atleast_1d(y_sd)[0])\n",
    "\n",
    "Xe_tr_n = apply_standardizer(Xe_tr, enc_mu, enc_sd)\n",
    "Xd_tr_n = apply_standardizer(Xd_tr, dec_mu, dec_sd)\n",
    "Y_tr_n  = apply_standardizer(Y_tr_t, y_mu, y_sd).reshape(Y_tr.shape)\n",
    "Y0_tr_t = sgn_log1p(Y0_tr.reshape(-1,1)).reshape(-1,1)\n",
    "Y0_tr_n = apply_standardizer(Y0_tr_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = apply_standardizer(Xe_va, enc_mu, enc_sd)\n",
    "    Xd_va_n = apply_standardizer(Xd_va, dec_mu, dec_sd)\n",
    "    Y_va_t  = sgn_log1p(Y_va)\n",
    "    Y_va_n  = apply_standardizer(Y_va_t,  y_mu,  y_sd).reshape(Y_va.shape)\n",
    "    Y0_va_t = sgn_log1p(Y0_va.reshape(-1,1))\n",
    "    Y0_va_n = apply_standardizer(Y0_va_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "\n",
    "# Final safety: ensure everything is finite\n",
    "assert_finite(\"Xe_tr_n\", Xe_tr_n)\n",
    "assert_finite(\"Xd_tr_n\", Xd_tr_n)\n",
    "assert_finite(\"Y_tr_n\",  Y_tr_n)\n",
    "assert_finite(\"Y0_tr_n\", Y0_tr_n)\n",
    "if Xe_va is not None:\n",
    "    assert_finite(\"Xe_va_n\", Xe_va_n)\n",
    "    assert_finite(\"Xd_va_n\", Xd_va_n)\n",
    "    assert_finite(\"Y_va_n\",  Y_va_n)\n",
    "    assert_finite(\"Y0_va_n\", Y0_va_n)\n",
    "\n",
    "# ---------- torch datasets ----------\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xe, Xd, Y, Y0):\n",
    "        self.Xe = torch.from_numpy(Xe).float()\n",
    "        self.Xd = torch.from_numpy(Xd).float()\n",
    "        self.Y  = torch.from_numpy(Y).float()\n",
    "        self.Y0 = torch.from_numpy(Y0).float()\n",
    "    def __len__(self): return self.Xe.shape[0]\n",
    "    def __getitem__(self, i): return self.Xe[i], self.Xd[i], self.Y[i], self.Y0[i]\n",
    "\n",
    "bs = 32\n",
    "train_loader = torch.utils.data.DataLoader(Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=bs, shuffle=True)\n",
    "val_loader = None if Xe_va is None else torch.utils.data.DataLoader(Seq2SeqDataset(Xe_va_n, Xd_va_n, Y_va_n, Y0_va_n), batch_size=bs, shuffle=False)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim, hidden, batch_first=True)\n",
    "    def forward(self, x):  # x: (B,W,E)\n",
    "        _, h = self.rnn(x)\n",
    "        return h  # (1,B,H)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim + 1, hidden, batch_first=True)  # + prev y\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "    def forward(self, future_feats, h0, y0, teacher=None, tf_prob=0.5):\n",
    "        B, T, D = future_feats.shape\n",
    "        y_prev = y0.view(B,1)\n",
    "        h = h0\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            x_t = torch.cat([future_feats[:,t,:], y_prev], dim=1).unsqueeze(1)  # (B,1,D+1)\n",
    "            o, h = self.rnn(x_t, h)\n",
    "            y_t = self.out(o[:, -1, :]).squeeze(1)\n",
    "            outs.append(y_t)\n",
    "            if (teacher is not None) and (rng.random() < tf_prob):\n",
    "                y_prev = teacher[:,t].view(B,1)\n",
    "            else:\n",
    "                y_prev = y_t.view(B,1)\n",
    "        return torch.stack(outs, dim=1)  # (B,24)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(enc_in, hidden)\n",
    "        self.dec = Decoder(dec_in, hidden)\n",
    "    def forward(self, x_enc, x_dec, y0, y_teacher=None, tf_prob=0.5):\n",
    "        h = self.enc(x_enc)\n",
    "        return self.dec(x_dec, h, y0, y_teacher, tf_prob)\n",
    "\n",
    "enc_in = Xe_tr_n.shape[2]\n",
    "dec_in = Xd_tr_n.shape[2]\n",
    "model = Seq2Seq(enc_in, dec_in, HIDDEN).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "def huber_weighted(yhat, y, w, delta=1.0):\n",
    "    d = torch.abs(yhat - y)\n",
    "    q = torch.clamp(d, max=delta)\n",
    "    l = 0.5 * q**2 + delta * (d - q)\n",
    "    return (l * w).mean()\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in train_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=y, tf_prob=0.5)\n",
    "        # time-step weights (B,24)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    if val_loader is None: return None\n",
    "    model.eval()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(val_loader.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    print(f\"Epoch {ep:02d}  train_loss={tr:.4f}\" + (\"\" if va is None else f\"  val_loss={va:.4f}\"))\n",
    "\n",
    "# ---- evaluate holdout MAE in real units (SAFE) ----\n",
    "if val_loader is not None:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xenc, xdec, y, y0 in val_loader:\n",
    "            xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "            yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "            y_n    = y.detach().cpu().numpy()\n",
    "            # invert scaling + inverse spike transform\n",
    "            yhat_t = (yhat_n * y_sd + y_mu)\n",
    "            yt_t   = (y_n    * y_sd + y_mu)\n",
    "            yhat = sgn_expm1(yhat_t).reshape(-1)\n",
    "            yt   = sgn_expm1(yt_t).reshape(-1)\n",
    "            preds.append(yhat)\n",
    "            trues.append(yt)\n",
    "    preds = np.concatenate(preds)\n",
    "    trues = np.concatenate(trues)\n",
    "    mask = np.isfinite(preds) & np.isfinite(trues)\n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped:\n",
    "        print(f\"Dropped {dropped} non-finite pairs from holdout scoring.\")\n",
    "    print({\"holdout_MAE\": float(mean_absolute_error(trues[mask], preds[mask]))})\n",
    "\n",
    "# ---- predict requested DELIVERY_DATE ----\n",
    "\n",
    "# find base row (previous day EOD with price)\n",
    "prev_day = DELIVERY_DATE - dt.timedelta(days=1)\n",
    "base_row = df_lags[(pd.to_datetime(df_lags[\"OperatingDTM\"]).dt.date == prev_day) & (df_lags[\"Interval\"]==24) & df_lags[\"hb_houston\"].notna()]\n",
    "if base_row.empty:\n",
    "    raise RuntimeError(f\"No base EOD price for {prev_day}\")\n",
    "ts0 = pd.to_datetime(base_row.iloc[0][\"ts\"])\n",
    "y0  = np.float32(base_row.iloc[0][\"hb_houston\"])\n",
    "\n",
    "# encoder window\n",
    "win = df_lags.set_index(\"ts\").loc[ts0 - pd.Timedelta(hours=W_PAST-1): ts0][enc_cols]\n",
    "if win.shape[0] != W_PAST or win.isna().any().any():\n",
    "    raise RuntimeError(\"Insufficient/NaN history for encoder window.\")\n",
    "Xe_pred = win.values.astype(np.float32)[None, ...]\n",
    "\n",
    "# decoder future features (24 rows)\n",
    "frows = df_future[(df_future[\"delivery_date\"]==DELIVERY_DATE)].sort_values(\"delivery_interval\")\n",
    "if frows.shape[0] != H_FUT:\n",
    "    raise RuntimeError(\"Spine missing 24 rows for delivery date.\")\n",
    "Xd_pred_full = frows[dec_future_cols].values.astype(np.float32)[None, ...]\n",
    "\n",
    "# Apply the same keep mask used in training (drop columns that were all-NaN in train)\n",
    "if 'dec_keep_mask' in globals():\n",
    "    Xd_pred = Xd_pred_full[:, :, dec_keep_mask]\n",
    "else:\n",
    "    Xd_pred = Xd_pred_full\n",
    "\n",
    "# impute + standardize pred using TRAIN stats\n",
    "m = np.isnan(Xd_pred)\n",
    "if m.any():\n",
    "    Xd_pred[m] = np.broadcast_to(dec_med, Xd_pred.shape)[m]\n",
    "Xe_pred_n = apply_standardizer(Xe_pred, enc_mu, enc_sd)\n",
    "Xd_pred_n = apply_standardizer(Xd_pred, dec_mu, dec_sd)\n",
    "# compute normalized scalar y0_n using spike-aware transform\n",
    "y0_t = sgn_log1p(y0)\n",
    "y0_n = np.float32(((y0_t - y_mu) / y_sd)).ravel()[0]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_n = model(torch.from_numpy(Xe_pred_n).to(DEVICE),\n",
    "                   torch.from_numpy(Xd_pred_n).to(DEVICE),\n",
    "                   torch.tensor([y0_n], dtype=torch.float32, device=DEVICE),\n",
    "                   y_teacher=None, tf_prob=0.0).detach().cpu().numpy()[0]\n",
    "yhat_t = (yhat_n * y_sd + y_mu).reshape(-1)\n",
    "yhat = sgn_expm1(yhat_t)\n",
    "# replace any residual NaN with the day's median\n",
    "if np.isnan(yhat).any():\n",
    "    day_med = float(np.nanmedian(yhat))\n",
    "    yhat = np.nan_to_num(yhat, nan=day_med)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*H_FUT,\n",
    "    \"Interval\": list(range(1, H_FUT+1)),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "})\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57557f0f",
   "metadata": {},
   "source": [
    " holdout MAE improved again ($10.52 vs ~10.9 before), which is a good sign.\n",
    "\n",
    "The shape now captures the afternoon ramp and a sizeable evening surge.\n",
    "\n",
    "What’s left: it still ramps a bit too early ( H16 122 vs actual ~66) and still underestimates the peak top (H19–H21 predicted ~112/135/74 vs actual ~224/216/135).\n",
    "\n",
    "Quick read by key hours\n",
    "\n",
    "    Early morning (H1–H10): within single-digit dollars most hours \n",
    "\n",
    "    Midday (H12–H16): over (e.g., ~37/51/70/95/122 vs ~35/43/51/55/66) → ramp starts too soon\n",
    "\n",
    "    Evening (H19–H21): under (111/135/74 vs 224/216/135) → peak height still low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7a0ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5397c371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  train_loss=0.4124  val_loss=0.2501\n",
      "Epoch 02  train_loss=0.3173  val_loss=0.2876\n",
      "Epoch 03  train_loss=0.2384  val_loss=0.2097\n",
      "Epoch 04  train_loss=0.2014  val_loss=0.3906\n",
      "Epoch 05  train_loss=0.1751  val_loss=0.3983\n",
      "Epoch 06  train_loss=0.1753  val_loss=0.1780\n",
      "Epoch 07  train_loss=0.1625  val_loss=0.2344\n",
      "Epoch 08  train_loss=0.1478  val_loss=0.2333\n",
      "Epoch 09  train_loss=0.1466  val_loss=0.1654\n",
      "Epoch 10  train_loss=0.1379  val_loss=0.5173\n",
      "Epoch 11  train_loss=0.1425  val_loss=0.1813\n",
      "Epoch 12  train_loss=0.1393  val_loss=0.7863\n",
      "Epoch 13  train_loss=0.1332  val_loss=0.1645\n",
      "Epoch 14  train_loss=0.1265  val_loss=0.2795\n",
      "Epoch 15  train_loss=0.1274  val_loss=0.2342\n",
      "Epoch 16  train_loss=0.1191  val_loss=0.1505\n",
      "Epoch 17  train_loss=0.1209  val_loss=0.2534\n",
      "Epoch 18  train_loss=0.1173  val_loss=0.1583\n",
      "Epoch 19  train_loss=0.1149  val_loss=0.2554\n",
      "Epoch 20  train_loss=0.1095  val_loss=0.1776\n",
      "{'holdout_MAE': 10.248531341552734}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5328\n",
      "[LightGBM] [Info] Number of data points in the train set: 20352, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 7.523834\n",
      "Residual booster trained (LightGBM).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OperatingDTM</th>\n",
       "      <th>Interval</th>\n",
       "      <th>hb_houston_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>1</td>\n",
       "      <td>36.722523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>2</td>\n",
       "      <td>37.694660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>3</td>\n",
       "      <td>34.816631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>4</td>\n",
       "      <td>32.699329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>5</td>\n",
       "      <td>32.672554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>6</td>\n",
       "      <td>33.808319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>7</td>\n",
       "      <td>33.666367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>8</td>\n",
       "      <td>30.430222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>9</td>\n",
       "      <td>27.215673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>10</td>\n",
       "      <td>27.650932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>11</td>\n",
       "      <td>36.302597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>12</td>\n",
       "      <td>49.344604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>13</td>\n",
       "      <td>67.986649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>14</td>\n",
       "      <td>93.122459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>15</td>\n",
       "      <td>129.049072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>16</td>\n",
       "      <td>164.839050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>17</td>\n",
       "      <td>173.463654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>18</td>\n",
       "      <td>177.394974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>19</td>\n",
       "      <td>218.553680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>20</td>\n",
       "      <td>232.858383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>21</td>\n",
       "      <td>106.198997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>22</td>\n",
       "      <td>51.781578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>23</td>\n",
       "      <td>39.074677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>24</td>\n",
       "      <td>32.031742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OperatingDTM  Interval  hb_houston_pred\n",
       "0    2025-08-18         1        36.722523\n",
       "1    2025-08-18         2        37.694660\n",
       "2    2025-08-18         3        34.816631\n",
       "3    2025-08-18         4        32.699329\n",
       "4    2025-08-18         5        32.672554\n",
       "5    2025-08-18         6        33.808319\n",
       "6    2025-08-18         7        33.666367\n",
       "7    2025-08-18         8        30.430222\n",
       "8    2025-08-18         9        27.215673\n",
       "9    2025-08-18        10        27.650932\n",
       "10   2025-08-18        11        36.302597\n",
       "11   2025-08-18        12        49.344604\n",
       "12   2025-08-18        13        67.986649\n",
       "13   2025-08-18        14        93.122459\n",
       "14   2025-08-18        15       129.049072\n",
       "15   2025-08-18        16       164.839050\n",
       "16   2025-08-18        17       173.463654\n",
       "17   2025-08-18        18       177.394974\n",
       "18   2025-08-18        19       218.553680\n",
       "19   2025-08-18        20       232.858383\n",
       "20   2025-08-18        21       106.198997\n",
       "21   2025-08-18        22        51.781578\n",
       "22   2025-08-18        23        39.074677\n",
       "23   2025-08-18        24        32.031742"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== PyTorch seq2seq with ROBUST preprocessing (median impute + safe standardize + safe MAE) ====\n",
    "import duckdb, pandas as pd, numpy as np, datetime as dt, math\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()   # change target date if needed\n",
    "W_PAST = 168\n",
    "H_FUT = 24\n",
    "HOLDOUT_DAYS = 60\n",
    "HIDDEN = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def fit_standardizer(X2d):\n",
    "    \"\"\"Return (mean,std) with std guarded (==0 -> 1). X2d: (N, F)\"\"\"\n",
    "    mu = np.nanmean(X2d, axis=0)\n",
    "    sd = np.nanstd(X2d, axis=0)\n",
    "    sd = np.where(sd <= 1e-8, 1.0, sd)\n",
    "    return mu, sd\n",
    "\n",
    "def apply_standardizer(X, mu, sd):\n",
    "    return (X - mu) / sd\n",
    "\n",
    "# robust 3D imputer\n",
    "def impute_inplace_3d(X, med_vec):\n",
    "    \"\"\"X: (N, T, F) or (N, W, F); med_vec: (F,). Fills NaNs in-place with per-feature medians.\"\"\"\n",
    "    mask = np.isnan(X)\n",
    "    if mask.any():\n",
    "        med_broadcast = np.broadcast_to(med_vec, X.shape)\n",
    "        X[mask] = med_broadcast[mask]\n",
    "\n",
    "# sanity checker\n",
    "def assert_finite(name, arr):\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = int(np.isnan(arr).sum() + np.isinf(arr).sum())\n",
    "        raise RuntimeError(f\"{name} has {bad} non-finite values\")\n",
    "\n",
    "# ---------- ensure weather-enhanced views (hist + fcst) exist & helper transforms ----------\n",
    "con_ensure = duckdb.connect(DB)\n",
    "# Historical weather enhanced (to backfill when forecast is missing during training)\n",
    "con_ensure.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW wx_hist_enh AS\n",
    "WITH agg AS (\n",
    "  SELECT\n",
    "    OperatingDTM, \"interval\" AS Interval,\n",
    "    (hist_temp_the_woodlands_tx + hist_temp_katy_tx + hist_temp_friendswood_tx + hist_temp_baytown_tx + hist_temp_houston_tx)/5.0 AS temp_avg,\n",
    "    (hist_hum_the_woodlands_tx  + hist_hum_katy_tx  + hist_hum_friendswood_tx  + hist_hum_baytown_tx  + hist_hum_houston_tx)/5.0  AS hum_avg,\n",
    "    GREATEST(hist_temp_the_woodlands_tx, hist_temp_katy_tx, hist_temp_friendswood_tx, hist_temp_baytown_tx, hist_temp_houston_tx)\n",
    "      - LEAST(hist_temp_the_woodlands_tx, hist_temp_katy_tx, hist_temp_friendswood_tx, hist_temp_baytown_tx, hist_temp_houston_tx) AS temp_spread,\n",
    "    GREATEST(hist_hum_the_woodlands_tx, hist_hum_katy_tx, hist_hum_friendswood_tx, hist_hum_baytown_tx, hist_hum_houston_tx)\n",
    "      - LEAST(hist_hum_the_woodlands_tx, hist_hum_katy_tx, hist_hum_friendswood_tx, hist_hum_baytown_tx, hist_hum_houston_tx) AS hum_spread\n",
    "  FROM vw_historical_weather_by_city\n",
    ")\n",
    "SELECT\n",
    "  a.*,\n",
    "  a.temp_avg - LAG(a.temp_avg) OVER (ORDER BY OperatingDTM, Interval) AS temp_avg_ramp1,\n",
    "  a.hum_avg  - LAG(a.hum_avg)  OVER (ORDER BY OperatingDTM, Interval) AS hum_avg_ramp1\n",
    "FROM agg a;\n",
    "\"\"\")\n",
    "con_ensure.close()\n",
    "\n",
    "# Signed log transforms for spikes\n",
    "sgn = np.sign\n",
    "abs_ = np.abs\n",
    "\n",
    "def sgn_log1p(y):\n",
    "    return sgn(y) * np.log1p(abs_(y))\n",
    "\n",
    "def sgn_expm1(z):\n",
    "    return sgn(z) * np.expm1(abs_(z))\n",
    "\n",
    "# Hour-of-day weights (emphasize 17-21 local hours)\n",
    "HOUR_WEIGHTS = np.ones(24, dtype=np.float32)\n",
    "# Reweight hours to reduce early overshoot and emphasize true peak\n",
    "# index 0->H1, ..., 23->H24\n",
    "HOUR_WEIGHTS[:] = 1.0\n",
    "HOUR_WEIGHTS[15] = 1.5     # H16\n",
    "HOUR_WEIGHTS[16:18] = 2.0  # H17-18\n",
    "HOUR_WEIGHTS[18:20] = 5.0  # H19-20 (peak focus)\n",
    "HOUR_WEIGHTS[20] = 3.0     # H21\n",
    "\n",
    "# ---------- pull data ----------\n",
    "con = duckdb.connect(DB)\n",
    "df_lags = con.execute(\"\"\"\n",
    "SELECT\n",
    "  ts, OperatingDTM, Interval, hb_houston,\n",
    "  p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "  dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "FROM vw_master_spine_lags\n",
    "ORDER BY ts\n",
    "\"\"\").df()\n",
    "df_lags[\"ts\"] = pd.to_datetime(df_lags[\"ts\"])\n",
    "\n",
    "df_future = con.execute(\"\"\"\n",
    "SELECT\n",
    "  f.OperatingDTM AS delivery_date,\n",
    "  f.Interval     AS delivery_interval,\n",
    "  f.hb_houston   AS target,\n",
    "  -- base future features\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "  f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f,\n",
    "  f.cal_sin_dow  AS cal_sin_dow_f,  f.cal_cos_dow  AS cal_cos_dow_f,\n",
    "  -- load-enhanced joins\n",
    "  l.net_wz, l.net_lz, l.net_wz_ramp1, l.net_lz_ramp1, l.lz_houston_ramp1, l.wz_spread, l.lz_spread,\n",
    "  -- weather: COALESCE forecast -> historical\n",
    "  COALESCE(wf.temp_avg,      wh.temp_avg)      AS temp_avg,\n",
    "  COALESCE(wf.temp_spread,   wh.temp_spread)   AS temp_spread,\n",
    "  COALESCE(wf.temp_avg_ramp1,wh.temp_avg_ramp1)AS temp_avg_ramp1,\n",
    "  COALESCE(wf.hum_avg,       wh.hum_avg)       AS hum_avg,\n",
    "  COALESCE(wf.hum_spread,    wh.hum_spread)    AS hum_spread,\n",
    "  COALESCE(wf.hum_avg_ramp1, wh.hum_avg_ramp1) AS hum_avg_ramp1,\n",
    "  CASE WHEN wf.temp_avg IS NULL THEN 1 ELSE 0 END AS is_weather_proxy,\n",
    "  -- additional ramps & day-over-day deltas to help with peaks (computed over time order)\n",
    "  (l.net_wz      - LAG(l.net_wz,      3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_ramp3,\n",
    "  (l.net_wz      - LAG(l.net_wz,      6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_ramp6,\n",
    "  (l.net_lz      - LAG(l.net_lz,      3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_ramp3,\n",
    "  (l.net_lz      - LAG(l.net_lz,      6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_ramp6,\n",
    "  (l.lz_houston  - LAG(l.lz_houston,  3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_ramp3,\n",
    "  (l.lz_houston  - LAG(l.lz_houston,  6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_ramp6,\n",
    "  (l.net_wz      - LAG(l.net_wz,     24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_dod,\n",
    "  (l.net_lz      - LAG(l.net_lz,     24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_dod,\n",
    "  (l.lz_houston  - LAG(l.lz_houston, 24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_dod\n",
    "FROM vw_master_spine_ts f\n",
    "LEFT JOIN load_fcst_enh l\n",
    "  ON l.OperatingDTM = f.OperatingDTM AND l.Interval = f.Interval\n",
    "LEFT JOIN wx_fcst_enh  wf\n",
    "  ON wf.OperatingDTM = f.OperatingDTM AND wf.Interval = f.Interval\n",
    "LEFT JOIN wx_hist_enh  wh\n",
    "  ON wh.OperatingDTM = f.OperatingDTM AND wh.Interval = f.Interval\n",
    "ORDER BY f.OperatingDTM, f.Interval\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "df_future[\"delivery_date\"] = pd.to_datetime(df_future[\"delivery_date\"]).dt.date\n",
    "df_future[\"delivery_interval\"] = df_future[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# ---------- assemble samples (day-ahead) ----------\n",
    "enc_cols = [\n",
    "    \"hb_houston\",\"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"\n",
    "]\n",
    "dec_future_cols = [\n",
    "    \"cal_hour_f\",\"cal_dow_f\",\"cal_is_weekend_f\",\"cal_sin_hour_f\",\"cal_cos_hour_f\",\"cal_sin_dow_f\",\"cal_cos_dow_f\",\n",
    "    \"net_wz\",\"net_lz\",\"net_wz_ramp1\",\"net_lz_ramp1\",\"lz_houston_ramp1\",\"wz_spread\",\"lz_spread\",\n",
    "    \"temp_avg\",\"temp_spread\",\"temp_avg_ramp1\",\"hum_avg\",\"hum_spread\",\"hum_avg_ramp1\",\n",
    "    # NEW: multi-horizon ramps + day-over-day deltas\n",
    "    \"net_wz_ramp3\",\"net_wz_ramp6\",\"net_lz_ramp3\",\"net_lz_ramp6\",\n",
    "    \"lz_houston_ramp3\",\"lz_houston_ramp6\",\"net_wz_dod\",\"net_lz_dod\",\"lz_houston_dod\",\n",
    "    # Identify when weather came from historical backfill instead of forecast\n",
    "    \"is_weather_proxy\"\n",
    "]\n",
    "\n",
    "# base rows = end-of-day with price\n",
    "base_rows = df_lags[(df_lags[\"hb_houston\"].notna()) & (df_lags[\"Interval\"]==24)][[\"ts\",\"OperatingDTM\",\"hb_houston\"]].copy()\n",
    "base_rows[\"base_date\"] = pd.to_datetime(base_rows[\"OperatingDTM\"]).dt.date\n",
    "base_rows[\"delivery_date\"] = base_rows[\"base_date\"] + dt.timedelta(days=1)\n",
    "\n",
    "# only use delivery days with 24 actuals for training/val\n",
    "has24 = df_future.groupby(\"delivery_date\")[\"target\"].apply(lambda s: s.notna().sum()==24)\n",
    "valid_delivery_dates = set(has24[has24].index)\n",
    "\n",
    "df_lags_idx = df_lags.set_index(\"ts\").sort_index()\n",
    "\n",
    "samples = []\n",
    "for _, r in base_rows.iterrows():\n",
    "    ddate = r[\"delivery_date\"]\n",
    "    ts0 = r[\"ts\"]\n",
    "    # encoder window\n",
    "    start = ts0 - pd.Timedelta(hours=W_PAST-1)\n",
    "    enc_df = df_lags_idx.loc[start:ts0][enc_cols]\n",
    "    if enc_df.shape[0] != W_PAST: \n",
    "        continue\n",
    "    if enc_df.isna().any().any():\n",
    "        continue  # strict: skip any encoder NaNs\n",
    "    # decoder future rows\n",
    "    fdf = df_future[(df_future[\"delivery_date\"]==ddate)].sort_values(\"delivery_interval\")\n",
    "    if fdf.shape[0] != H_FUT:\n",
    "        continue\n",
    "    X_enc = enc_df.values.astype(np.float32)\n",
    "    X_dec = fdf[dec_future_cols].values.astype(np.float32)\n",
    "    y     = fdf[\"target\"].values.astype(np.float32)\n",
    "    y0    = np.float32(r[\"hb_houston\"])\n",
    "    samples.append((X_enc, X_dec, y, y0, ddate))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples assembled; check data coverage or reduce W_PAST.\")\n",
    "\n",
    "# split by date\n",
    "dates = sorted({s[4] for s in samples})\n",
    "cut_date = dates[-1] - dt.timedelta(days=HOLDOUT_DAYS) if len(dates)>HOLDOUT_DAYS else dates[0]\n",
    "train_idx = [i for i,s in enumerate(samples) if s[4] <= cut_date and s[4] in valid_delivery_dates]\n",
    "val_idx   = [i for i,s in enumerate(samples) if s[4] >  cut_date and s[4] in valid_delivery_dates]\n",
    "\n",
    "def stack(idxs):\n",
    "    Xe = np.stack([samples[i][0] for i in idxs], axis=0)\n",
    "    Xd = np.stack([samples[i][1] for i in idxs], axis=0)\n",
    "    Y  = np.stack([samples[i][2] for i in idxs], axis=0)\n",
    "    Y0 = np.stack([samples[i][3] for i in idxs], axis=0)\n",
    "    return Xe, Xd, Y, Y0\n",
    "\n",
    "Xe_tr, Xd_tr, Y_tr, Y0_tr = stack(train_idx)\n",
    "Xe_va, Xd_va, Y_va, Y0_va = (None, None, None, None)\n",
    "if len(val_idx) > 0:\n",
    "    Xe_va, Xd_va, Y_va, Y0_va = stack(val_idx)\n",
    "\n",
    "# ---------- diagnose + drop all-NaN decoder features, then impute (robust) ----------\n",
    "# Flatten decoder train to 2D to inspect per-feature missingness\n",
    "F_orig = Xd_tr.shape[2]\n",
    "Xd_tr_2d_raw = Xd_tr.reshape(-1, F_orig)\n",
    "nan_counts = np.isnan(Xd_tr_2d_raw).sum(axis=0)\n",
    "all_nan_cols = nan_counts == Xd_tr_2d_raw.shape[0]\n",
    "\n",
    "if all_nan_cols.any():\n",
    "    dropped_cols = [dec_future_cols[i] for i,flag in enumerate(all_nan_cols) if flag]\n",
    "    print(f\"Dropping {int(all_nan_cols.sum())} decoder feature(s) with all-NaN in TRAIN: {dropped_cols}\")\n",
    "else:\n",
    "    dropped_cols = []\n",
    "\n",
    "# Keep mask to be reused at prediction time\n",
    "dec_keep_mask = ~all_nan_cols\n",
    "\n",
    "# Apply mask to decoder tensors\n",
    "Xd_tr = Xd_tr[:, :, dec_keep_mask]\n",
    "if Xe_va is not None:\n",
    "    Xd_va = Xd_va[:, :, dec_keep_mask]\n",
    "\n",
    "# Recompute medians on KEPT features\n",
    "dec_med = np.nanmedian(Xd_tr.reshape(-1, Xd_tr.shape[2]), axis=0)\n",
    "# Guard: if a median is still NaN for some reason, set to 0\n",
    "dec_med = np.where(np.isnan(dec_med), 0.0, dec_med)\n",
    "\n",
    "# Encoder median (rare but safe)\n",
    "enc_med = np.nanmedian(Xe_tr.reshape(-1, Xe_tr.shape[2]), axis=0)\n",
    "enc_med = np.where(np.isnan(enc_med), 0.0, enc_med)\n",
    "\n",
    "# Impute in-place (3D-safe)\n",
    "impute_inplace_3d(Xe_tr, enc_med)\n",
    "impute_inplace_3d(Xd_tr, dec_med)\n",
    "if Xe_va is not None:\n",
    "    impute_inplace_3d(Xe_va, enc_med)\n",
    "    impute_inplace_3d(Xd_va, dec_med)\n",
    "\n",
    "# Targets should be complete for training/val; assert\n",
    "assert np.isfinite(Y_tr).all(), \"Y_tr has non-finite values\"\n",
    "if Y_va is not None:\n",
    "    assert np.isfinite(Y_va).all(), \"Y_va has non-finite values\"\n",
    "\n",
    "# ---------- safe standardization ----------\n",
    "enc_mu, enc_sd = fit_standardizer(Xe_tr.reshape(-1, Xe_tr.shape[2]))\n",
    "dec_mu, dec_sd = fit_standardizer(Xd_tr.reshape(-1, Xd_tr.shape[2]))\n",
    "# ---- target transform (spike-aware) then standardize ----\n",
    "Y_tr_t  = sgn_log1p(Y_tr)\n",
    "y_mu, y_sd = fit_standardizer(Y_tr_t.reshape(-1,1))\n",
    "y_mu = float(np.atleast_1d(y_mu)[0]); y_sd = float(np.atleast_1d(y_sd)[0])\n",
    "\n",
    "Xe_tr_n = apply_standardizer(Xe_tr, enc_mu, enc_sd)\n",
    "Xd_tr_n = apply_standardizer(Xd_tr, dec_mu, dec_sd)\n",
    "Y_tr_n  = apply_standardizer(Y_tr_t, y_mu, y_sd).reshape(Y_tr.shape)\n",
    "Y0_tr_t = sgn_log1p(Y0_tr.reshape(-1,1)).reshape(-1,1)\n",
    "Y0_tr_n = apply_standardizer(Y0_tr_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = apply_standardizer(Xe_va, enc_mu, enc_sd)\n",
    "    Xd_va_n = apply_standardizer(Xd_va, dec_mu, dec_sd)\n",
    "    Y_va_t  = sgn_log1p(Y_va)\n",
    "    Y_va_n  = apply_standardizer(Y_va_t,  y_mu,  y_sd).reshape(Y_va.shape)\n",
    "    Y0_va_t = sgn_log1p(Y0_va.reshape(-1,1))\n",
    "    Y0_va_n = apply_standardizer(Y0_va_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "\n",
    "# Final safety: ensure everything is finite\n",
    "assert_finite(\"Xe_tr_n\", Xe_tr_n)\n",
    "assert_finite(\"Xd_tr_n\", Xd_tr_n)\n",
    "assert_finite(\"Y_tr_n\",  Y_tr_n)\n",
    "assert_finite(\"Y0_tr_n\", Y0_tr_n)\n",
    "if Xe_va is not None:\n",
    "    assert_finite(\"Xe_va_n\", Xe_va_n)\n",
    "    assert_finite(\"Xd_va_n\", Xd_va_n)\n",
    "    assert_finite(\"Y_va_n\",  Y_va_n)\n",
    "    assert_finite(\"Y0_va_n\", Y0_va_n)\n",
    "\n",
    "# ---------- torch datasets ----------\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xe, Xd, Y, Y0):\n",
    "        self.Xe = torch.from_numpy(Xe).float()\n",
    "        self.Xd = torch.from_numpy(Xd).float()\n",
    "        self.Y  = torch.from_numpy(Y).float()\n",
    "        self.Y0 = torch.from_numpy(Y0).float()\n",
    "    def __len__(self): return self.Xe.shape[0]\n",
    "    def __getitem__(self, i): return self.Xe[i], self.Xd[i], self.Y[i], self.Y0[i]\n",
    "\n",
    "bs = 32\n",
    "train_loader = torch.utils.data.DataLoader(Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=bs, shuffle=True)\n",
    "val_loader = None if Xe_va is None else torch.utils.data.DataLoader(Seq2SeqDataset(Xe_va_n, Xd_va_n, Y_va_n, Y0_va_n), batch_size=bs, shuffle=False)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim, hidden, batch_first=True)\n",
    "    def forward(self, x):  # x: (B,W,E)\n",
    "        _, h = self.rnn(x)\n",
    "        return h  # (1,B,H)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim + 1, hidden, batch_first=True)  # + prev y\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "    def forward(self, future_feats, h0, y0, teacher=None, tf_prob=0.5):\n",
    "        B, T, D = future_feats.shape\n",
    "        y_prev = y0.view(B,1)\n",
    "        h = h0\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            x_t = torch.cat([future_feats[:,t,:], y_prev], dim=1).unsqueeze(1)  # (B,1,D+1)\n",
    "            o, h = self.rnn(x_t, h)\n",
    "            y_t = self.out(o[:, -1, :]).squeeze(1)\n",
    "            outs.append(y_t)\n",
    "            if (teacher is not None) and (rng.random() < tf_prob):\n",
    "                y_prev = teacher[:,t].view(B,1)\n",
    "            else:\n",
    "                y_prev = y_t.view(B,1)\n",
    "        return torch.stack(outs, dim=1)  # (B,24)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(enc_in, hidden)\n",
    "        self.dec = Decoder(dec_in, hidden)\n",
    "    def forward(self, x_enc, x_dec, y0, y_teacher=None, tf_prob=0.5):\n",
    "        h = self.enc(x_enc)\n",
    "        return self.dec(x_dec, h, y0, y_teacher, tf_prob)\n",
    "\n",
    "enc_in = Xe_tr_n.shape[2]\n",
    "dec_in = Xd_tr_n.shape[2]\n",
    "model = Seq2Seq(enc_in, dec_in, HIDDEN).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "def huber_weighted(yhat, y, w, delta=1.0):\n",
    "    d = torch.abs(yhat - y)\n",
    "    q = torch.clamp(d, max=delta)\n",
    "    l = 0.5 * q**2 + delta * (d - q)\n",
    "    return (l * w).mean()\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in train_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=y, tf_prob=0.5)\n",
    "        # time-step weights (B,24)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    if val_loader is None: return None\n",
    "    model.eval()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(val_loader.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    print(f\"Epoch {ep:02d}  train_loss={tr:.4f}\" + (\"\" if va is None else f\"  val_loss={va:.4f}\"))\n",
    "\n",
    "# ---- evaluate holdout MAE in real units (SAFE) ----\n",
    "if val_loader is not None:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xenc, xdec, y, y0 in val_loader:\n",
    "            xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "            yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "            y_n    = y.detach().cpu().numpy()\n",
    "            # invert scaling + inverse spike transform\n",
    "            yhat_t = (yhat_n * y_sd + y_mu)\n",
    "            yt_t   = (y_n    * y_sd + y_mu)\n",
    "            yhat = sgn_expm1(yhat_t).reshape(-1)\n",
    "            yt   = sgn_expm1(yt_t).reshape(-1)\n",
    "            preds.append(yhat)\n",
    "            trues.append(yt)\n",
    "    preds = np.concatenate(preds)\n",
    "    trues = np.concatenate(trues)\n",
    "    mask = np.isfinite(preds) & np.isfinite(trues)\n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped:\n",
    "        print(f\"Dropped {dropped} non-finite pairs from holdout scoring.\")\n",
    "    print({\"holdout_MAE\": float(mean_absolute_error(trues[mask], preds[mask]))})\n",
    "\n",
    "# ---- optional LightGBM residual booster (train on TRAIN residuals) ----\n",
    "USE_LGBM_RESIDUAL = True\n",
    "booster = None\n",
    "if USE_LGBM_RESIDUAL:\n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        # Build a non-shuffled loader to preserve order\n",
    "        train_loader_eval = torch.utils.data.DataLoader(\n",
    "            Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=64, shuffle=False)\n",
    "        base_preds = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for xenc, xdec, y, y0 in train_loader_eval:\n",
    "                xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "                yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "                base_preds.append(yhat_n)\n",
    "        base_preds = np.concatenate(base_preds, axis=0)  # (N_train, 24)\n",
    "        # inverse transforms -> true $/MWh\n",
    "        base_preds_t = (base_preds * y_sd + y_mu)\n",
    "        base_preds_true = sgn_expm1(base_preds_t)\n",
    "        y_true_tr = Y_tr  # already true units\n",
    "        # flatten\n",
    "        y_base_flat = base_preds_true.reshape(-1)\n",
    "        y_true_flat = y_true_tr.reshape(-1)\n",
    "        X_flat = Xd_tr.reshape(-1, Xd_tr.shape[2])  # kept raw decoder features\n",
    "        mask = np.isfinite(y_base_flat) & np.isfinite(y_true_flat) & np.isfinite(X_flat).all(axis=1)\n",
    "        X_flat = X_flat[mask]\n",
    "        y_resid_flat = (y_true_flat - y_base_flat)[mask]\n",
    "        # Fit booster\n",
    "        booster = lgb.LGBMRegressor(\n",
    "            n_estimators=600, learning_rate=0.03, subsample=0.8, colsample_bytree=0.8,\n",
    "            max_depth=-1, reg_alpha=0.0, reg_lambda=0.0, min_child_samples=20, random_state=42)\n",
    "        booster.fit(X_flat, y_resid_flat)\n",
    "        print(\"Residual booster trained (LightGBM).\")\n",
    "    except Exception as e:\n",
    "        print(f\"LightGBM residual booster skipped: {e}\")\n",
    "\n",
    "# ---- predict requested DELIVERY_DATE ----\n",
    "\n",
    "# find base row (previous day EOD with price)\n",
    "prev_day = DELIVERY_DATE - dt.timedelta(days=1)\n",
    "base_row = df_lags[(pd.to_datetime(df_lags[\"OperatingDTM\"]).dt.date == prev_day) & (df_lags[\"Interval\"]==24) & df_lags[\"hb_houston\"].notna()]\n",
    "if base_row.empty:\n",
    "    raise RuntimeError(f\"No base EOD price for {prev_day}\")\n",
    "ts0 = pd.to_datetime(base_row.iloc[0][\"ts\"])\n",
    "y0  = np.float32(base_row.iloc[0][\"hb_houston\"])\n",
    "\n",
    "# encoder window\n",
    "win = df_lags.set_index(\"ts\").loc[ts0 - pd.Timedelta(hours=W_PAST-1): ts0][enc_cols]\n",
    "if win.shape[0] != W_PAST or win.isna().any().any():\n",
    "    raise RuntimeError(\"Insufficient/NaN history for encoder window.\")\n",
    "Xe_pred = win.values.astype(np.float32)[None, ...]\n",
    "\n",
    "# decoder future features (24 rows)\n",
    "frows = df_future[(df_future[\"delivery_date\"]==DELIVERY_DATE)].sort_values(\"delivery_interval\")\n",
    "if frows.shape[0] != H_FUT:\n",
    "    raise RuntimeError(\"Spine missing 24 rows for delivery date.\")\n",
    "Xd_pred_full = frows[dec_future_cols].values.astype(np.float32)[None, ...]\n",
    "\n",
    "# Apply the same keep mask used in training (drop columns that were all-NaN in train)\n",
    "if 'dec_keep_mask' in globals():\n",
    "    Xd_pred = Xd_pred_full[:, :, dec_keep_mask]\n",
    "else:\n",
    "    Xd_pred = Xd_pred_full\n",
    "\n",
    "# impute + standardize pred using TRAIN stats\n",
    "m = np.isnan(Xd_pred)\n",
    "if m.any():\n",
    "    Xd_pred[m] = np.broadcast_to(dec_med, Xd_pred.shape)[m]\n",
    "Xe_pred_n = apply_standardizer(Xe_pred, enc_mu, enc_sd)\n",
    "Xd_pred_n = apply_standardizer(Xd_pred, dec_mu, dec_sd)\n",
    "# compute normalized scalar y0_n using spike-aware transform\n",
    "y0_t = sgn_log1p(y0)\n",
    "y0_n = np.float32(((y0_t - y_mu) / y_sd)).ravel()[0]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_n = model(torch.from_numpy(Xe_pred_n).to(DEVICE),\n",
    "                   torch.from_numpy(Xd_pred_n).to(DEVICE),\n",
    "                   torch.tensor([y0_n], dtype=torch.float32, device=DEVICE),\n",
    "                   y_teacher=None, tf_prob=0.0).detach().cpu().numpy()[0]\n",
    "yhat_t = (yhat_n * y_sd + y_mu).reshape(-1)\n",
    "yhat = sgn_expm1(yhat_t)\n",
    "# replace any residual NaN with the day's median\n",
    "if np.isnan(yhat).any():\n",
    "    day_med = float(np.nanmedian(yhat))\n",
    "    yhat = np.nan_to_num(yhat, nan=day_med)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*H_FUT,\n",
    "    \"Interval\": list(range(1, H_FUT+1)),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "})\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e4fd7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  train_loss=0.4124  val_loss=0.2501\n",
      "Epoch 02  train_loss=0.3173  val_loss=0.2876\n",
      "Epoch 03  train_loss=0.2384  val_loss=0.2097\n",
      "Epoch 04  train_loss=0.2014  val_loss=0.3906\n",
      "Epoch 05  train_loss=0.1751  val_loss=0.3983\n",
      "Epoch 06  train_loss=0.1753  val_loss=0.1780\n",
      "Epoch 07  train_loss=0.1625  val_loss=0.2344\n",
      "Epoch 08  train_loss=0.1478  val_loss=0.2333\n",
      "Epoch 09  train_loss=0.1466  val_loss=0.1654\n",
      "Epoch 10  train_loss=0.1379  val_loss=0.5173\n",
      "Epoch 11  train_loss=0.1425  val_loss=0.1813\n",
      "Epoch 12  train_loss=0.1393  val_loss=0.7863\n",
      "Epoch 13  train_loss=0.1332  val_loss=0.1645\n",
      "Epoch 14  train_loss=0.1265  val_loss=0.2795\n",
      "Epoch 15  train_loss=0.1274  val_loss=0.2342\n",
      "Epoch 16  train_loss=0.1191  val_loss=0.1505\n",
      "Epoch 17  train_loss=0.1209  val_loss=0.2534\n",
      "Epoch 18  train_loss=0.1173  val_loss=0.1583\n",
      "Epoch 19  train_loss=0.1149  val_loss=0.2554\n",
      "Epoch 20  train_loss=0.1095  val_loss=0.1776\n",
      "{'holdout_MAE': 10.248531341552734}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5328\n",
      "[LightGBM] [Info] Number of data points in the train set: 20352, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 7.523834\n",
      "Residual booster trained (LightGBM).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\db\\DrewBCapstone\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OperatingDTM</th>\n",
       "      <th>Interval</th>\n",
       "      <th>hb_houston_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>1</td>\n",
       "      <td>36.722523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>2</td>\n",
       "      <td>37.694660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>3</td>\n",
       "      <td>34.816631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>4</td>\n",
       "      <td>32.699329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>5</td>\n",
       "      <td>32.672554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>6</td>\n",
       "      <td>33.808319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>7</td>\n",
       "      <td>33.666367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>8</td>\n",
       "      <td>30.430222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>9</td>\n",
       "      <td>27.215673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>10</td>\n",
       "      <td>27.650932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>11</td>\n",
       "      <td>36.302597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>12</td>\n",
       "      <td>49.344604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>13</td>\n",
       "      <td>67.986649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>14</td>\n",
       "      <td>93.122459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>15</td>\n",
       "      <td>129.049072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>16</td>\n",
       "      <td>164.839050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>17</td>\n",
       "      <td>173.463654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>18</td>\n",
       "      <td>105.695387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>19</td>\n",
       "      <td>266.560860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>20</td>\n",
       "      <td>128.432091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>21</td>\n",
       "      <td>87.786651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>22</td>\n",
       "      <td>51.781578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>23</td>\n",
       "      <td>39.074677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>24</td>\n",
       "      <td>32.031742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OperatingDTM  Interval  hb_houston_pred\n",
       "0    2025-08-18         1        36.722523\n",
       "1    2025-08-18         2        37.694660\n",
       "2    2025-08-18         3        34.816631\n",
       "3    2025-08-18         4        32.699329\n",
       "4    2025-08-18         5        32.672554\n",
       "5    2025-08-18         6        33.808319\n",
       "6    2025-08-18         7        33.666367\n",
       "7    2025-08-18         8        30.430222\n",
       "8    2025-08-18         9        27.215673\n",
       "9    2025-08-18        10        27.650932\n",
       "10   2025-08-18        11        36.302597\n",
       "11   2025-08-18        12        49.344604\n",
       "12   2025-08-18        13        67.986649\n",
       "13   2025-08-18        14        93.122459\n",
       "14   2025-08-18        15       129.049072\n",
       "15   2025-08-18        16       164.839050\n",
       "16   2025-08-18        17       173.463654\n",
       "17   2025-08-18        18       105.695387\n",
       "18   2025-08-18        19       266.560860\n",
       "19   2025-08-18        20       128.432091\n",
       "20   2025-08-18        21        87.786651\n",
       "21   2025-08-18        22        51.781578\n",
       "22   2025-08-18        23        39.074677\n",
       "23   2025-08-18        24        32.031742"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== PyTorch seq2seq with ROBUST preprocessing (median impute + safe standardize + safe MAE) ====\n",
    "import duckdb, pandas as pd, numpy as np, datetime as dt, math\n",
    "import torch, torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "DB = \"../ProjectMain/db/data.duckdb\"\n",
    "DELIVERY_DATE = pd.Timestamp(\"2025-08-18\").date()   # change target date if needed\n",
    "W_PAST = 168\n",
    "H_FUT = 24\n",
    "HOLDOUT_DAYS = 60\n",
    "HIDDEN = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def fit_standardizer(X2d):\n",
    "    \"\"\"Return (mean,std) with std guarded (==0 -> 1). X2d: (N, F)\"\"\"\n",
    "    mu = np.nanmean(X2d, axis=0)\n",
    "    sd = np.nanstd(X2d, axis=0)\n",
    "    sd = np.where(sd <= 1e-8, 1.0, sd)\n",
    "    return mu, sd\n",
    "\n",
    "def apply_standardizer(X, mu, sd):\n",
    "    return (X - mu) / sd\n",
    "\n",
    "# robust 3D imputer\n",
    "def impute_inplace_3d(X, med_vec):\n",
    "    \"\"\"X: (N, T, F) or (N, W, F); med_vec: (F,). Fills NaNs in-place with per-feature medians.\"\"\"\n",
    "    mask = np.isnan(X)\n",
    "    if mask.any():\n",
    "        med_broadcast = np.broadcast_to(med_vec, X.shape)\n",
    "        X[mask] = med_broadcast[mask]\n",
    "\n",
    "# sanity checker\n",
    "def assert_finite(name, arr):\n",
    "    if not np.isfinite(arr).all():\n",
    "        bad = int(np.isnan(arr).sum() + np.isinf(arr).sum())\n",
    "        raise RuntimeError(f\"{name} has {bad} non-finite values\")\n",
    "\n",
    "# ---------- ensure weather-enhanced views (hist + fcst) exist & helper transforms ----------\n",
    "con_ensure = duckdb.connect(DB)\n",
    "# Historical weather enhanced (to backfill when forecast is missing during training)\n",
    "con_ensure.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW wx_hist_enh AS\n",
    "WITH agg AS (\n",
    "  SELECT\n",
    "    OperatingDTM, \"interval\" AS Interval,\n",
    "    (hist_temp_the_woodlands_tx + hist_temp_katy_tx + hist_temp_friendswood_tx + hist_temp_baytown_tx + hist_temp_houston_tx)/5.0 AS temp_avg,\n",
    "    (hist_hum_the_woodlands_tx  + hist_hum_katy_tx  + hist_hum_friendswood_tx  + hist_hum_baytown_tx  + hist_hum_houston_tx)/5.0  AS hum_avg,\n",
    "    GREATEST(hist_temp_the_woodlands_tx, hist_temp_katy_tx, hist_temp_friendswood_tx, hist_temp_baytown_tx, hist_temp_houston_tx)\n",
    "      - LEAST(hist_temp_the_woodlands_tx, hist_temp_katy_tx, hist_temp_friendswood_tx, hist_temp_baytown_tx, hist_temp_houston_tx) AS temp_spread,\n",
    "    GREATEST(hist_hum_the_woodlands_tx, hist_hum_katy_tx, hist_hum_friendswood_tx, hist_hum_baytown_tx, hist_hum_houston_tx)\n",
    "      - LEAST(hist_hum_the_woodlands_tx, hist_hum_katy_tx, hist_hum_friendswood_tx, hist_hum_baytown_tx, hist_hum_houston_tx) AS hum_spread\n",
    "  FROM vw_historical_weather_by_city\n",
    ")\n",
    "SELECT\n",
    "  a.*,\n",
    "  a.temp_avg - LAG(a.temp_avg) OVER (ORDER BY OperatingDTM, Interval) AS temp_avg_ramp1,\n",
    "  a.hum_avg  - LAG(a.hum_avg)  OVER (ORDER BY OperatingDTM, Interval) AS hum_avg_ramp1\n",
    "FROM agg a;\n",
    "\"\"\")\n",
    "con_ensure.close()\n",
    "\n",
    "# Signed log transforms for spikes\n",
    "sgn = np.sign\n",
    "abs_ = np.abs\n",
    "\n",
    "def sgn_log1p(y):\n",
    "    return sgn(y) * np.log1p(abs_(y))\n",
    "\n",
    "def sgn_expm1(z):\n",
    "    return sgn(z) * np.expm1(abs_(z))\n",
    "\n",
    "# Hour-of-day weights (emphasize 17-21 local hours)\n",
    "HOUR_WEIGHTS = np.ones(24, dtype=np.float32)\n",
    "# Reweight hours to reduce early overshoot and emphasize true peak\n",
    "# index 0->H1, ..., 23->H24\n",
    "HOUR_WEIGHTS[:] = 1.0\n",
    "HOUR_WEIGHTS[15] = 1.5     # H16\n",
    "HOUR_WEIGHTS[16:18] = 2.0  # H17-18\n",
    "HOUR_WEIGHTS[18:20] = 5.0  # H19-20 (peak focus)\n",
    "HOUR_WEIGHTS[20] = 3.0     # H21\n",
    "\n",
    "# ---------- pull data ----------\n",
    "con = duckdb.connect(DB)\n",
    "df_lags = con.execute(\"\"\"\n",
    "SELECT\n",
    "  ts, OperatingDTM, Interval, hb_houston,\n",
    "  p_lag1,p_lag2,p_lag3,p_lag6,p_lag12,p_lag24,p_lag48,p_lag72,p_lag168,\n",
    "  dp1,dp24,p_roll24_mean,p_roll24_std,p_roll72_mean,p_roll168_mean\n",
    "FROM vw_master_spine_lags\n",
    "ORDER BY ts\n",
    "\"\"\").df()\n",
    "df_lags[\"ts\"] = pd.to_datetime(df_lags[\"ts\"])\n",
    "\n",
    "df_future = con.execute(\"\"\"\n",
    "SELECT\n",
    "  f.OperatingDTM AS delivery_date,\n",
    "  f.Interval     AS delivery_interval,\n",
    "  f.hb_houston   AS target,\n",
    "  -- base future features\n",
    "  f.wz_southcentral, f.wz_east, f.wz_west, f.wz_northcentral, f.wz_farwest, f.wz_north, f.wz_southern, f.wz_coast,\n",
    "  f.lz_north, f.lz_west, f.lz_south, f.lz_houston,\n",
    "  f.cal_hour AS cal_hour_f, f.cal_dow AS cal_dow_f, f.cal_is_weekend AS cal_is_weekend_f,\n",
    "  f.cal_sin_hour AS cal_sin_hour_f, f.cal_cos_hour AS cal_cos_hour_f,\n",
    "  f.cal_sin_dow  AS cal_sin_dow_f,  f.cal_cos_dow  AS cal_cos_dow_f,\n",
    "  -- load-enhanced joins\n",
    "  l.net_wz, l.net_lz, l.net_wz_ramp1, l.net_lz_ramp1, l.lz_houston_ramp1, l.wz_spread, l.lz_spread,\n",
    "  -- weather: COALESCE forecast -> historical\n",
    "  COALESCE(wf.temp_avg,      wh.temp_avg)      AS temp_avg,\n",
    "  COALESCE(wf.temp_spread,   wh.temp_spread)   AS temp_spread,\n",
    "  COALESCE(wf.temp_avg_ramp1,wh.temp_avg_ramp1)AS temp_avg_ramp1,\n",
    "  COALESCE(wf.hum_avg,       wh.hum_avg)       AS hum_avg,\n",
    "  COALESCE(wf.hum_spread,    wh.hum_spread)    AS hum_spread,\n",
    "  COALESCE(wf.hum_avg_ramp1, wh.hum_avg_ramp1) AS hum_avg_ramp1,\n",
    "  CASE WHEN wf.temp_avg IS NULL THEN 1 ELSE 0 END AS is_weather_proxy,\n",
    "  -- additional ramps & day-over-day deltas to help with peaks (computed over time order)\n",
    "  (l.net_wz      - LAG(l.net_wz,      3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_ramp3,\n",
    "  (l.net_wz      - LAG(l.net_wz,      6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_ramp6,\n",
    "  (l.net_lz      - LAG(l.net_lz,      3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_ramp3,\n",
    "  (l.net_lz      - LAG(l.net_lz,      6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_ramp6,\n",
    "  (l.lz_houston  - LAG(l.lz_houston,  3)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_ramp3,\n",
    "  (l.lz_houston  - LAG(l.lz_houston,  6)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_ramp6,\n",
    "  (l.net_wz      - LAG(l.net_wz,     24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_wz_dod,\n",
    "  (l.net_lz      - LAG(l.net_lz,     24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS net_lz_dod,\n",
    "  (l.lz_houston  - LAG(l.lz_houston, 24)  OVER (ORDER BY f.OperatingDTM, f.Interval)) AS lz_houston_dod\n",
    "FROM vw_master_spine_ts f\n",
    "LEFT JOIN load_fcst_enh l\n",
    "  ON l.OperatingDTM = f.OperatingDTM AND l.Interval = f.Interval\n",
    "LEFT JOIN wx_fcst_enh  wf\n",
    "  ON wf.OperatingDTM = f.OperatingDTM AND wf.Interval = f.Interval\n",
    "LEFT JOIN wx_hist_enh  wh\n",
    "  ON wh.OperatingDTM = f.OperatingDTM AND wh.Interval = f.Interval\n",
    "ORDER BY f.OperatingDTM, f.Interval\n",
    "\"\"\").df()\n",
    "con.close()\n",
    "\n",
    "df_future[\"delivery_date\"] = pd.to_datetime(df_future[\"delivery_date\"]).dt.date\n",
    "df_future[\"delivery_interval\"] = df_future[\"delivery_interval\"].astype(int)\n",
    "\n",
    "# ---------- assemble samples (day-ahead) ----------\n",
    "enc_cols = [\n",
    "    \"hb_houston\",\"p_lag1\",\"p_lag2\",\"p_lag3\",\"p_lag6\",\"p_lag12\",\"p_lag24\",\"p_lag48\",\"p_lag72\",\"p_lag168\",\n",
    "    \"dp1\",\"dp24\",\"p_roll24_mean\",\"p_roll24_std\",\"p_roll72_mean\",\"p_roll168_mean\"\n",
    "]\n",
    "dec_future_cols = [\n",
    "    \"cal_hour_f\",\"cal_dow_f\",\"cal_is_weekend_f\",\"cal_sin_hour_f\",\"cal_cos_hour_f\",\"cal_sin_dow_f\",\"cal_cos_dow_f\",\n",
    "    \"net_wz\",\"net_lz\",\"net_wz_ramp1\",\"net_lz_ramp1\",\"lz_houston_ramp1\",\"wz_spread\",\"lz_spread\",\n",
    "    \"temp_avg\",\"temp_spread\",\"temp_avg_ramp1\",\"hum_avg\",\"hum_spread\",\"hum_avg_ramp1\",\n",
    "    # NEW: multi-horizon ramps + day-over-day deltas\n",
    "    \"net_wz_ramp3\",\"net_wz_ramp6\",\"net_lz_ramp3\",\"net_lz_ramp6\",\n",
    "    \"lz_houston_ramp3\",\"lz_houston_ramp6\",\"net_wz_dod\",\"net_lz_dod\",\"lz_houston_dod\",\n",
    "    # Identify when weather came from historical backfill instead of forecast\n",
    "    \"is_weather_proxy\"\n",
    "]\n",
    "\n",
    "# base rows = end-of-day with price\n",
    "base_rows = df_lags[(df_lags[\"hb_houston\"].notna()) & (df_lags[\"Interval\"]==24)][[\"ts\",\"OperatingDTM\",\"hb_houston\"]].copy()\n",
    "base_rows[\"base_date\"] = pd.to_datetime(base_rows[\"OperatingDTM\"]).dt.date\n",
    "base_rows[\"delivery_date\"] = base_rows[\"base_date\"] + dt.timedelta(days=1)\n",
    "\n",
    "# only use delivery days with 24 actuals for training/val\n",
    "has24 = df_future.groupby(\"delivery_date\")[\"target\"].apply(lambda s: s.notna().sum()==24)\n",
    "valid_delivery_dates = set(has24[has24].index)\n",
    "\n",
    "df_lags_idx = df_lags.set_index(\"ts\").sort_index()\n",
    "\n",
    "samples = []\n",
    "for _, r in base_rows.iterrows():\n",
    "    ddate = r[\"delivery_date\"]\n",
    "    ts0 = r[\"ts\"]\n",
    "    # encoder window\n",
    "    start = ts0 - pd.Timedelta(hours=W_PAST-1)\n",
    "    enc_df = df_lags_idx.loc[start:ts0][enc_cols]\n",
    "    if enc_df.shape[0] != W_PAST: \n",
    "        continue\n",
    "    if enc_df.isna().any().any():\n",
    "        continue  # strict: skip any encoder NaNs\n",
    "    # decoder future rows\n",
    "    fdf = df_future[(df_future[\"delivery_date\"]==ddate)].sort_values(\"delivery_interval\")\n",
    "    if fdf.shape[0] != H_FUT:\n",
    "        continue\n",
    "    X_enc = enc_df.values.astype(np.float32)\n",
    "    X_dec = fdf[dec_future_cols].values.astype(np.float32)\n",
    "    y     = fdf[\"target\"].values.astype(np.float32)\n",
    "    y0    = np.float32(r[\"hb_houston\"])\n",
    "    samples.append((X_enc, X_dec, y, y0, ddate))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples assembled; check data coverage or reduce W_PAST.\")\n",
    "\n",
    "# split by date\n",
    "dates = sorted({s[4] for s in samples})\n",
    "cut_date = dates[-1] - dt.timedelta(days=HOLDOUT_DAYS) if len(dates)>HOLDOUT_DAYS else dates[0]\n",
    "train_idx = [i for i,s in enumerate(samples) if s[4] <= cut_date and s[4] in valid_delivery_dates]\n",
    "val_idx   = [i for i,s in enumerate(samples) if s[4] >  cut_date and s[4] in valid_delivery_dates]\n",
    "\n",
    "def stack(idxs):\n",
    "    Xe = np.stack([samples[i][0] for i in idxs], axis=0)\n",
    "    Xd = np.stack([samples[i][1] for i in idxs], axis=0)\n",
    "    Y  = np.stack([samples[i][2] for i in idxs], axis=0)\n",
    "    Y0 = np.stack([samples[i][3] for i in idxs], axis=0)\n",
    "    return Xe, Xd, Y, Y0\n",
    "\n",
    "Xe_tr, Xd_tr, Y_tr, Y0_tr = stack(train_idx)\n",
    "Xe_va, Xd_va, Y_va, Y0_va = (None, None, None, None)\n",
    "if len(val_idx) > 0:\n",
    "    Xe_va, Xd_va, Y_va, Y0_va = stack(val_idx)\n",
    "\n",
    "# ---------- diagnose + drop all-NaN decoder features, then impute (robust) ----------\n",
    "# Flatten decoder train to 2D to inspect per-feature missingness\n",
    "F_orig = Xd_tr.shape[2]\n",
    "Xd_tr_2d_raw = Xd_tr.reshape(-1, F_orig)\n",
    "nan_counts = np.isnan(Xd_tr_2d_raw).sum(axis=0)\n",
    "all_nan_cols = nan_counts == Xd_tr_2d_raw.shape[0]\n",
    "\n",
    "if all_nan_cols.any():\n",
    "    dropped_cols = [dec_future_cols[i] for i,flag in enumerate(all_nan_cols) if flag]\n",
    "    print(f\"Dropping {int(all_nan_cols.sum())} decoder feature(s) with all-NaN in TRAIN: {dropped_cols}\")\n",
    "else:\n",
    "    dropped_cols = []\n",
    "\n",
    "# Keep mask to be reused at prediction time\n",
    "dec_keep_mask = ~all_nan_cols\n",
    "\n",
    "# Apply mask to decoder tensors\n",
    "Xd_tr = Xd_tr[:, :, dec_keep_mask]\n",
    "if Xe_va is not None:\n",
    "    Xd_va = Xd_va[:, :, dec_keep_mask]\n",
    "\n",
    "# Recompute medians on KEPT features\n",
    "dec_med = np.nanmedian(Xd_tr.reshape(-1, Xd_tr.shape[2]), axis=0)\n",
    "# Guard: if a median is still NaN for some reason, set to 0\n",
    "dec_med = np.where(np.isnan(dec_med), 0.0, dec_med)\n",
    "\n",
    "# Encoder median (rare but safe)\n",
    "enc_med = np.nanmedian(Xe_tr.reshape(-1, Xe_tr.shape[2]), axis=0)\n",
    "enc_med = np.where(np.isnan(enc_med), 0.0, enc_med)\n",
    "\n",
    "# Impute in-place (3D-safe)\n",
    "impute_inplace_3d(Xe_tr, enc_med)\n",
    "impute_inplace_3d(Xd_tr, dec_med)\n",
    "if Xe_va is not None:\n",
    "    impute_inplace_3d(Xe_va, enc_med)\n",
    "    impute_inplace_3d(Xd_va, dec_med)\n",
    "\n",
    "# Targets should be complete for training/val; assert\n",
    "assert np.isfinite(Y_tr).all(), \"Y_tr has non-finite values\"\n",
    "if Y_va is not None:\n",
    "    assert np.isfinite(Y_va).all(), \"Y_va has non-finite values\"\n",
    "\n",
    "# ---------- safe standardization ----------\n",
    "enc_mu, enc_sd = fit_standardizer(Xe_tr.reshape(-1, Xe_tr.shape[2]))\n",
    "dec_mu, dec_sd = fit_standardizer(Xd_tr.reshape(-1, Xd_tr.shape[2]))\n",
    "# ---- target transform (spike-aware) then standardize ----\n",
    "Y_tr_t  = sgn_log1p(Y_tr)\n",
    "y_mu, y_sd = fit_standardizer(Y_tr_t.reshape(-1,1))\n",
    "y_mu = float(np.atleast_1d(y_mu)[0]); y_sd = float(np.atleast_1d(y_sd)[0])\n",
    "\n",
    "Xe_tr_n = apply_standardizer(Xe_tr, enc_mu, enc_sd)\n",
    "Xd_tr_n = apply_standardizer(Xd_tr, dec_mu, dec_sd)\n",
    "Y_tr_n  = apply_standardizer(Y_tr_t, y_mu, y_sd).reshape(Y_tr.shape)\n",
    "Y0_tr_t = sgn_log1p(Y0_tr.reshape(-1,1)).reshape(-1,1)\n",
    "Y0_tr_n = apply_standardizer(Y0_tr_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "if Xe_va is not None:\n",
    "    Xe_va_n = apply_standardizer(Xe_va, enc_mu, enc_sd)\n",
    "    Xd_va_n = apply_standardizer(Xd_va, dec_mu, dec_sd)\n",
    "    Y_va_t  = sgn_log1p(Y_va)\n",
    "    Y_va_n  = apply_standardizer(Y_va_t,  y_mu,  y_sd).reshape(Y_va.shape)\n",
    "    Y0_va_t = sgn_log1p(Y0_va.reshape(-1,1))\n",
    "    Y0_va_n = apply_standardizer(Y0_va_t, y_mu, y_sd).reshape(-1)\n",
    "\n",
    "\n",
    "# Final safety: ensure everything is finite\n",
    "assert_finite(\"Xe_tr_n\", Xe_tr_n)\n",
    "assert_finite(\"Xd_tr_n\", Xd_tr_n)\n",
    "assert_finite(\"Y_tr_n\",  Y_tr_n)\n",
    "assert_finite(\"Y0_tr_n\", Y0_tr_n)\n",
    "if Xe_va is not None:\n",
    "    assert_finite(\"Xe_va_n\", Xe_va_n)\n",
    "    assert_finite(\"Xd_va_n\", Xd_va_n)\n",
    "    assert_finite(\"Y_va_n\",  Y_va_n)\n",
    "    assert_finite(\"Y0_va_n\", Y0_va_n)\n",
    "\n",
    "# ---------- torch datasets ----------\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, Xe, Xd, Y, Y0):\n",
    "        self.Xe = torch.from_numpy(Xe).float()\n",
    "        self.Xd = torch.from_numpy(Xd).float()\n",
    "        self.Y  = torch.from_numpy(Y).float()\n",
    "        self.Y0 = torch.from_numpy(Y0).float()\n",
    "    def __len__(self): return self.Xe.shape[0]\n",
    "    def __getitem__(self, i): return self.Xe[i], self.Xd[i], self.Y[i], self.Y0[i]\n",
    "\n",
    "bs = 32\n",
    "train_loader = torch.utils.data.DataLoader(Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=bs, shuffle=True)\n",
    "val_loader = None if Xe_va is None else torch.utils.data.DataLoader(Seq2SeqDataset(Xe_va_n, Xd_va_n, Y_va_n, Y0_va_n), batch_size=bs, shuffle=False)\n",
    "\n",
    "# ---------- model ----------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim, hidden, batch_first=True)\n",
    "    def forward(self, x):  # x: (B,W,E)\n",
    "        _, h = self.rnn(x)\n",
    "        return h  # (1,B,H)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(in_dim + 1, hidden, batch_first=True)  # + prev y\n",
    "        self.out = nn.Linear(hidden, 1)\n",
    "    def forward(self, future_feats, h0, y0, teacher=None, tf_prob=0.5):\n",
    "        B, T, D = future_feats.shape\n",
    "        y_prev = y0.view(B,1)\n",
    "        h = h0\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            x_t = torch.cat([future_feats[:,t,:], y_prev], dim=1).unsqueeze(1)  # (B,1,D+1)\n",
    "            o, h = self.rnn(x_t, h)\n",
    "            y_t = self.out(o[:, -1, :]).squeeze(1)\n",
    "            outs.append(y_t)\n",
    "            if (teacher is not None) and (rng.random() < tf_prob):\n",
    "                y_prev = teacher[:,t].view(B,1)\n",
    "            else:\n",
    "                y_prev = y_t.view(B,1)\n",
    "        return torch.stack(outs, dim=1)  # (B,24)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(enc_in, hidden)\n",
    "        self.dec = Decoder(dec_in, hidden)\n",
    "    def forward(self, x_enc, x_dec, y0, y_teacher=None, tf_prob=0.5):\n",
    "        h = self.enc(x_enc)\n",
    "        return self.dec(x_dec, h, y0, y_teacher, tf_prob)\n",
    "\n",
    "enc_in = Xe_tr_n.shape[2]\n",
    "dec_in = Xd_tr_n.shape[2]\n",
    "model = Seq2Seq(enc_in, dec_in, HIDDEN).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "def huber_weighted(yhat, y, w, delta=1.0):\n",
    "    d = torch.abs(yhat - y)\n",
    "    q = torch.clamp(d, max=delta)\n",
    "    l = 0.5 * q**2 + delta * (d - q)\n",
    "    return (l * w).mean()\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in train_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=y, tf_prob=0.5)\n",
    "        # time-step weights (B,24)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch():\n",
    "    if val_loader is None: return None\n",
    "    model.eval()\n",
    "    tot = 0.0\n",
    "    for xenc, xdec, y, y0 in val_loader:\n",
    "        xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "        yhat = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0)\n",
    "        w = torch.tensor(HOUR_WEIGHTS, device=y.device).unsqueeze(0).expand_as(y)\n",
    "        loss = huber_weighted(yhat, y, w)\n",
    "        tot += loss.item() * xenc.size(0)\n",
    "    return tot / len(val_loader.dataset)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr = train_epoch()\n",
    "    va = eval_epoch()\n",
    "    print(f\"Epoch {ep:02d}  train_loss={tr:.4f}\" + (\"\" if va is None else f\"  val_loss={va:.4f}\"))\n",
    "\n",
    "# ---- evaluate holdout MAE in real units (SAFE) ----\n",
    "if val_loader is not None:\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xenc, xdec, y, y0 in val_loader:\n",
    "            xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "            yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "            y_n    = y.detach().cpu().numpy()\n",
    "            # invert scaling + inverse spike transform\n",
    "            yhat_t = (yhat_n * y_sd + y_mu)\n",
    "            yt_t   = (y_n    * y_sd + y_mu)\n",
    "            yhat = sgn_expm1(yhat_t).reshape(-1)\n",
    "            yt   = sgn_expm1(yt_t).reshape(-1)\n",
    "            preds.append(yhat)\n",
    "            trues.append(yt)\n",
    "    preds = np.concatenate(preds)\n",
    "    trues = np.concatenate(trues)\n",
    "    mask = np.isfinite(preds) & np.isfinite(trues)\n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped:\n",
    "        print(f\"Dropped {dropped} non-finite pairs from holdout scoring.\")\n",
    "    print({\"holdout_MAE\": float(mean_absolute_error(trues[mask], preds[mask]))})\n",
    "\n",
    "# ---- optional LightGBM residual booster (train on TRAIN residuals) ----\n",
    "USE_LGBM_RESIDUAL = True\n",
    "booster = None\n",
    "if USE_LGBM_RESIDUAL:\n",
    "    try:\n",
    "        import lightgbm as lgb\n",
    "        # Build a non-shuffled loader to preserve order\n",
    "        train_loader_eval = torch.utils.data.DataLoader(\n",
    "            Seq2SeqDataset(Xe_tr_n, Xd_tr_n, Y_tr_n, Y0_tr_n), batch_size=64, shuffle=False)\n",
    "        base_preds = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for xenc, xdec, y, y0 in train_loader_eval:\n",
    "                xenc, xdec, y, y0 = xenc.to(DEVICE), xdec.to(DEVICE), y.to(DEVICE), y0.to(DEVICE)\n",
    "                yhat_n = model(xenc, xdec, y0, y_teacher=None, tf_prob=0.0).detach().cpu().numpy()\n",
    "                base_preds.append(yhat_n)\n",
    "        base_preds = np.concatenate(base_preds, axis=0)  # (N_train, 24)\n",
    "        # inverse transforms -> true $/MWh\n",
    "        base_preds_t = (base_preds * y_sd + y_mu)\n",
    "        base_preds_true = sgn_expm1(base_preds_t)\n",
    "        y_true_tr = Y_tr  # already true units\n",
    "        # flatten\n",
    "        y_base_flat = base_preds_true.reshape(-1)\n",
    "        y_true_flat = y_true_tr.reshape(-1)\n",
    "        X_flat = Xd_tr.reshape(-1, Xd_tr.shape[2])  # kept raw decoder features\n",
    "        mask = np.isfinite(y_base_flat) & np.isfinite(y_true_flat) & np.isfinite(X_flat).all(axis=1)\n",
    "        X_flat = X_flat[mask]\n",
    "        y_resid_flat = (y_true_flat - y_base_flat)[mask]\n",
    "        # Fit booster\n",
    "        booster = lgb.LGBMRegressor(\n",
    "            n_estimators=600, learning_rate=0.03, subsample=0.8, colsample_bytree=0.8,\n",
    "            max_depth=-1, reg_alpha=0.0, reg_lambda=0.0, min_child_samples=20, random_state=42)\n",
    "        booster.fit(X_flat, y_resid_flat)\n",
    "        print(\"Residual booster trained (LightGBM).\")\n",
    "    except Exception as e:\n",
    "        print(f\"LightGBM residual booster skipped: {e}\")\n",
    "\n",
    "# ---- predict requested DELIVERY_DATE ----\n",
    "\n",
    "# find base row (previous day EOD with price)\n",
    "prev_day = DELIVERY_DATE - dt.timedelta(days=1)\n",
    "base_row = df_lags[(pd.to_datetime(df_lags[\"OperatingDTM\"]).dt.date == prev_day) & (df_lags[\"Interval\"]==24) & df_lags[\"hb_houston\"].notna()]\n",
    "if base_row.empty:\n",
    "    raise RuntimeError(f\"No base EOD price for {prev_day}\")\n",
    "ts0 = pd.to_datetime(base_row.iloc[0][\"ts\"])\n",
    "y0  = np.float32(base_row.iloc[0][\"hb_houston\"])\n",
    "\n",
    "# encoder window\n",
    "win = df_lags.set_index(\"ts\").loc[ts0 - pd.Timedelta(hours=W_PAST-1): ts0][enc_cols]\n",
    "if win.shape[0] != W_PAST or win.isna().any().any():\n",
    "    raise RuntimeError(\"Insufficient/NaN history for encoder window.\")\n",
    "Xe_pred = win.values.astype(np.float32)[None, ...]\n",
    "\n",
    "# decoder future features (24 rows)\n",
    "frows = df_future[(df_future[\"delivery_date\"]==DELIVERY_DATE)].sort_values(\"delivery_interval\")\n",
    "if frows.shape[0] != H_FUT:\n",
    "    raise RuntimeError(\"Spine missing 24 rows for delivery date.\")\n",
    "Xd_pred_full = frows[dec_future_cols].values.astype(np.float32)[None, ...]\n",
    "\n",
    "# Apply the same keep mask used in training (drop columns that were all-NaN in train)\n",
    "if 'dec_keep_mask' in globals():\n",
    "    Xd_pred = Xd_pred_full[:, :, dec_keep_mask]\n",
    "else:\n",
    "    Xd_pred = Xd_pred_full\n",
    "\n",
    "# impute + standardize pred using TRAIN stats\n",
    "m = np.isnan(Xd_pred)\n",
    "if m.any():\n",
    "    Xd_pred[m] = np.broadcast_to(dec_med, Xd_pred.shape)[m]\n",
    "Xe_pred_n = apply_standardizer(Xe_pred, enc_mu, enc_sd)\n",
    "Xd_pred_n = apply_standardizer(Xd_pred, dec_mu, dec_sd)\n",
    "# compute normalized scalar y0_n using spike-aware transform\n",
    "y0_t = sgn_log1p(y0)\n",
    "y0_n = np.float32(((y0_t - y_mu) / y_sd)).ravel()[0]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yhat_n = model(torch.from_numpy(Xe_pred_n).to(DEVICE),\n",
    "                   torch.from_numpy(Xd_pred_n).to(DEVICE),\n",
    "                   torch.tensor([y0_n], dtype=torch.float32, device=DEVICE),\n",
    "                   y_teacher=None, tf_prob=0.0).detach().cpu().numpy()[0]\n",
    "yhat_t = (yhat_n * y_sd + y_mu).reshape(-1)\n",
    "yhat = sgn_expm1(yhat_t)\n",
    "# --- optional: add residual booster only on peak hours (H18–H21) ---\n",
    "if booster is not None:\n",
    "    Xd_pred_kept = Xd_pred.copy()  # (1,24,F_kept)\n",
    "    resid_pred = booster.predict(Xd_pred_kept.reshape(24, Xd_pred_kept.shape[2]))\n",
    "    hour_mask = np.zeros(24, dtype=np.float32)\n",
    "    hour_mask[17:21] = 1.0  # apply only to intervals 18,19,20,21\n",
    "    yhat = yhat + resid_pred * hour_mask\n",
    "\n",
    "# replace any residual NaN with the day's median\n",
    "if np.isnan(yhat).any():\n",
    "    day_med = float(np.nanmedian(yhat))\n",
    "    yhat = np.nan_to_num(yhat, nan=day_med)\n",
    "\n",
    "forecast = pd.DataFrame({\n",
    "    \"OperatingDTM\": [DELIVERY_DATE]*H_FUT,\n",
    "    \"Interval\": list(range(1, H_FUT+1)),\n",
    "    \"hb_houston_pred\": yhat.tolist()\n",
    "})\n",
    "forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93718a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    OperatingDTM           timestamp  interval  temp_the_woodlands_tx  \\\n",
      "0     2025-08-24 2025-08-24 01:00:00         1                   30.1   \n",
      "1     2025-08-24 2025-08-24 02:00:00         2                   29.5   \n",
      "2     2025-08-24 2025-08-24 03:00:00         3                   28.9   \n",
      "3     2025-08-24 2025-08-24 04:00:00         4                   28.4   \n",
      "4     2025-08-24 2025-08-24 05:00:00         5                   28.0   \n",
      "..           ...                 ...       ...                    ...   \n",
      "187   2025-08-17 2025-08-17 20:00:00        20                   35.5   \n",
      "188   2025-08-17 2025-08-17 21:00:00        21                   33.8   \n",
      "189   2025-08-17 2025-08-17 22:00:00        22                   32.3   \n",
      "190   2025-08-17 2025-08-17 23:00:00        23                   30.6   \n",
      "191   2025-08-17 2025-08-18 00:00:00        24                   29.6   \n",
      "\n",
      "     hum_the_woodlands_tx  wind_the_woodlands_tx  precip_the_woodlands_tx  \\\n",
      "0                    60.0                   2.97                      0.0   \n",
      "1                    62.0                   2.69                      0.0   \n",
      "2                    64.0                   2.55                      0.0   \n",
      "3                    66.0                   2.42                      0.0   \n",
      "4                    68.0                   2.28                      0.0   \n",
      "..                    ...                    ...                      ...   \n",
      "187                  40.0                   3.44                      0.0   \n",
      "188                  39.0                   2.48                      0.0   \n",
      "189                  38.0                   2.14                      0.0   \n",
      "190                  48.0                   2.60                      0.0   \n",
      "191                  64.0                   2.12                      0.0   \n",
      "\n",
      "     temp_katy_tx  hum_katy_tx  wind_katy_tx  ...  wind_friendswood_tx  \\\n",
      "0            26.3         78.0          3.06  ...                 2.39   \n",
      "1            26.0         78.0          2.69  ...                 2.27   \n",
      "2            25.7         78.0          2.55  ...                 2.11   \n",
      "3            25.6         79.0          2.58  ...                 1.97   \n",
      "4            25.5         79.0          2.55  ...                 1.80   \n",
      "..            ...          ...           ...  ...                  ...   \n",
      "187          35.9         38.0          2.69  ...                 4.30   \n",
      "188          34.1         42.0          2.22  ...                 3.40   \n",
      "189          31.8         54.0          4.41  ...                 2.92   \n",
      "190          30.1         63.0          3.72  ...                 2.55   \n",
      "191          29.2         67.0          3.01  ...                 1.75   \n",
      "\n",
      "     precip_friendswood_tx  temp_baytown_tx  hum_baytown_tx  wind_baytown_tx  \\\n",
      "0                      0.0             28.4            72.0             3.26   \n",
      "1                      0.0             28.0            73.0             3.00   \n",
      "2                      0.0             27.6            74.0             2.82   \n",
      "3                      0.0             27.2            76.0             2.53   \n",
      "4                      0.0             26.9            77.0             2.43   \n",
      "..                     ...              ...             ...              ...   \n",
      "187                    0.0             32.5            59.0             3.52   \n",
      "188                    0.0             31.1            66.0             3.41   \n",
      "189                    0.0             29.5            73.0             3.34   \n",
      "190                    0.0             28.7            76.0             2.75   \n",
      "191                    0.0             28.2            81.0             2.01   \n",
      "\n",
      "     precip_baytown_tx  temp_houston_tx  hum_houston_tx  wind_houston_tx  \\\n",
      "0                  0.0             29.4            63.0             3.01   \n",
      "1                  0.0             29.1            65.0             2.80   \n",
      "2                  0.0             28.8            66.0             2.62   \n",
      "3                  0.0             28.6            67.0             2.33   \n",
      "4                  0.0             28.5            67.0             2.05   \n",
      "..                 ...              ...             ...              ...   \n",
      "187                0.0             31.6            53.0             2.55   \n",
      "188                0.0             29.7            62.0             2.98   \n",
      "189                0.0             28.8            66.0             2.80   \n",
      "190                0.0             27.7            72.0             2.70   \n",
      "191                0.0             27.2            78.0             2.35   \n",
      "\n",
      "     precip_houston_tx  \n",
      "0                  0.0  \n",
      "1                  0.0  \n",
      "2                  0.0  \n",
      "3                  0.0  \n",
      "4                  0.0  \n",
      "..                 ...  \n",
      "187                0.0  \n",
      "188                0.0  \n",
      "189                0.0  \n",
      "190                0.0  \n",
      "191                0.0  \n",
      "\n",
      "[192 rows x 23 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 192 entries, 0 to 191\n",
      "Data columns (total 23 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   OperatingDTM             192 non-null    datetime64[us]\n",
      " 1   timestamp                192 non-null    datetime64[us]\n",
      " 2   interval                 192 non-null    int32         \n",
      " 3   temp_the_woodlands_tx    192 non-null    float64       \n",
      " 4   hum_the_woodlands_tx     192 non-null    float64       \n",
      " 5   wind_the_woodlands_tx    192 non-null    float64       \n",
      " 6   precip_the_woodlands_tx  192 non-null    float64       \n",
      " 7   temp_katy_tx             192 non-null    float64       \n",
      " 8   hum_katy_tx              192 non-null    float64       \n",
      " 9   wind_katy_tx             192 non-null    float64       \n",
      " 10  precip_katy_tx           192 non-null    float64       \n",
      " 11  temp_friendswood_tx      192 non-null    float64       \n",
      " 12  hum_friendswood_tx       192 non-null    float64       \n",
      " 13  wind_friendswood_tx      192 non-null    float64       \n",
      " 14  precip_friendswood_tx    192 non-null    float64       \n",
      " 15  temp_baytown_tx          192 non-null    float64       \n",
      " 16  hum_baytown_tx           192 non-null    float64       \n",
      " 17  wind_baytown_tx          192 non-null    float64       \n",
      " 18  precip_baytown_tx        192 non-null    float64       \n",
      " 19  temp_houston_tx          192 non-null    float64       \n",
      " 20  hum_houston_tx           192 non-null    float64       \n",
      " 21  wind_houston_tx          192 non-null    float64       \n",
      " 22  precip_houston_tx        192 non-null    float64       \n",
      "dtypes: datetime64[us](2), float64(20), int32(1)\n",
      "memory usage: 33.9 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"../ProjectMain/db/data.duckdb\")\n",
    "\n",
    "df = con.execute(\"\"\"\n",
    "\n",
    "select * from vw_forecast_weather_by_city order by 1 desc\n",
    "                 --WHERE OperatingDTM >= X and OperatingDTM <= Y\n",
    "                 ;\n",
    "\n",
    "\"\"\").fetchdf()\n",
    "print(df)\n",
    "print(df.info())\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e85fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
